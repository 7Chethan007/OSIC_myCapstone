{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 13th Oct","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\")\nprint(\"=\" * 60)\nprint(f\"üì± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    \"\"\"Optimized tabular features - simpler but more effective\"\"\"\n    vector = []\n    \n    # Basic but effective features\n    age = df_row['Age']\n    vector.extend([\n        (age - 50) / 30,  # Centered age\n        age / 100,  # Scaled age\n    ])\n    \n    # Simple sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(1.0)\n    else:\n        vector.append(0.0)\n    \n    # Simple smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # FVC features\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,  # Normalized FVC\n            (fvc - 2500) / 1000,  # Centered FVC\n        ])\n    \n    # Percent predicted (approximate)\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        \n        # Approximate percent predicted FVC\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n            \n        vector.append(min(pp_fvc, 2.0))  # Cap at 200%\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"Calculate Log Laplace Likelihood\"\"\"\n    sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n\n# Improved coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating optimized linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) >= 2:\n        try:\n            # Simple robust slope calculation\n            if len(weeks) == 2:\n                slope = (fvc[1] - fvc[0]) / (weeks[1] - weeks[0])\n            else:\n                # Use Theil-Sen estimator for robustness\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i+1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slope = (fvc[j] - fvc[i]) / (weeks[j] - weeks[i])\n                            slopes.append(slope)\n                slope = np.median(slopes) if slopes else 0.0\n            \n            A[patient] = slope\n        except:\n            A[patient] = 0.0\n    else:\n        A[patient] = 0.0\n    \n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with optimized features\")\n\n# Analyze target distribution\ndecay_values = np.array(list(A.values()))\nprint(f\"Target statistics: mean={decay_values.mean():.4f}, std={decay_values.std():.4f}\")\nprint(f\"Target range: [{decay_values.min():.4f}, {decay_values.max():.4f}]\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(OptimizedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100  # Only unfreeze later layers\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Simple but effective tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Feature fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain output\n        )\n        \n        # Initialize output layers for better convergence\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.mean_head, self.log_var_head]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (clipped to reasonable range)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            img = cv2.resize(img, (384, 384))\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Combined loss\n                mse_loss = F.mse_loss(mean_pred, targets)\n                uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                \n                # Start with more MSE focus, transition to uncertainty\n                if epoch < 20:\n                    loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                else:\n                    loss = 0.3 * mse_loss + 0.7 * uncertainty_loss\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Validation - FIXED: Handle scalar predictions properly\n            self.model.eval()\n            val_predictions, val_targets, val_log_vars = [], [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Convert to numpy properly (handle both scalar and tensor cases)\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    \n                    # Ensure we have arrays, not scalars\n                    if mean_pred_np.ndim == 0:  # scalar\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:  # array\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            \n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate metrics\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}, Loss={avg_train_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, LLL={avg_lll:.4f}\")\n                \n                # Update scheduler\n                scheduler.step(r2)\n                \n                # Save best model\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_MAE_1_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                \n                print(\"-\" * 50)\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    \n    # Simple stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    \n    # Data loaders - ensure batch size > 1 to avoid scalar issues\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Initialize model\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test forward pass\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        \n        print(f\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    \n    # Train model\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    \n    print(f\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    \n    return best_r2, best_mae, best_lll\n\nif __name__ == \"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:18:05.246560Z","iopub.execute_input":"2025-10-14T10:18:05.247102Z","iopub.status.idle":"2025-10-14T10:23:07.347958Z","shell.execute_reply.started":"2025-10-14T10:18:05.247073Z","shell.execute_reply":"2025-10-14T10:23:07.346994Z"}},"outputs":[{"name":"stdout","text":"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating optimized linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1083.03it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with optimized features\nTarget statistics: mean=-4.8107, std=6.7150\nTarget range: [-39.0741, 11.1389]\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","output_type":"stream"},{"name":"stdout","text":"Dataset val: 25 patients with images\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 169MB/s]\n","output_type":"stream"},{"name":"stdout","text":"üìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.10 GB\nEpoch 1: LR=1.00e-04, Loss=44.5500\n          R¬≤=-0.0548, MAE=4.5869, LLL=-5.1727\nüéØ NEW BEST! R¬≤: -0.0548\n--------------------------------------------------\nEpoch 2: LR=1.00e-04, Loss=31.4685\n          R¬≤=0.0800, MAE=4.3757, LLL=-4.8889\nüéØ NEW BEST! R¬≤: 0.0800\n--------------------------------------------------\nEpoch 3: LR=1.00e-04, Loss=30.7683\n          R¬≤=0.0031, MAE=4.6065, LLL=-4.9003\n--------------------------------------------------\nEpoch 4: LR=1.00e-04, Loss=29.8601\n          R¬≤=-0.0725, MAE=4.7057, LLL=-5.0710\n--------------------------------------------------\nEpoch 5: LR=1.00e-04, Loss=28.4130\n          R¬≤=-0.0204, MAE=4.5591, LLL=-4.8947\n--------------------------------------------------\nEpoch 6: LR=1.00e-04, Loss=26.6444\n          R¬≤=0.0027, MAE=4.5835, LLL=-4.8495\n--------------------------------------------------\nEpoch 7: LR=1.00e-04, Loss=25.6820\n          R¬≤=0.2864, MAE=3.8639, LLL=-4.2250\nüéØ NEW BEST! R¬≤: 0.2864\n--------------------------------------------------\nEpoch 8: LR=1.00e-04, Loss=22.9773\n          R¬≤=-0.1378, MAE=4.7471, LLL=-5.0278\n--------------------------------------------------\nEpoch 9: LR=1.00e-04, Loss=23.0101\n          R¬≤=0.0229, MAE=4.4708, LLL=-4.7226\n--------------------------------------------------\nEpoch 10: LR=1.00e-04, Loss=20.3382\n          R¬≤=-0.1058, MAE=4.6862, LLL=-4.8825\n--------------------------------------------------\nEpoch 11: LR=1.00e-04, Loss=20.4805\n          R¬≤=0.0326, MAE=4.4200, LLL=-4.6969\n--------------------------------------------------\nEpoch 12: LR=1.00e-04, Loss=19.3369\n          R¬≤=0.2048, MAE=3.9955, LLL=-4.2942\n--------------------------------------------------\nEpoch 13: LR=1.00e-04, Loss=19.6084\n          R¬≤=0.2447, MAE=4.0672, LLL=-4.3965\n--------------------------------------------------\nEpoch 14: LR=5.00e-05, Loss=18.3512\n          R¬≤=0.0766, MAE=4.5099, LLL=-4.7718\n--------------------------------------------------\nEpoch 15: LR=5.00e-05, Loss=18.7560\n          R¬≤=0.1062, MAE=4.4535, LLL=-4.7018\n--------------------------------------------------\nEpoch 16: LR=5.00e-05, Loss=17.3495\n          R¬≤=0.0635, MAE=4.4409, LLL=-4.6967\n--------------------------------------------------\nEpoch 17: LR=5.00e-05, Loss=15.0991\n          R¬≤=0.0715, MAE=4.2933, LLL=-4.5751\nEarly stopping at epoch 17\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.2864\nBest MAE = 3.8639\nBest LLL = -4.2250\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# R^2 NOT ABLE TO PUSH MORE THAN THIS ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:23:07.350085Z","iopub.execute_input":"2025-10-14T10:23:07.350372Z","iopub.status.idle":"2025-10-14T10:23:07.354435Z","shell.execute_reply.started":"2025-10-14T10:23:07.350349Z","shell.execute_reply":"2025-10-14T10:23:07.353708Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# LAST TRY WITH LLL AS THE MAIN METRIC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:23:07.355109Z","iopub.execute_input":"2025-10-14T10:23:07.355313Z","iopub.status.idle":"2025-10-14T10:23:07.376642Z","shell.execute_reply.started":"2025-10-14T10:23:07.355296Z","shell.execute_reply":"2025-10-14T10:23:07.375806Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ Optimized OSIC Model - LLL as Main Loss\")\nprint(\"=\"*60)\nprint(f\"üì± Device: {DEVICE}\")\n\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([\n        (age - 50)/30,\n        age / 100,\n    ])\n    vector.append(1.0 if df_row['Sex']=='Male' else 0.0)\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1,0,0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0,1,0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0,0,1])\n    else:\n        vector.extend([0,0,0])\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,\n            (fvc - 2500)/1000,\n        ])\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112*age) if age>0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101*age) if age>0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    return np.array(vector)\n\ndef calculate_lll_loss(mean_pred, log_var, targets):\n    # Numerically stable programmatic LLL negative for loss minimization\n    var = torch.exp(log_var)\n    delta = torch.abs(mean_pred - targets)\n    lll = - ( - torch.sqrt(torch.tensor(2.0)) * delta / (var.sqrt() + 1e-6) - torch.log(var.sqrt() * torch.sqrt(torch.tensor(2.0))) )\n    return lll.mean()\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2)*delta/sigma - np.log(sigma*np.sqrt(2))\n\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating decays ...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient']==patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) >=2:\n        try:\n            if len(weeks)==2:\n                slope = (fvc[1]-fvc[0])/(weeks[1]-weeks[0])\n            else:\n                slopes=[]\n                for i in range(len(weeks)):\n                    for j in range(i+1,len(weeks)):\n                        if weeks[j]!=weeks[i]:\n                            slopes.append((fvc[j]-fvc[i])/(weeks[j]-weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            A[patient] = slope\n        except:\n            A[patient]=0.0\n    else:\n        A[patient]=0.0\n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients.\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10,p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0,20.0), p=0.3),\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        for i,param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,64), nn.ReLU(),\n            nn.Linear(64,1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256,32), nn.ReLU(),\n            nn.Linear(32,1), nn.Tanh()\n        )\n        self._initialize_weights()\n    def _initialize_weights(self):\n        for m in [self.mean_head,self.log_var_head]:\n            if isinstance(m,nn.Linear):\n                nn.init.normal_(m.weight,0,0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias,0.0)\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(b,-1)\n        tab_features = self.tabular_processor(tabular)\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused)\n        log_var = self.log_var_head(fused)\n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184','ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower()=='.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split=='train':\n            return len(self.valid_patients)*8\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        if self.split=='train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n        patient = self.valid_patients[patient_idx]\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_features, target, patient\n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape)==3:\n                img = img[img.shape[0]//2]\n            img = cv2.resize(img,(384,384))\n            img_min,img_max = img.min(), img.max()\n            if img_max>img_min:\n                img = (img-img_min)/(img_max-img_min)*255\n            else:\n                img = np.zeros_like(img)\n            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img,img,img],axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384,384,3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                # Use negative LLL as loss\n                var = torch.exp(log_var)\n                delta = torch.abs(mean_pred - targets)\n                # Calculate negative log likelihood loss (Laplace)\n                loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                loss = loss.mean()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                train_batches += 1\n            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n            self.model.eval()\n            val_loss_sum = 0.0\n            val_batches = 0\n            val_predictions, val_targets, val_log_vars = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    var = torch.exp(log_var)\n                    delta = torch.abs(mean_pred - targets)\n                    val_loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                    val_loss = val_loss.mean()\n                    val_loss_sum += val_loss.item()\n                    val_batches += 1\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    if mean_pred_np.ndim == 0:\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else 0\n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                rmse = np.sqrt(np.mean((val_pred_np - val_target_np) ** 2))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}\")\n                print(f\"          Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, LLL={avg_lll:.4f}\")\n                scheduler.step(r2)\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_LLL_1_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                print(\"-\"*50)\n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        print(\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    print(\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    return best_r2, best_mae, best_lll\n\nif __name__==\"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:23:07.377903Z","iopub.execute_input":"2025-10-14T10:23:07.378465Z","iopub.status.idle":"2025-10-14T10:26:27.142743Z","shell.execute_reply.started":"2025-10-14T10:23:07.378442Z","shell.execute_reply":"2025-10-14T10:26:27.141514Z"}},"outputs":[{"name":"stdout","text":"üöÄ Optimized OSIC Model - LLL as Main Loss\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating decays ...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1185.09it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04\n          Train Loss=5.9234, Val Loss=5.0917\n          R¬≤=-0.1123, MAE=4.7440, RMSE=5.7397, LLL=-5.1170\nüéØ NEW BEST! R¬≤: -0.1123\n--------------------------------------------------\nEpoch 2: LR=1.00e-04\n          Train Loss=4.6866, Val Loss=4.4823\n          R¬≤=0.2448, MAE=3.9316, RMSE=4.7294, LLL=-4.3417\nüéØ NEW BEST! R¬≤: 0.2448\n--------------------------------------------------\nEpoch 3: LR=1.00e-04\n          Train Loss=4.5315, Val Loss=4.3381\n          R¬≤=-0.0834, MAE=4.6140, RMSE=5.6645, LLL=-4.9184\n--------------------------------------------------\nEpoch 4: LR=1.00e-04\n          Train Loss=4.4311, Val Loss=5.8354\n          R¬≤=-0.1471, MAE=4.9009, RMSE=5.8286, LLL=-5.0718\n--------------------------------------------------\nEpoch 5: LR=1.00e-04\n          Train Loss=4.3794, Val Loss=5.1903\n          R¬≤=0.0092, MAE=4.7053, RMSE=5.4170, LLL=-4.9092\n--------------------------------------------------\nEpoch 6: LR=1.00e-04\n          Train Loss=4.2580, Val Loss=5.0331\n          R¬≤=-0.0005, MAE=4.6153, RMSE=5.4434, LLL=-4.8172\n--------------------------------------------------\nEpoch 7: LR=1.00e-04\n          Train Loss=4.2309, Val Loss=4.0444\n          R¬≤=0.1178, MAE=4.2226, RMSE=5.1115, LLL=-4.4786\n--------------------------------------------------\nEpoch 8: LR=1.00e-04\n          Train Loss=4.1469, Val Loss=4.5843\n          R¬≤=0.0561, MAE=4.2920, RMSE=5.2874, LLL=-4.5393\n--------------------------------------------------\nEpoch 9: LR=5.00e-05\n          Train Loss=4.0768, Val Loss=3.9886\n          R¬≤=0.1739, MAE=4.0083, RMSE=4.9463, LLL=-4.2988\n--------------------------------------------------\nEpoch 10: LR=5.00e-05\n          Train Loss=3.8382, Val Loss=4.1563\n          R¬≤=0.2043, MAE=3.9306, RMSE=4.8545, LLL=-4.2308\n--------------------------------------------------\nEpoch 11: LR=5.00e-05\n          Train Loss=3.7811, Val Loss=4.6873\n          R¬≤=-0.0366, MAE=4.5117, RMSE=5.5410, LLL=-4.7222\n--------------------------------------------------\nEpoch 12: LR=5.00e-05\n          Train Loss=3.7622, Val Loss=4.2209\n          R¬≤=0.1326, MAE=4.1697, RMSE=5.0685, LLL=-4.4269\nEarly stopping at epoch 12\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.2448\nBest MAE = 3.9316\nBest LLL = -4.3417\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\")\nprint(\"=\" * 60)\nprint(f\"üì± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    \"\"\"Optimized tabular features - simpler but more effective\"\"\"\n    vector = []\n    \n    # Basic but effective features\n    age = df_row['Age']\n    vector.extend([\n        (age - 50) / 30,  # Centered age\n        age / 100,  # Scaled age\n    ])\n    \n    # Simple sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(1.0)\n    else:\n        vector.append(0.0)\n    \n    # Simple smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # FVC features\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,  # Normalized FVC\n            (fvc - 2500) / 1000,  # Centered FVC\n        ])\n    \n    # Percent predicted (approximate)\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        \n        # Approximate percent predicted FVC\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n            \n        vector.append(min(pp_fvc, 2.0))  # Cap at 200%\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"Calculate Log Laplace Likelihood\"\"\"\n    sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n\n# Improved coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating optimized linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) >= 2:\n        try:\n            # Simple robust slope calculation\n            if len(weeks) == 2:\n                slope = (fvc[1] - fvc[0]) / (weeks[1] - weeks[0])\n            else:\n                # Use Theil-Sen estimator for robustness\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i+1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slope = (fvc[j] - fvc[i]) / (weeks[j] - weeks[i])\n                            slopes.append(slope)\n                slope = np.median(slopes) if slopes else 0.0\n            \n            A[patient] = slope\n        except:\n            A[patient] = 0.0\n    else:\n        A[patient] = 0.0\n    \n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with optimized features\")\n\n# Analyze target distribution\ndecay_values = np.array(list(A.values()))\nprint(f\"Target statistics: mean={decay_values.mean():.4f}, std={decay_values.std():.4f}\")\nprint(f\"Target range: [{decay_values.min():.4f}, {decay_values.max():.4f}]\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(OptimizedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100  # Only unfreeze later layers\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Simple but effective tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Feature fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain output\n        )\n        \n        # Initialize output layers for better convergence\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.mean_head, self.log_var_head]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (clipped to reasonable range)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            img = cv2.resize(img, (384, 384))\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Combined loss\n                mse_loss = F.mse_loss(mean_pred, targets)\n                uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                \n                # Start with more MSE focus, transition to uncertainty\n                if epoch < 20:\n                    loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                else:\n                    loss = 0.3 * mse_loss + 0.7 * uncertainty_loss\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Validation - FIXED: Handle scalar predictions properly\n            self.model.eval()\n            val_predictions, val_targets, val_log_vars = [], [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Convert to numpy properly (handle both scalar and tensor cases)\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    \n                    # Ensure we have arrays, not scalars\n                    if mean_pred_np.ndim == 0:  # scalar\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:  # array\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            \n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate metrics\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}, Loss={avg_train_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, LLL={avg_lll:.4f}\")\n                \n                # Update scheduler\n                scheduler.step(r2)\n                \n                # Save best model\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_MAE_2_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                \n                print(\"-\" * 50)\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    \n    # Simple stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    \n    # Data loaders - ensure batch size > 1 to avoid scalar issues\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Initialize model\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test forward pass\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        \n        print(f\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    \n    # Train model\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    \n    print(f\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    \n    return best_r2, best_mae, best_lll\n\nif __name__ == \"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:26:27.146016Z","iopub.execute_input":"2025-10-14T10:26:27.146373Z","iopub.status.idle":"2025-10-14T10:35:02.338273Z","shell.execute_reply.started":"2025-10-14T10:26:27.146344Z","shell.execute_reply":"2025-10-14T10:35:02.337351Z"}},"outputs":[{"name":"stdout","text":"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating optimized linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1082.62it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with optimized features\nTarget statistics: mean=-4.8107, std=6.7150\nTarget range: [-39.0741, 11.1389]\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04, Loss=44.3931\n          R¬≤=0.0366, MAE=4.6583, LLL=-5.0808\nüéØ NEW BEST! R¬≤: 0.0366\n--------------------------------------------------\nEpoch 2: LR=1.00e-04, Loss=30.2252\n          R¬≤=0.1124, MAE=4.0819, LLL=-4.5801\nüéØ NEW BEST! R¬≤: 0.1124\n--------------------------------------------------\nEpoch 3: LR=1.00e-04, Loss=29.9113\n          R¬≤=0.0393, MAE=4.6941, LLL=-5.1883\n--------------------------------------------------\nEpoch 4: LR=1.00e-04, Loss=27.7596\n          R¬≤=-0.2982, MAE=5.3590, LLL=-5.8118\n--------------------------------------------------\nEpoch 5: LR=1.00e-04, Loss=28.1790\n          R¬≤=0.1421, MAE=4.3691, LLL=-4.7704\nüéØ NEW BEST! R¬≤: 0.1421\n--------------------------------------------------\nEpoch 6: LR=1.00e-04, Loss=25.5618\n          R¬≤=-0.0027, MAE=4.7108, LLL=-4.9841\n--------------------------------------------------\nEpoch 7: LR=1.00e-04, Loss=22.3282\n          R¬≤=0.0993, MAE=4.4088, LLL=-4.7432\n--------------------------------------------------\nEpoch 8: LR=1.00e-04, Loss=21.6495\n          R¬≤=-0.2529, MAE=5.2754, LLL=-5.5070\n--------------------------------------------------\nEpoch 9: LR=1.00e-04, Loss=22.4733\n          R¬≤=0.0123, MAE=4.6334, LLL=-4.8702\n--------------------------------------------------\nEpoch 10: LR=1.00e-04, Loss=20.6737\n          R¬≤=-0.1403, MAE=4.6680, LLL=-4.8918\n--------------------------------------------------\nEpoch 11: LR=1.00e-04, Loss=19.5252\n          R¬≤=-0.1086, MAE=5.0529, LLL=-5.2596\n--------------------------------------------------\nEpoch 12: LR=5.00e-05, Loss=18.3312\n          R¬≤=-0.1451, MAE=4.6703, LLL=-4.8812\n--------------------------------------------------\nEpoch 13: LR=5.00e-05, Loss=16.5409\n          R¬≤=0.3690, MAE=3.5609, LLL=-3.9502\nüéØ NEW BEST! R¬≤: 0.3690\n--------------------------------------------------\nEpoch 14: LR=5.00e-05, Loss=15.6858\n          R¬≤=0.0722, MAE=4.3240, LLL=-4.6027\n--------------------------------------------------\nEpoch 15: LR=5.00e-05, Loss=17.5599\n          R¬≤=0.0376, MAE=4.6319, LLL=-4.8522\n--------------------------------------------------\nEpoch 16: LR=5.00e-05, Loss=14.6796\n          R¬≤=0.0995, MAE=4.3299, LLL=-4.6042\n--------------------------------------------------\nEpoch 17: LR=5.00e-05, Loss=14.2652\n          R¬≤=-0.0824, MAE=4.6267, LLL=-4.8456\n--------------------------------------------------\nEpoch 18: LR=5.00e-05, Loss=14.3276\n          R¬≤=0.0972, MAE=4.2458, LLL=-4.5216\n--------------------------------------------------\nEpoch 19: LR=5.00e-05, Loss=14.3892\n          R¬≤=-0.1045, MAE=4.7642, LLL=-4.9532\n--------------------------------------------------\nEpoch 20: LR=2.50e-05, Loss=13.1137\n          R¬≤=-0.0354, MAE=4.4052, LLL=-4.6446\n--------------------------------------------------\nEpoch 21: LR=2.50e-05, Loss=6.4237\n          R¬≤=0.4643, MAE=3.2795, LLL=-3.6665\nüéØ NEW BEST! R¬≤: 0.4643\n--------------------------------------------------\nEpoch 22: LR=2.50e-05, Loss=7.2648\n          R¬≤=-0.1623, MAE=5.1628, LLL=-5.2937\n--------------------------------------------------\nEpoch 23: LR=2.50e-05, Loss=7.1096\n          R¬≤=-0.1174, MAE=4.8299, LLL=-5.0092\n--------------------------------------------------\nEpoch 24: LR=2.50e-05, Loss=6.6777\n          R¬≤=0.1444, MAE=4.1765, LLL=-4.4526\n--------------------------------------------------\nEpoch 25: LR=2.50e-05, Loss=7.0159\n          R¬≤=-0.2604, MAE=5.1148, LLL=-5.2731\n--------------------------------------------------\nEpoch 26: LR=2.50e-05, Loss=6.9815\n          R¬≤=0.1209, MAE=4.1046, LLL=-4.3782\n--------------------------------------------------\nEpoch 27: LR=2.50e-05, Loss=6.9332\n          R¬≤=-0.0533, MAE=4.8761, LLL=-5.0431\n--------------------------------------------------\nEpoch 28: LR=1.25e-05, Loss=6.2443\n          R¬≤=0.2558, MAE=3.9233, LLL=-4.2251\n--------------------------------------------------\nEpoch 29: LR=1.25e-05, Loss=6.0517\n          R¬≤=0.0485, MAE=4.4509, LLL=-4.6771\n--------------------------------------------------\nEpoch 30: LR=1.25e-05, Loss=5.8502\n          R¬≤=-0.0054, MAE=4.6701, LLL=-4.8747\n--------------------------------------------------\nEpoch 31: LR=1.25e-05, Loss=5.5734\n          R¬≤=0.1956, MAE=4.1213, LLL=-4.4092\nEarly stopping at epoch 31\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.4643\nBest MAE = 3.2795\nBest LLL = -3.6665\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ Optimized OSIC Model - LLL as Main Loss\")\nprint(\"=\"*60)\nprint(f\"üì± Device: {DEVICE}\")\n\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([\n        (age - 50)/30,\n        age / 100,\n    ])\n    vector.append(1.0 if df_row['Sex']=='Male' else 0.0)\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1,0,0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0,1,0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0,0,1])\n    else:\n        vector.extend([0,0,0])\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,\n            (fvc - 2500)/1000,\n        ])\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112*age) if age>0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101*age) if age>0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    return np.array(vector)\n\ndef calculate_lll_loss(mean_pred, log_var, targets):\n    # Numerically stable programmatic LLL negative for loss minimization\n    var = torch.exp(log_var)\n    delta = torch.abs(mean_pred - targets)\n    lll = - ( - torch.sqrt(torch.tensor(2.0)) * delta / (var.sqrt() + 1e-6) - torch.log(var.sqrt() * torch.sqrt(torch.tensor(2.0))) )\n    return lll.mean()\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2)*delta/sigma - np.log(sigma*np.sqrt(2))\n\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating decays ...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient']==patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) >=2:\n        try:\n            if len(weeks)==2:\n                slope = (fvc[1]-fvc[0])/(weeks[1]-weeks[0])\n            else:\n                slopes=[]\n                for i in range(len(weeks)):\n                    for j in range(i+1,len(weeks)):\n                        if weeks[j]!=weeks[i]:\n                            slopes.append((fvc[j]-fvc[i])/(weeks[j]-weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            A[patient] = slope\n        except:\n            A[patient]=0.0\n    else:\n        A[patient]=0.0\n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients.\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10,p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0,20.0), p=0.3),\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        for i,param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,64), nn.ReLU(),\n            nn.Linear(64,1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256,32), nn.ReLU(),\n            nn.Linear(32,1), nn.Tanh()\n        )\n        self._initialize_weights()\n    def _initialize_weights(self):\n        for m in [self.mean_head,self.log_var_head]:\n            if isinstance(m,nn.Linear):\n                nn.init.normal_(m.weight,0,0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias,0.0)\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(b,-1)\n        tab_features = self.tabular_processor(tabular)\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused)\n        log_var = self.log_var_head(fused)\n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184','ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower()=='.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split=='train':\n            return len(self.valid_patients)*8\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        if self.split=='train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n        patient = self.valid_patients[patient_idx]\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_features, target, patient\n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape)==3:\n                img = img[img.shape[0]//2]\n            img = cv2.resize(img,(384,384))\n            img_min,img_max = img.min(), img.max()\n            if img_max>img_min:\n                img = (img-img_min)/(img_max-img_min)*255\n            else:\n                img = np.zeros_like(img)\n            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img,img,img],axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384,384,3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                # Use negative LLL as loss\n                var = torch.exp(log_var)\n                delta = torch.abs(mean_pred - targets)\n                # Calculate negative log likelihood loss (Laplace)\n                loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                loss = loss.mean()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                train_batches += 1\n            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n            self.model.eval()\n            val_loss_sum = 0.0\n            val_batches = 0\n            val_predictions, val_targets, val_log_vars = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    var = torch.exp(log_var)\n                    delta = torch.abs(mean_pred - targets)\n                    val_loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                    val_loss = val_loss.mean()\n                    val_loss_sum += val_loss.item()\n                    val_batches += 1\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    if mean_pred_np.ndim == 0:\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else 0\n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                rmse = np.sqrt(np.mean((val_pred_np - val_target_np) ** 2))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}\")\n                print(f\"          Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, LLL={avg_lll:.4f}\")\n                scheduler.step(r2)\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_LLL_2_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                print(\"-\"*50)\n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        print(\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    print(\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    return best_r2, best_mae, best_lll\n\nif __name__==\"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:35:02.340245Z","iopub.execute_input":"2025-10-14T10:35:02.340506Z","iopub.status.idle":"2025-10-14T10:39:43.580698Z","shell.execute_reply.started":"2025-10-14T10:35:02.340479Z","shell.execute_reply":"2025-10-14T10:39:43.579636Z"}},"outputs":[{"name":"stdout","text":"üöÄ Optimized OSIC Model - LLL as Main Loss\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating decays ...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1160.05it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04\n          Train Loss=5.8574, Val Loss=5.5960\n          R¬≤=-0.2365, MAE=5.0468, RMSE=6.0516, LLL=-5.4790\nüéØ NEW BEST! R¬≤: -0.2365\n--------------------------------------------------\nEpoch 2: LR=1.00e-04\n          Train Loss=4.6673, Val Loss=6.0457\n          R¬≤=-0.1616, MAE=4.8955, RMSE=5.8654, LLL=-5.1542\nüéØ NEW BEST! R¬≤: -0.1616\n--------------------------------------------------\nEpoch 3: LR=1.00e-04\n          Train Loss=4.5751, Val Loss=4.6113\n          R¬≤=0.0155, MAE=4.6390, RMSE=5.3997, LLL=-4.8801\nüéØ NEW BEST! R¬≤: 0.0155\n--------------------------------------------------\nEpoch 4: LR=1.00e-04\n          Train Loss=4.3810, Val Loss=3.8305\n          R¬≤=0.2895, MAE=3.8107, RMSE=4.5872, LLL=-4.1558\nüéØ NEW BEST! R¬≤: 0.2895\n--------------------------------------------------\nEpoch 5: LR=1.00e-04\n          Train Loss=4.3655, Val Loss=4.2077\n          R¬≤=0.0176, MAE=4.3423, RMSE=5.3941, LLL=-4.6227\n--------------------------------------------------\nEpoch 6: LR=1.00e-04\n          Train Loss=4.3254, Val Loss=4.9522\n          R¬≤=-0.0352, MAE=4.7187, RMSE=5.5372, LLL=-4.9131\n--------------------------------------------------\nEpoch 7: LR=1.00e-04\n          Train Loss=4.2225, Val Loss=3.9310\n          R¬≤=0.2935, MAE=3.7653, RMSE=4.5743, LLL=-4.0893\nüéØ NEW BEST! R¬≤: 0.2935\n--------------------------------------------------\nEpoch 8: LR=1.00e-04\n          Train Loss=4.0047, Val Loss=5.1416\n          R¬≤=-0.2149, MAE=5.1451, RMSE=5.9985, LLL=-5.2676\n--------------------------------------------------\nEpoch 9: LR=1.00e-04\n          Train Loss=4.0698, Val Loss=4.5877\n          R¬≤=-0.0476, MAE=4.5625, RMSE=5.5702, LLL=-4.7647\n--------------------------------------------------\nEpoch 10: LR=1.00e-04\n          Train Loss=3.9314, Val Loss=4.0671\n          R¬≤=0.1179, MAE=4.2921, RMSE=5.1112, LLL=-4.5392\n--------------------------------------------------\nEpoch 11: LR=1.00e-04\n          Train Loss=3.9609, Val Loss=5.7296\n          R¬≤=-0.5385, MAE=5.6507, RMSE=6.7503, LLL=-5.7001\n--------------------------------------------------\nEpoch 12: LR=1.00e-04\n          Train Loss=3.8514, Val Loss=4.9958\n          R¬≤=0.0757, MAE=4.4940, RMSE=5.2322, LLL=-4.7034\n--------------------------------------------------\nEpoch 13: LR=1.00e-04\n          Train Loss=3.8279, Val Loss=4.2404\n          R¬≤=0.2140, MAE=4.1337, RMSE=4.8248, LLL=-4.3959\n--------------------------------------------------\nEpoch 14: LR=5.00e-05\n          Train Loss=3.7248, Val Loss=4.6628\n          R¬≤=0.1572, MAE=4.2050, RMSE=4.9960, LLL=-4.4583\n--------------------------------------------------\nEpoch 15: LR=5.00e-05\n          Train Loss=3.6099, Val Loss=4.2780\n          R¬≤=0.1595, MAE=4.2486, RMSE=4.9894, LLL=-4.4960\n--------------------------------------------------\nEpoch 16: LR=5.00e-05\n          Train Loss=3.5283, Val Loss=4.2208\n          R¬≤=0.1274, MAE=4.1044, RMSE=5.0838, LLL=-4.3728\n--------------------------------------------------\nEpoch 17: LR=5.00e-05\n          Train Loss=3.6067, Val Loss=4.8671\n          R¬≤=-0.0628, MAE=4.5054, RMSE=5.6104, LLL=-4.7152\nEarly stopping at epoch 17\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.2935\nBest MAE = 3.7653\nBest LLL = -4.0893\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\")\nprint(\"=\" * 60)\nprint(f\"üì± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    \"\"\"Optimized tabular features - simpler but more effective\"\"\"\n    vector = []\n    \n    # Basic but effective features\n    age = df_row['Age']\n    vector.extend([\n        (age - 50) / 30,  # Centered age\n        age / 100,  # Scaled age\n    ])\n    \n    # Simple sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(1.0)\n    else:\n        vector.append(0.0)\n    \n    # Simple smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # FVC features\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,  # Normalized FVC\n            (fvc - 2500) / 1000,  # Centered FVC\n        ])\n    \n    # Percent predicted (approximate)\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        \n        # Approximate percent predicted FVC\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n            \n        vector.append(min(pp_fvc, 2.0))  # Cap at 200%\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"Calculate Log Laplace Likelihood\"\"\"\n    sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n\n# Improved coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating optimized linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) >= 2:\n        try:\n            # Simple robust slope calculation\n            if len(weeks) == 2:\n                slope = (fvc[1] - fvc[0]) / (weeks[1] - weeks[0])\n            else:\n                # Use Theil-Sen estimator for robustness\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i+1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slope = (fvc[j] - fvc[i]) / (weeks[j] - weeks[i])\n                            slopes.append(slope)\n                slope = np.median(slopes) if slopes else 0.0\n            \n            A[patient] = slope\n        except:\n            A[patient] = 0.0\n    else:\n        A[patient] = 0.0\n    \n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with optimized features\")\n\n# Analyze target distribution\ndecay_values = np.array(list(A.values()))\nprint(f\"Target statistics: mean={decay_values.mean():.4f}, std={decay_values.std():.4f}\")\nprint(f\"Target range: [{decay_values.min():.4f}, {decay_values.max():.4f}]\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(OptimizedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100  # Only unfreeze later layers\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Simple but effective tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Feature fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain output\n        )\n        \n        # Initialize output layers for better convergence\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.mean_head, self.log_var_head]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (clipped to reasonable range)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            img = cv2.resize(img, (384, 384))\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Combined loss\n                mse_loss = F.mse_loss(mean_pred, targets)\n                uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                \n                # Start with more MSE focus, transition to uncertainty\n                if epoch < 20:\n                    loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                else:\n                    loss = 0.3 * mse_loss + 0.7 * uncertainty_loss\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Validation - FIXED: Handle scalar predictions properly\n            self.model.eval()\n            val_predictions, val_targets, val_log_vars = [], [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Convert to numpy properly (handle both scalar and tensor cases)\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    \n                    # Ensure we have arrays, not scalars\n                    if mean_pred_np.ndim == 0:  # scalar\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:  # array\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            \n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate metrics\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}, Loss={avg_train_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, LLL={avg_lll:.4f}\")\n                \n                # Update scheduler\n                scheduler.step(r2)\n                \n                # Save best model\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_MAE_3_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                \n                print(\"-\" * 50)\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    \n    # Simple stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    \n    # Data loaders - ensure batch size > 1 to avoid scalar issues\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Initialize model\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test forward pass\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        \n        print(f\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    \n    # Train model\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    \n    print(f\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    \n    return best_r2, best_mae, best_lll\n\nif __name__ == \"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:39:43.583100Z","iopub.execute_input":"2025-10-14T10:39:43.583371Z","iopub.status.idle":"2025-10-14T10:46:02.796768Z","shell.execute_reply.started":"2025-10-14T10:39:43.583342Z","shell.execute_reply":"2025-10-14T10:46:02.795747Z"}},"outputs":[{"name":"stdout","text":"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating optimized linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1184.58it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with optimized features\nTarget statistics: mean=-4.8107, std=6.7150\nTarget range: [-39.0741, 11.1389]\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04, Loss=44.7973\n          R¬≤=-0.0271, MAE=4.6716, LLL=-5.3058\nüéØ NEW BEST! R¬≤: -0.0271\n--------------------------------------------------\nEpoch 2: LR=1.00e-04, Loss=31.3869\n          R¬≤=0.2216, MAE=3.9747, LLL=-4.6449\nüéØ NEW BEST! R¬≤: 0.2216\n--------------------------------------------------\nEpoch 3: LR=1.00e-04, Loss=28.8738\n          R¬≤=0.1079, MAE=4.4628, LLL=-4.9425\n--------------------------------------------------\nEpoch 4: LR=1.00e-04, Loss=28.4719\n          R¬≤=-0.0012, MAE=4.7483, LLL=-5.2659\n--------------------------------------------------\nEpoch 5: LR=1.00e-04, Loss=25.8545\n          R¬≤=0.2243, MAE=4.1031, LLL=-4.5335\nüéØ NEW BEST! R¬≤: 0.2243\n--------------------------------------------------\nEpoch 6: LR=1.00e-04, Loss=23.4152\n          R¬≤=-0.7422, MAE=6.2147, LLL=-6.2293\n--------------------------------------------------\nEpoch 7: LR=1.00e-04, Loss=23.3181\n          R¬≤=0.1114, MAE=4.2136, LLL=-4.5890\n--------------------------------------------------\nEpoch 8: LR=1.00e-04, Loss=22.0760\n          R¬≤=-0.5760, MAE=5.6613, LLL=-5.8588\n--------------------------------------------------\nEpoch 9: LR=1.00e-04, Loss=21.0806\n          R¬≤=0.0578, MAE=4.2613, LLL=-4.5473\n--------------------------------------------------\nEpoch 10: LR=1.00e-04, Loss=19.5917\n          R¬≤=0.0373, MAE=4.6198, LLL=-4.8468\n--------------------------------------------------\nEpoch 11: LR=1.00e-04, Loss=19.6638\n          R¬≤=0.0471, MAE=4.3568, LLL=-4.6150\n--------------------------------------------------\nEpoch 12: LR=5.00e-05, Loss=18.2211\n          R¬≤=0.2401, MAE=3.7957, LLL=-4.1161\nüéØ NEW BEST! R¬≤: 0.2401\n--------------------------------------------------\nEpoch 13: LR=5.00e-05, Loss=16.6356\n          R¬≤=0.2515, MAE=4.0969, LLL=-4.4011\nüéØ NEW BEST! R¬≤: 0.2515\n--------------------------------------------------\nEpoch 14: LR=5.00e-05, Loss=16.1086\n          R¬≤=0.0969, MAE=4.2870, LLL=-4.5601\n--------------------------------------------------\nEpoch 15: LR=5.00e-05, Loss=15.8568\n          R¬≤=0.1260, MAE=4.4142, LLL=-4.6546\n--------------------------------------------------\nEpoch 16: LR=5.00e-05, Loss=14.1120\n          R¬≤=0.0000, MAE=4.4857, LLL=-4.7223\n--------------------------------------------------\nEpoch 17: LR=5.00e-05, Loss=13.2846\n          R¬≤=0.0754, MAE=4.6312, LLL=-4.9021\n--------------------------------------------------\nEpoch 18: LR=5.00e-05, Loss=14.0560\n          R¬≤=0.1537, MAE=4.2367, LLL=-4.5079\n--------------------------------------------------\nEpoch 19: LR=5.00e-05, Loss=13.0846\n          R¬≤=0.0910, MAE=4.4544, LLL=-4.6809\n--------------------------------------------------\nEpoch 20: LR=2.50e-05, Loss=13.5158\n          R¬≤=-0.0827, MAE=4.8052, LLL=-4.9844\n--------------------------------------------------\nEpoch 21: LR=2.50e-05, Loss=6.2399\n          R¬≤=0.2474, MAE=3.8747, LLL=-4.1916\n--------------------------------------------------\nEpoch 22: LR=2.50e-05, Loss=7.3003\n          R¬≤=-0.0624, MAE=5.0015, LLL=-5.1573\n--------------------------------------------------\nEpoch 23: LR=2.50e-05, Loss=7.9472\n          R¬≤=0.1179, MAE=4.0842, LLL=-4.3632\nEarly stopping at epoch 23\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.2515\nBest MAE = 4.0969\nBest LLL = -4.4011\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ Optimized OSIC Model - LLL as Main Loss\")\nprint(\"=\"*60)\nprint(f\"üì± Device: {DEVICE}\")\n\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([\n        (age - 50)/30,\n        age / 100,\n    ])\n    vector.append(1.0 if df_row['Sex']=='Male' else 0.0)\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1,0,0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0,1,0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0,0,1])\n    else:\n        vector.extend([0,0,0])\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,\n            (fvc - 2500)/1000,\n        ])\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112*age) if age>0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101*age) if age>0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    return np.array(vector)\n\ndef calculate_lll_loss(mean_pred, log_var, targets):\n    # Numerically stable programmatic LLL negative for loss minimization\n    var = torch.exp(log_var)\n    delta = torch.abs(mean_pred - targets)\n    lll = - ( - torch.sqrt(torch.tensor(2.0)) * delta / (var.sqrt() + 1e-6) - torch.log(var.sqrt() * torch.sqrt(torch.tensor(2.0))) )\n    return lll.mean()\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2)*delta/sigma - np.log(sigma*np.sqrt(2))\n\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating decays ...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient']==patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) >=2:\n        try:\n            if len(weeks)==2:\n                slope = (fvc[1]-fvc[0])/(weeks[1]-weeks[0])\n            else:\n                slopes=[]\n                for i in range(len(weeks)):\n                    for j in range(i+1,len(weeks)):\n                        if weeks[j]!=weeks[i]:\n                            slopes.append((fvc[j]-fvc[i])/(weeks[j]-weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            A[patient] = slope\n        except:\n            A[patient]=0.0\n    else:\n        A[patient]=0.0\n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients.\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10,p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0,20.0), p=0.3),\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        for i,param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,64), nn.ReLU(),\n            nn.Linear(64,1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256,32), nn.ReLU(),\n            nn.Linear(32,1), nn.Tanh()\n        )\n        self._initialize_weights()\n    def _initialize_weights(self):\n        for m in [self.mean_head,self.log_var_head]:\n            if isinstance(m,nn.Linear):\n                nn.init.normal_(m.weight,0,0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias,0.0)\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(b,-1)\n        tab_features = self.tabular_processor(tabular)\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused)\n        log_var = self.log_var_head(fused)\n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184','ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower()=='.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split=='train':\n            return len(self.valid_patients)*8\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        if self.split=='train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n        patient = self.valid_patients[patient_idx]\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_features, target, patient\n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape)==3:\n                img = img[img.shape[0]//2]\n            img = cv2.resize(img,(384,384))\n            img_min,img_max = img.min(), img.max()\n            if img_max>img_min:\n                img = (img-img_min)/(img_max-img_min)*255\n            else:\n                img = np.zeros_like(img)\n            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img,img,img],axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384,384,3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                # Use negative LLL as loss\n                var = torch.exp(log_var)\n                delta = torch.abs(mean_pred - targets)\n                # Calculate negative log likelihood loss (Laplace)\n                loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                loss = loss.mean()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                train_batches += 1\n            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n            self.model.eval()\n            val_loss_sum = 0.0\n            val_batches = 0\n            val_predictions, val_targets, val_log_vars = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    var = torch.exp(log_var)\n                    delta = torch.abs(mean_pred - targets)\n                    val_loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                    val_loss = val_loss.mean()\n                    val_loss_sum += val_loss.item()\n                    val_batches += 1\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    if mean_pred_np.ndim == 0:\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else 0\n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                rmse = np.sqrt(np.mean((val_pred_np - val_target_np) ** 2))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}\")\n                print(f\"          Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, LLL={avg_lll:.4f}\")\n                scheduler.step(r2)\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_LLL_3_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                print(\"-\"*50)\n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        print(\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    print(\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    return best_r2, best_mae, best_lll\n\nif __name__==\"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:46:02.798871Z","iopub.execute_input":"2025-10-14T10:46:02.799922Z","iopub.status.idle":"2025-10-14T10:49:56.047768Z","shell.execute_reply.started":"2025-10-14T10:46:02.799884Z","shell.execute_reply":"2025-10-14T10:49:56.046715Z"}},"outputs":[{"name":"stdout","text":"üöÄ Optimized OSIC Model - LLL as Main Loss\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating decays ...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1145.06it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.14 GB\nEpoch 1: LR=1.00e-04\n          Train Loss=5.8157, Val Loss=4.9924\n          R¬≤=-0.1428, MAE=4.7830, RMSE=5.8177, LLL=-5.1054\nüéØ NEW BEST! R¬≤: -0.1428\n--------------------------------------------------\nEpoch 2: LR=1.00e-04\n          Train Loss=4.6919, Val Loss=4.8649\n          R¬≤=0.0731, MAE=4.3889, RMSE=5.2395, LLL=-4.6991\nüéØ NEW BEST! R¬≤: 0.0731\n--------------------------------------------------\nEpoch 3: LR=1.00e-04\n          Train Loss=4.5497, Val Loss=4.6014\n          R¬≤=0.0994, MAE=4.4487, RMSE=5.1647, LLL=-4.7165\nüéØ NEW BEST! R¬≤: 0.0994\n--------------------------------------------------\nEpoch 4: LR=1.00e-04\n          Train Loss=4.4777, Val Loss=4.3432\n          R¬≤=0.1981, MAE=4.1655, RMSE=4.8735, LLL=-4.4575\nüéØ NEW BEST! R¬≤: 0.1981\n--------------------------------------------------\nEpoch 5: LR=1.00e-04\n          Train Loss=4.3949, Val Loss=5.6248\n          R¬≤=-0.1375, MAE=4.8815, RMSE=5.8042, LLL=-5.0653\n--------------------------------------------------\nEpoch 6: LR=1.00e-04\n          Train Loss=4.2705, Val Loss=5.0129\n          R¬≤=-0.1286, MAE=4.7854, RMSE=5.7816, LLL=-4.9624\n--------------------------------------------------\nEpoch 7: LR=1.00e-04\n          Train Loss=4.1512, Val Loss=4.7598\n          R¬≤=-0.0858, MAE=4.8435, RMSE=5.6708, LLL=-5.0160\n--------------------------------------------------\nEpoch 8: LR=1.00e-04\n          Train Loss=4.0938, Val Loss=4.4247\n          R¬≤=-0.0349, MAE=4.7417, RMSE=5.5364, LLL=-4.9531\n--------------------------------------------------\nEpoch 9: LR=1.00e-04\n          Train Loss=4.0455, Val Loss=5.0380\n          R¬≤=-0.1071, MAE=4.7110, RMSE=5.7263, LLL=-4.9021\n--------------------------------------------------\nEpoch 10: LR=1.00e-04\n          Train Loss=3.9447, Val Loss=4.5534\n          R¬≤=-0.0319, MAE=4.6123, RMSE=5.5283, LLL=-4.8152\n--------------------------------------------------\nEpoch 11: LR=5.00e-05\n          Train Loss=3.8149, Val Loss=4.5481\n          R¬≤=0.0247, MAE=4.5138, RMSE=5.3746, LLL=-4.7305\n--------------------------------------------------\nEpoch 12: LR=5.00e-05\n          Train Loss=3.6516, Val Loss=4.6555\n          R¬≤=0.1550, MAE=4.1197, RMSE=5.0026, LLL=-4.3840\n--------------------------------------------------\nEpoch 13: LR=5.00e-05\n          Train Loss=3.7049, Val Loss=4.3742\n          R¬≤=0.1140, MAE=4.1459, RMSE=5.1226, LLL=-4.4103\n--------------------------------------------------\nEpoch 14: LR=5.00e-05\n          Train Loss=3.6437, Val Loss=4.8401\n          R¬≤=0.0794, MAE=4.4737, RMSE=5.2217, LLL=-4.6881\nEarly stopping at epoch 14\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.1981\nBest MAE = 4.1655\nBest LLL = -4.4575\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\")\nprint(\"=\" * 60)\nprint(f\"üì± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    \"\"\"Optimized tabular features - simpler but more effective\"\"\"\n    vector = []\n    \n    # Basic but effective features\n    age = df_row['Age']\n    vector.extend([\n        (age - 50) / 30,  # Centered age\n        age / 100,  # Scaled age\n    ])\n    \n    # Simple sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(1.0)\n    else:\n        vector.append(0.0)\n    \n    # Simple smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # FVC features\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,  # Normalized FVC\n            (fvc - 2500) / 1000,  # Centered FVC\n        ])\n    \n    # Percent predicted (approximate)\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        \n        # Approximate percent predicted FVC\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n            \n        vector.append(min(pp_fvc, 2.0))  # Cap at 200%\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"Calculate Log Laplace Likelihood\"\"\"\n    sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n\n# Improved coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating optimized linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) >= 2:\n        try:\n            # Simple robust slope calculation\n            if len(weeks) == 2:\n                slope = (fvc[1] - fvc[0]) / (weeks[1] - weeks[0])\n            else:\n                # Use Theil-Sen estimator for robustness\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i+1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slope = (fvc[j] - fvc[i]) / (weeks[j] - weeks[i])\n                            slopes.append(slope)\n                slope = np.median(slopes) if slopes else 0.0\n            \n            A[patient] = slope\n        except:\n            A[patient] = 0.0\n    else:\n        A[patient] = 0.0\n    \n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with optimized features\")\n\n# Analyze target distribution\ndecay_values = np.array(list(A.values()))\nprint(f\"Target statistics: mean={decay_values.mean():.4f}, std={decay_values.std():.4f}\")\nprint(f\"Target range: [{decay_values.min():.4f}, {decay_values.max():.4f}]\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(OptimizedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100  # Only unfreeze later layers\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Simple but effective tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Feature fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain output\n        )\n        \n        # Initialize output layers for better convergence\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.mean_head, self.log_var_head]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (clipped to reasonable range)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            img = cv2.resize(img, (384, 384))\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Combined loss\n                mse_loss = F.mse_loss(mean_pred, targets)\n                uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                \n                # Start with more MSE focus, transition to uncertainty\n                if epoch < 20:\n                    loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                else:\n                    loss = 0.3 * mse_loss + 0.7 * uncertainty_loss\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Validation - FIXED: Handle scalar predictions properly\n            self.model.eval()\n            val_predictions, val_targets, val_log_vars = [], [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Convert to numpy properly (handle both scalar and tensor cases)\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    \n                    # Ensure we have arrays, not scalars\n                    if mean_pred_np.ndim == 0:  # scalar\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:  # array\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            \n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate metrics\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}, Loss={avg_train_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, LLL={avg_lll:.4f}\")\n                \n                # Update scheduler\n                scheduler.step(r2)\n                \n                # Save best model\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_MAE_4_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                \n                print(\"-\" * 50)\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    \n    # Simple stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    \n    # Data loaders - ensure batch size > 1 to avoid scalar issues\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Initialize model\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test forward pass\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        \n        print(f\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    \n    # Train model\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    \n    print(f\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    \n    return best_r2, best_mae, best_lll\n\nif __name__ == \"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:49:56.050121Z","iopub.execute_input":"2025-10-14T10:49:56.050422Z","iopub.status.idle":"2025-10-14T10:56:30.143952Z","shell.execute_reply.started":"2025-10-14T10:49:56.050394Z","shell.execute_reply":"2025-10-14T10:56:30.142997Z"}},"outputs":[{"name":"stdout","text":"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating optimized linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1115.01it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with optimized features\nTarget statistics: mean=-4.8107, std=6.7150\nTarget range: [-39.0741, 11.1389]\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.14 GB\nEpoch 1: LR=1.00e-04, Loss=44.2671\n          R¬≤=0.1958, MAE=4.0305, LLL=-4.5077\nüéØ NEW BEST! R¬≤: 0.1958\n--------------------------------------------------\nEpoch 2: LR=1.00e-04, Loss=30.8762\n          R¬≤=-0.1627, MAE=4.7715, LLL=-5.1491\n--------------------------------------------------\nEpoch 3: LR=1.00e-04, Loss=30.3178\n          R¬≤=0.0661, MAE=4.5636, LLL=-4.9858\n--------------------------------------------------\nEpoch 4: LR=1.00e-04, Loss=27.2203\n          R¬≤=-0.0258, MAE=4.6960, LLL=-5.1812\n--------------------------------------------------\nEpoch 5: LR=1.00e-04, Loss=25.9201\n          R¬≤=0.2225, MAE=4.0389, LLL=-4.5297\nüéØ NEW BEST! R¬≤: 0.2225\n--------------------------------------------------\nEpoch 6: LR=1.00e-04, Loss=24.8922\n          R¬≤=0.0504, MAE=4.3681, LLL=-4.7262\n--------------------------------------------------\nEpoch 7: LR=1.00e-04, Loss=23.4320\n          R¬≤=0.1081, MAE=4.1788, LLL=-4.5400\n--------------------------------------------------\nEpoch 8: LR=1.00e-04, Loss=23.1685\n          R¬≤=-0.2655, MAE=4.9888, LLL=-5.1504\n--------------------------------------------------\nEpoch 9: LR=1.00e-04, Loss=21.5579\n          R¬≤=0.1577, MAE=4.1552, LLL=-4.4298\n--------------------------------------------------\nEpoch 10: LR=1.00e-04, Loss=21.5087\n          R¬≤=0.0762, MAE=4.4550, LLL=-4.6801\n--------------------------------------------------\nEpoch 11: LR=1.00e-04, Loss=19.6339\n          R¬≤=-1.0354, MAE=5.6657, LLL=-5.7312\n--------------------------------------------------\nEpoch 12: LR=5.00e-05, Loss=19.3814\n          R¬≤=-0.3205, MAE=4.6842, LLL=-4.8846\n--------------------------------------------------\nEpoch 13: LR=5.00e-05, Loss=17.9820\n          R¬≤=0.1905, MAE=3.9989, LLL=-4.3125\n--------------------------------------------------\nEpoch 14: LR=5.00e-05, Loss=17.1499\n          R¬≤=0.1072, MAE=4.1411, LLL=-4.4259\n--------------------------------------------------\nEpoch 15: LR=5.00e-05, Loss=15.1525\n          R¬≤=0.2856, MAE=3.9202, LLL=-4.2371\nüéØ NEW BEST! R¬≤: 0.2856\n--------------------------------------------------\nEpoch 16: LR=5.00e-05, Loss=15.7650\n          R¬≤=-0.0450, MAE=4.6988, LLL=-4.8915\n--------------------------------------------------\nEpoch 17: LR=5.00e-05, Loss=13.4315\n          R¬≤=0.0122, MAE=4.7186, LLL=-4.9383\n--------------------------------------------------\nEpoch 18: LR=5.00e-05, Loss=15.0867\n          R¬≤=0.1300, MAE=4.2796, LLL=-4.5388\n--------------------------------------------------\nEpoch 19: LR=5.00e-05, Loss=14.1276\n          R¬≤=0.1367, MAE=4.2208, LLL=-4.4866\n--------------------------------------------------\nEpoch 20: LR=5.00e-05, Loss=14.2368\n          R¬≤=0.0558, MAE=4.2335, LLL=-4.4999\n--------------------------------------------------\nEpoch 21: LR=5.00e-05, Loss=7.2649\n          R¬≤=0.1231, MAE=4.0443, LLL=-4.3285\n--------------------------------------------------\nEpoch 22: LR=2.50e-05, Loss=7.2896\n          R¬≤=-0.1162, MAE=4.8344, LLL=-5.0092\n--------------------------------------------------\nEpoch 23: LR=2.50e-05, Loss=7.0614\n          R¬≤=0.1367, MAE=4.3670, LLL=-4.6041\n--------------------------------------------------\nEpoch 24: LR=2.50e-05, Loss=7.0339\n          R¬≤=0.1469, MAE=4.2944, LLL=-4.5513\n--------------------------------------------------\nEpoch 25: LR=2.50e-05, Loss=7.2992\n          R¬≤=-0.4899, MAE=5.3558, LLL=-5.4669\nEarly stopping at epoch 25\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.2856\nBest MAE = 3.9202\nBest LLL = -4.2371\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# LLL\nimport os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ Optimized OSIC Model - LLL as Main Loss\")\nprint(\"=\"*60)\nprint(f\"üì± Device: {DEVICE}\")\n\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([\n        (age - 50)/30,\n        age / 100,\n    ])\n    vector.append(1.0 if df_row['Sex']=='Male' else 0.0)\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1,0,0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0,1,0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0,0,1])\n    else:\n        vector.extend([0,0,0])\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,\n            (fvc - 2500)/1000,\n        ])\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112*age) if age>0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101*age) if age>0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    return np.array(vector)\n\ndef calculate_lll_loss(mean_pred, log_var, targets):\n    # Numerically stable programmatic LLL negative for loss minimization\n    var = torch.exp(log_var)\n    delta = torch.abs(mean_pred - targets)\n    lll = - ( - torch.sqrt(torch.tensor(2.0)) * delta / (var.sqrt() + 1e-6) - torch.log(var.sqrt() * torch.sqrt(torch.tensor(2.0))) )\n    return lll.mean()\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2)*delta/sigma - np.log(sigma*np.sqrt(2))\n\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating decays ...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient']==patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) >=2:\n        try:\n            if len(weeks)==2:\n                slope = (fvc[1]-fvc[0])/(weeks[1]-weeks[0])\n            else:\n                slopes=[]\n                for i in range(len(weeks)):\n                    for j in range(i+1,len(weeks)):\n                        if weeks[j]!=weeks[i]:\n                            slopes.append((fvc[j]-fvc[i])/(weeks[j]-weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            A[patient] = slope\n        except:\n            A[patient]=0.0\n    else:\n        A[patient]=0.0\n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients.\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10,p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0,20.0), p=0.3),\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        for i,param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,64), nn.ReLU(),\n            nn.Linear(64,1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256,32), nn.ReLU(),\n            nn.Linear(32,1), nn.Tanh()\n        )\n        self._initialize_weights()\n    def _initialize_weights(self):\n        for m in [self.mean_head,self.log_var_head]:\n            if isinstance(m,nn.Linear):\n                nn.init.normal_(m.weight,0,0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias,0.0)\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(b,-1)\n        tab_features = self.tabular_processor(tabular)\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused)\n        log_var = self.log_var_head(fused)\n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184','ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower()=='.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split=='train':\n            return len(self.valid_patients)*8\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        if self.split=='train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n        patient = self.valid_patients[patient_idx]\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_features, target, patient\n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape)==3:\n                img = img[img.shape[0]//2]\n            img = cv2.resize(img,(384,384))\n            img_min,img_max = img.min(), img.max()\n            if img_max>img_min:\n                img = (img-img_min)/(img_max-img_min)*255\n            else:\n                img = np.zeros_like(img)\n            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img,img,img],axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384,384,3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                # Use negative LLL as loss\n                var = torch.exp(log_var)\n                delta = torch.abs(mean_pred - targets)\n                # Calculate negative log likelihood loss (Laplace)\n                loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                loss = loss.mean()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                train_batches += 1\n            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n            self.model.eval()\n            val_loss_sum = 0.0\n            val_batches = 0\n            val_predictions, val_targets, val_log_vars = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    var = torch.exp(log_var)\n                    delta = torch.abs(mean_pred - targets)\n                    val_loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                    val_loss = val_loss.mean()\n                    val_loss_sum += val_loss.item()\n                    val_batches += 1\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    if mean_pred_np.ndim == 0:\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else 0\n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                rmse = np.sqrt(np.mean((val_pred_np - val_target_np) ** 2))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}\")\n                print(f\"          Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, LLL={avg_lll:.4f}\")\n                scheduler.step(r2)\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_LLL_4_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                print(\"-\"*50)\n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        print(\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    print(\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    return best_r2, best_mae, best_lll\n\nif __name__==\"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:56:30.145755Z","iopub.execute_input":"2025-10-14T10:56:30.145995Z","iopub.status.idle":"2025-10-14T11:02:30.911465Z","shell.execute_reply.started":"2025-10-14T10:56:30.145971Z","shell.execute_reply":"2025-10-14T11:02:30.910724Z"}},"outputs":[{"name":"stdout","text":"üöÄ Optimized OSIC Model - LLL as Main Loss\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating decays ...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1166.23it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04\n          Train Loss=5.8504, Val Loss=5.2851\n          R¬≤=-0.1432, MAE=4.8567, RMSE=5.8187, LLL=-5.1850\nüéØ NEW BEST! R¬≤: -0.1432\n--------------------------------------------------\nEpoch 2: LR=1.00e-04\n          Train Loss=4.6566, Val Loss=4.9946\n          R¬≤=0.0021, MAE=4.5669, RMSE=5.4365, LLL=-4.8305\nüéØ NEW BEST! R¬≤: 0.0021\n--------------------------------------------------\nEpoch 3: LR=1.00e-04\n          Train Loss=4.5233, Val Loss=4.4612\n          R¬≤=0.0331, MAE=4.5442, RMSE=5.3515, LLL=-4.8250\nüéØ NEW BEST! R¬≤: 0.0331\n--------------------------------------------------\nEpoch 4: LR=1.00e-04\n          Train Loss=4.4749, Val Loss=4.9620\n          R¬≤=-0.0022, MAE=4.7128, RMSE=5.4481, LLL=-4.9588\n--------------------------------------------------\nEpoch 5: LR=1.00e-04\n          Train Loss=4.3201, Val Loss=5.6652\n          R¬≤=-0.0046, MAE=4.6334, RMSE=5.4547, LLL=-4.8650\n--------------------------------------------------\nEpoch 6: LR=1.00e-04\n          Train Loss=4.2969, Val Loss=4.6722\n          R¬≤=0.0920, MAE=4.3761, RMSE=5.1858, LLL=-4.6124\nüéØ NEW BEST! R¬≤: 0.0920\n--------------------------------------------------\nEpoch 7: LR=1.00e-04\n          Train Loss=4.2243, Val Loss=4.4240\n          R¬≤=-0.0467, MAE=4.4067, RMSE=5.5679, LLL=-4.6345\n--------------------------------------------------\nEpoch 8: LR=1.00e-04\n          Train Loss=4.1398, Val Loss=4.5532\n          R¬≤=0.1490, MAE=4.2817, RMSE=5.0203, LLL=-4.5300\nüéØ NEW BEST! R¬≤: 0.1490\n--------------------------------------------------\nEpoch 9: LR=1.00e-04\n          Train Loss=4.0416, Val Loss=4.5928\n          R¬≤=0.0041, MAE=4.3189, RMSE=5.4311, LLL=-4.5532\n--------------------------------------------------\nEpoch 10: LR=1.00e-04\n          Train Loss=3.9648, Val Loss=3.8625\n          R¬≤=0.1722, MAE=3.9288, RMSE=4.9514, LLL=-4.2251\nüéØ NEW BEST! R¬≤: 0.1722\n--------------------------------------------------\nEpoch 11: LR=1.00e-04\n          Train Loss=3.9675, Val Loss=4.8344\n          R¬≤=-0.0246, MAE=4.4751, RMSE=5.5087, LLL=-4.6942\n--------------------------------------------------\nEpoch 12: LR=1.00e-04\n          Train Loss=3.8637, Val Loss=4.9173\n          R¬≤=-0.2303, MAE=5.0160, RMSE=6.0365, LLL=-5.1527\n--------------------------------------------------\nEpoch 13: LR=1.00e-04\n          Train Loss=3.8714, Val Loss=3.8410\n          R¬≤=0.3034, MAE=3.6161, RMSE=4.5423, LLL=-3.9521\nüéØ NEW BEST! R¬≤: 0.3034\n--------------------------------------------------\nEpoch 14: LR=1.00e-04\n          Train Loss=3.7126, Val Loss=5.1725\n          R¬≤=-0.2157, MAE=4.7060, RMSE=6.0004, LLL=-4.8874\n--------------------------------------------------\nEpoch 15: LR=1.00e-04\n          Train Loss=3.6792, Val Loss=4.1353\n          R¬≤=0.2485, MAE=3.9201, RMSE=4.7179, LLL=-4.2116\n--------------------------------------------------\nEpoch 16: LR=1.00e-04\n          Train Loss=3.6796, Val Loss=4.4973\n          R¬≤=0.0651, MAE=4.2112, RMSE=5.2621, LLL=-4.4632\n--------------------------------------------------\nEpoch 17: LR=1.00e-04\n          Train Loss=3.6192, Val Loss=5.5426\n          R¬≤=-0.1327, MAE=4.6501, RMSE=5.7921, LLL=-4.8391\n--------------------------------------------------\nEpoch 18: LR=1.00e-04\n          Train Loss=3.5177, Val Loss=4.7270\n          R¬≤=0.1236, MAE=4.1231, RMSE=5.0949, LLL=-4.3861\n--------------------------------------------------\nEpoch 19: LR=1.00e-04\n          Train Loss=3.5725, Val Loss=4.4456\n          R¬≤=0.1933, MAE=3.9232, RMSE=4.8879, LLL=-4.2135\n--------------------------------------------------\nEpoch 20: LR=5.00e-05\n          Train Loss=3.4421, Val Loss=4.3922\n          R¬≤=0.1498, MAE=4.1059, RMSE=5.0180, LLL=-4.3706\n--------------------------------------------------\nEpoch 21: LR=5.00e-05\n          Train Loss=3.3300, Val Loss=3.6632\n          R¬≤=0.2675, MAE=3.6921, RMSE=4.6579, LLL=-4.0149\n--------------------------------------------------\nEpoch 22: LR=5.00e-05\n          Train Loss=3.3626, Val Loss=4.9868\n          R¬≤=-0.1790, MAE=5.1875, RMSE=5.9091, LLL=-5.2997\n--------------------------------------------------\nEpoch 23: LR=5.00e-05\n          Train Loss=3.2588, Val Loss=4.4144\n          R¬≤=0.1817, MAE=3.9154, RMSE=4.9229, LLL=-4.2066\nEarly stopping at epoch 23\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.3034\nBest MAE = 3.6161\nBest LLL = -3.9521\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\")\nprint(\"=\" * 60)\nprint(f\"üì± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    \"\"\"Optimized tabular features - simpler but more effective\"\"\"\n    vector = []\n    \n    # Basic but effective features\n    age = df_row['Age']\n    vector.extend([\n        (age - 50) / 30,  # Centered age\n        age / 100,  # Scaled age\n    ])\n    \n    # Simple sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(1.0)\n    else:\n        vector.append(0.0)\n    \n    # Simple smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # FVC features\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,  # Normalized FVC\n            (fvc - 2500) / 1000,  # Centered FVC\n        ])\n    \n    # Percent predicted (approximate)\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        \n        # Approximate percent predicted FVC\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n            \n        vector.append(min(pp_fvc, 2.0))  # Cap at 200%\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"Calculate Log Laplace Likelihood\"\"\"\n    sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n\n# Improved coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating optimized linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) >= 2:\n        try:\n            # Simple robust slope calculation\n            if len(weeks) == 2:\n                slope = (fvc[1] - fvc[0]) / (weeks[1] - weeks[0])\n            else:\n                # Use Theil-Sen estimator for robustness\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i+1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slope = (fvc[j] - fvc[i]) / (weeks[j] - weeks[i])\n                            slopes.append(slope)\n                slope = np.median(slopes) if slopes else 0.0\n            \n            A[patient] = slope\n        except:\n            A[patient] = 0.0\n    else:\n        A[patient] = 0.0\n    \n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with optimized features\")\n\n# Analyze target distribution\ndecay_values = np.array(list(A.values()))\nprint(f\"Target statistics: mean={decay_values.mean():.4f}, std={decay_values.std():.4f}\")\nprint(f\"Target range: [{decay_values.min():.4f}, {decay_values.max():.4f}]\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(OptimizedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100  # Only unfreeze later layers\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Simple but effective tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Feature fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain output\n        )\n        \n        # Initialize output layers for better convergence\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.mean_head, self.log_var_head]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (clipped to reasonable range)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            img = cv2.resize(img, (384, 384))\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Combined loss\n                mse_loss = F.mse_loss(mean_pred, targets)\n                uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                \n                # Start with more MSE focus, transition to uncertainty\n                if epoch < 20:\n                    loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                else:\n                    loss = 0.3 * mse_loss + 0.7 * uncertainty_loss\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Validation - FIXED: Handle scalar predictions properly\n            self.model.eval()\n            val_predictions, val_targets, val_log_vars = [], [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Convert to numpy properly (handle both scalar and tensor cases)\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    \n                    # Ensure we have arrays, not scalars\n                    if mean_pred_np.ndim == 0:  # scalar\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:  # array\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            \n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate metrics\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}, Loss={avg_train_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, LLL={avg_lll:.4f}\")\n                \n                # Update scheduler\n                scheduler.step(r2)\n                \n                # Save best model\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_MAE_5_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                \n                print(\"-\" * 50)\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    \n    # Simple stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    \n    # Data loaders - ensure batch size > 1 to avoid scalar issues\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Initialize model\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test forward pass\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        \n        print(f\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    \n    # Train model\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    \n    print(f\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    \n    return best_r2, best_mae, best_lll\n\nif __name__ == \"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:02:30.913215Z","iopub.execute_input":"2025-10-14T11:02:30.913743Z","iopub.status.idle":"2025-10-14T11:05:38.742070Z","shell.execute_reply.started":"2025-10-14T11:02:30.913715Z","shell.execute_reply":"2025-10-14T11:05:38.741302Z"}},"outputs":[{"name":"stdout","text":"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating optimized linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1214.66it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with optimized features\nTarget statistics: mean=-4.8107, std=6.7150\nTarget range: [-39.0741, 11.1389]\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04, Loss=44.4437\n          R¬≤=0.0121, MAE=4.4374, LLL=-5.0033\nüéØ NEW BEST! R¬≤: 0.0121\n--------------------------------------------------\nEpoch 2: LR=1.00e-04, Loss=31.7872\n          R¬≤=0.1668, MAE=4.1306, LLL=-4.7528\nüéØ NEW BEST! R¬≤: 0.1668\n--------------------------------------------------\nEpoch 3: LR=1.00e-04, Loss=30.2944\n          R¬≤=-0.1253, MAE=4.8788, LLL=-5.2693\n--------------------------------------------------\nEpoch 4: LR=1.00e-04, Loss=27.9926\n          R¬≤=-0.0448, MAE=4.7313, LLL=-5.1201\n--------------------------------------------------\nEpoch 5: LR=1.00e-04, Loss=25.7185\n          R¬≤=-0.3540, MAE=5.0493, LLL=-5.3283\n--------------------------------------------------\nEpoch 6: LR=1.00e-04, Loss=25.4835\n          R¬≤=0.0943, MAE=4.6970, LLL=-5.0098\n--------------------------------------------------\nEpoch 7: LR=1.00e-04, Loss=23.1125\n          R¬≤=0.1286, MAE=4.1088, LLL=-4.4740\n--------------------------------------------------\nEpoch 8: LR=1.00e-04, Loss=22.1952\n          R¬≤=-0.2523, MAE=5.1333, LLL=-5.3151\n--------------------------------------------------\nEpoch 9: LR=5.00e-05, Loss=19.8299\n          R¬≤=-0.0454, MAE=4.9322, LLL=-5.1936\n--------------------------------------------------\nEpoch 10: LR=5.00e-05, Loss=19.4914\n          R¬≤=0.1580, MAE=4.2900, LLL=-4.5825\n--------------------------------------------------\nEpoch 11: LR=5.00e-05, Loss=17.8395\n          R¬≤=-0.6992, MAE=5.3868, LLL=-5.5286\n--------------------------------------------------\nEpoch 12: LR=5.00e-05, Loss=19.7673\n          R¬≤=-0.5082, MAE=5.0837, LLL=-5.2443\nEarly stopping at epoch 12\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.1668\nBest MAE = 4.1306\nBest LLL = -4.7528\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# LLL\nimport os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ Optimized OSIC Model - LLL as Main Loss\")\nprint(\"=\"*60)\nprint(f\"üì± Device: {DEVICE}\")\n\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([\n        (age - 50)/30,\n        age / 100,\n    ])\n    vector.append(1.0 if df_row['Sex']=='Male' else 0.0)\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1,0,0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0,1,0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0,0,1])\n    else:\n        vector.extend([0,0,0])\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,\n            (fvc - 2500)/1000,\n        ])\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112*age) if age>0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101*age) if age>0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    return np.array(vector)\n\ndef calculate_lll_loss(mean_pred, log_var, targets):\n    # Numerically stable programmatic LLL negative for loss minimization\n    var = torch.exp(log_var)\n    delta = torch.abs(mean_pred - targets)\n    lll = - ( - torch.sqrt(torch.tensor(2.0)) * delta / (var.sqrt() + 1e-6) - torch.log(var.sqrt() * torch.sqrt(torch.tensor(2.0))) )\n    return lll.mean()\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2)*delta/sigma - np.log(sigma*np.sqrt(2))\n\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating decays ...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient']==patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) >=2:\n        try:\n            if len(weeks)==2:\n                slope = (fvc[1]-fvc[0])/(weeks[1]-weeks[0])\n            else:\n                slopes=[]\n                for i in range(len(weeks)):\n                    for j in range(i+1,len(weeks)):\n                        if weeks[j]!=weeks[i]:\n                            slopes.append((fvc[j]-fvc[i])/(weeks[j]-weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            A[patient] = slope\n        except:\n            A[patient]=0.0\n    else:\n        A[patient]=0.0\n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients.\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10,p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0,20.0), p=0.3),\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        for i,param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,64), nn.ReLU(),\n            nn.Linear(64,1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256,32), nn.ReLU(),\n            nn.Linear(32,1), nn.Tanh()\n        )\n        self._initialize_weights()\n    def _initialize_weights(self):\n        for m in [self.mean_head,self.log_var_head]:\n            if isinstance(m,nn.Linear):\n                nn.init.normal_(m.weight,0,0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias,0.0)\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(b,-1)\n        tab_features = self.tabular_processor(tabular)\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused)\n        log_var = self.log_var_head(fused)\n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184','ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower()=='.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split=='train':\n            return len(self.valid_patients)*8\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        if self.split=='train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n        patient = self.valid_patients[patient_idx]\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_features, target, patient\n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape)==3:\n                img = img[img.shape[0]//2]\n            img = cv2.resize(img,(384,384))\n            img_min,img_max = img.min(), img.max()\n            if img_max>img_min:\n                img = (img-img_min)/(img_max-img_min)*255\n            else:\n                img = np.zeros_like(img)\n            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img,img,img],axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384,384,3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                # Use negative LLL as loss\n                var = torch.exp(log_var)\n                delta = torch.abs(mean_pred - targets)\n                # Calculate negative log likelihood loss (Laplace)\n                loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                loss = loss.mean()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                train_batches += 1\n            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n            self.model.eval()\n            val_loss_sum = 0.0\n            val_batches = 0\n            val_predictions, val_targets, val_log_vars = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    var = torch.exp(log_var)\n                    delta = torch.abs(mean_pred - targets)\n                    val_loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                    val_loss = val_loss.mean()\n                    val_loss_sum += val_loss.item()\n                    val_batches += 1\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    if mean_pred_np.ndim == 0:\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else 0\n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                rmse = np.sqrt(np.mean((val_pred_np - val_target_np) ** 2))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}\")\n                print(f\"          Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, LLL={avg_lll:.4f}\")\n                scheduler.step(r2)\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_LLL_5_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                print(\"-\"*50)\n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        print(\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    print(\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    return best_r2, best_mae, best_lll\n\nif __name__==\"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:05:38.743710Z","iopub.execute_input":"2025-10-14T11:05:38.743921Z","iopub.status.idle":"2025-10-14T11:13:48.011220Z","shell.execute_reply.started":"2025-10-14T11:05:38.743899Z","shell.execute_reply":"2025-10-14T11:13:48.010407Z"}},"outputs":[{"name":"stdout","text":"üöÄ Optimized OSIC Model - LLL as Main Loss\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating decays ...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1174.96it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04\n          Train Loss=5.8683, Val Loss=4.6989\n          R¬≤=-0.0900, MAE=4.6104, RMSE=5.6818, LLL=-5.0058\nüéØ NEW BEST! R¬≤: -0.0900\n--------------------------------------------------\nEpoch 2: LR=1.00e-04\n          Train Loss=4.6390, Val Loss=5.5061\n          R¬≤=-0.1185, MAE=4.8148, RMSE=5.7556, LLL=-5.0280\n--------------------------------------------------\nEpoch 3: LR=1.00e-04\n          Train Loss=4.5449, Val Loss=4.4854\n          R¬≤=0.0720, MAE=4.3675, RMSE=5.2426, LLL=-4.6447\nüéØ NEW BEST! R¬≤: 0.0720\n--------------------------------------------------\nEpoch 4: LR=1.00e-04\n          Train Loss=4.4048, Val Loss=5.3072\n          R¬≤=-0.2498, MAE=4.8202, RMSE=6.0841, LLL=-5.0117\n--------------------------------------------------\nEpoch 5: LR=1.00e-04\n          Train Loss=4.3830, Val Loss=4.6219\n          R¬≤=0.0019, MAE=4.5726, RMSE=5.4370, LLL=-4.8160\n--------------------------------------------------\nEpoch 6: LR=1.00e-04\n          Train Loss=4.2242, Val Loss=4.8708\n          R¬≤=0.1292, MAE=4.4153, RMSE=5.0785, LLL=-4.6547\nüéØ NEW BEST! R¬≤: 0.1292\n--------------------------------------------------\nEpoch 7: LR=1.00e-04\n          Train Loss=4.1399, Val Loss=4.7200\n          R¬≤=-0.1450, MAE=4.7854, RMSE=5.8235, LLL=-4.9596\n--------------------------------------------------\nEpoch 8: LR=1.00e-04\n          Train Loss=4.0862, Val Loss=5.0192\n          R¬≤=-0.0866, MAE=4.4039, RMSE=5.6730, LLL=-4.6267\n--------------------------------------------------\nEpoch 9: LR=1.00e-04\n          Train Loss=4.0528, Val Loss=5.0705\n          R¬≤=-0.0807, MAE=4.6576, RMSE=5.6576, LLL=-4.8543\n--------------------------------------------------\nEpoch 10: LR=1.00e-04\n          Train Loss=3.9497, Val Loss=5.1652\n          R¬≤=-0.1750, MAE=4.8576, RMSE=5.8992, LLL=-5.0220\n--------------------------------------------------\nEpoch 11: LR=1.00e-04\n          Train Loss=3.9132, Val Loss=4.9479\n          R¬≤=-0.1269, MAE=4.9717, RMSE=5.7771, LLL=-5.1196\n--------------------------------------------------\nEpoch 12: LR=1.00e-04\n          Train Loss=3.8148, Val Loss=4.5595\n          R¬≤=-0.0578, MAE=4.6891, RMSE=5.5973, LLL=-4.8786\n--------------------------------------------------\nEpoch 13: LR=5.00e-05\n          Train Loss=3.7513, Val Loss=4.2189\n          R¬≤=0.2536, MAE=3.8951, RMSE=4.7017, LLL=-4.2025\nüéØ NEW BEST! R¬≤: 0.2536\n--------------------------------------------------\nEpoch 14: LR=5.00e-05\n          Train Loss=3.5957, Val Loss=5.0862\n          R¬≤=0.1087, MAE=4.2871, RMSE=5.1378, LLL=-4.5337\n--------------------------------------------------\nEpoch 15: LR=5.00e-05\n          Train Loss=3.5574, Val Loss=4.8548\n          R¬≤=0.1197, MAE=4.2171, RMSE=5.1061, LLL=-4.4712\n--------------------------------------------------\nEpoch 16: LR=5.00e-05\n          Train Loss=3.5614, Val Loss=4.4618\n          R¬≤=-0.0256, MAE=4.5653, RMSE=5.5113, LLL=-4.7672\n--------------------------------------------------\nEpoch 17: LR=5.00e-05\n          Train Loss=3.4739, Val Loss=5.0068\n          R¬≤=0.1200, MAE=4.0537, RMSE=5.1052, LLL=-4.3321\n--------------------------------------------------\nEpoch 18: LR=5.00e-05\n          Train Loss=3.4263, Val Loss=5.1086\n          R¬≤=0.1498, MAE=4.1732, RMSE=5.0179, LLL=-4.4346\n--------------------------------------------------\nEpoch 19: LR=5.00e-05\n          Train Loss=3.4924, Val Loss=4.6490\n          R¬≤=0.1429, MAE=4.2265, RMSE=5.0384, LLL=-4.4791\n--------------------------------------------------\nEpoch 20: LR=2.50e-05\n          Train Loss=3.3680, Val Loss=4.5557\n          R¬≤=0.1147, MAE=4.1987, RMSE=5.1204, LLL=-4.4522\n--------------------------------------------------\nEpoch 21: LR=2.50e-05\n          Train Loss=3.2805, Val Loss=4.0094\n          R¬≤=0.4331, MAE=3.3933, RMSE=4.0977, LLL=-3.7645\nüéØ NEW BEST! R¬≤: 0.4331\n--------------------------------------------------\nEpoch 22: LR=2.50e-05\n          Train Loss=3.2694, Val Loss=4.8704\n          R¬≤=0.1716, MAE=4.3729, RMSE=4.9533, LLL=-4.6032\n--------------------------------------------------\nEpoch 23: LR=2.50e-05\n          Train Loss=3.2369, Val Loss=5.6679\n          R¬≤=0.0146, MAE=4.4634, RMSE=5.4023, LLL=-4.6812\n--------------------------------------------------\nEpoch 24: LR=2.50e-05\n          Train Loss=3.2156, Val Loss=4.6497\n          R¬≤=0.2390, MAE=3.9219, RMSE=4.7476, LLL=-4.2174\n--------------------------------------------------\nEpoch 25: LR=2.50e-05\n          Train Loss=3.2635, Val Loss=4.9035\n          R¬≤=-0.0995, MAE=4.4187, RMSE=5.7064, LLL=-4.6421\n--------------------------------------------------\nEpoch 26: LR=2.50e-05\n          Train Loss=3.2277, Val Loss=5.2447\n          R¬≤=0.0970, MAE=4.3077, RMSE=5.1714, LLL=-4.5453\n--------------------------------------------------\nEpoch 27: LR=2.50e-05\n          Train Loss=3.2713, Val Loss=5.9117\n          R¬≤=-0.1525, MAE=4.8131, RMSE=5.8424, LLL=-4.9773\n--------------------------------------------------\nEpoch 28: LR=1.25e-05\n          Train Loss=3.1770, Val Loss=4.0577\n          R¬≤=0.2121, MAE=3.9716, RMSE=4.8305, LLL=-4.2588\n--------------------------------------------------\nEpoch 29: LR=1.25e-05\n          Train Loss=3.1760, Val Loss=4.5063\n          R¬≤=0.0954, MAE=4.2071, RMSE=5.1761, LLL=-4.4611\n--------------------------------------------------\nEpoch 30: LR=1.25e-05\n          Train Loss=3.0665, Val Loss=4.3715\n          R¬≤=0.2682, MAE=4.0886, RMSE=4.6555, LLL=-4.3590\n--------------------------------------------------\nEpoch 31: LR=1.25e-05\n          Train Loss=3.1138, Val Loss=4.7273\n          R¬≤=0.1473, MAE=4.0843, RMSE=5.0254, LLL=-4.3553\nEarly stopping at epoch 31\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.4331\nBest MAE = 3.3933\nBest LLL = -3.7645\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\")\nprint(\"=\" * 60)\nprint(f\"üì± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    \"\"\"Optimized tabular features - simpler but more effective\"\"\"\n    vector = []\n    \n    # Basic but effective features\n    age = df_row['Age']\n    vector.extend([\n        (age - 50) / 30,  # Centered age\n        age / 100,  # Scaled age\n    ])\n    \n    # Simple sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(1.0)\n    else:\n        vector.append(0.0)\n    \n    # Simple smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # FVC features\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,  # Normalized FVC\n            (fvc - 2500) / 1000,  # Centered FVC\n        ])\n    \n    # Percent predicted (approximate)\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        \n        # Approximate percent predicted FVC\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n            \n        vector.append(min(pp_fvc, 2.0))  # Cap at 200%\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"Calculate Log Laplace Likelihood\"\"\"\n    sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n\n# Improved coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating optimized linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) >= 2:\n        try:\n            # Simple robust slope calculation\n            if len(weeks) == 2:\n                slope = (fvc[1] - fvc[0]) / (weeks[1] - weeks[0])\n            else:\n                # Use Theil-Sen estimator for robustness\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i+1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slope = (fvc[j] - fvc[i]) / (weeks[j] - weeks[i])\n                            slopes.append(slope)\n                slope = np.median(slopes) if slopes else 0.0\n            \n            A[patient] = slope\n        except:\n            A[patient] = 0.0\n    else:\n        A[patient] = 0.0\n    \n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with optimized features\")\n\n# Analyze target distribution\ndecay_values = np.array(list(A.values()))\nprint(f\"Target statistics: mean={decay_values.mean():.4f}, std={decay_values.std():.4f}\")\nprint(f\"Target range: [{decay_values.min():.4f}, {decay_values.max():.4f}]\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(OptimizedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100  # Only unfreeze later layers\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Simple but effective tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Feature fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain output\n        )\n        \n        # Initialize output layers for better convergence\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.mean_head, self.log_var_head]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (clipped to reasonable range)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            img = cv2.resize(img, (384, 384))\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Combined loss\n                mse_loss = F.mse_loss(mean_pred, targets)\n                uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                \n                # Start with more MSE focus, transition to uncertainty\n                if epoch < 20:\n                    loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                else:\n                    loss = 0.3 * mse_loss + 0.7 * uncertainty_loss\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Validation - FIXED: Handle scalar predictions properly\n            self.model.eval()\n            val_predictions, val_targets, val_log_vars = [], [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Convert to numpy properly (handle both scalar and tensor cases)\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    \n                    # Ensure we have arrays, not scalars\n                    if mean_pred_np.ndim == 0:  # scalar\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:  # array\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            \n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate metrics\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}, Loss={avg_train_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, LLL={avg_lll:.4f}\")\n                \n                # Update scheduler\n                scheduler.step(r2)\n                \n                # Save best model\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_MAE_6_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                \n                print(\"-\" * 50)\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    \n    # Simple stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    \n    # Data loaders - ensure batch size > 1 to avoid scalar issues\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Initialize model\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test forward pass\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        \n        print(f\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    \n    # Train model\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    \n    print(f\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    \n    return best_r2, best_mae, best_lll\n\nif __name__ == \"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:13:48.013113Z","iopub.execute_input":"2025-10-14T11:13:48.013374Z","iopub.status.idle":"2025-10-14T11:19:48.065689Z","shell.execute_reply.started":"2025-10-14T11:13:48.013348Z","shell.execute_reply":"2025-10-14T11:19:48.064877Z"}},"outputs":[{"name":"stdout","text":"üöÄ OPTIMIZED OSIC Model - Targeting R¬≤ > 0.5\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating optimized linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1163.68it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with optimized features\nTarget statistics: mean=-4.8107, std=6.7150\nTarget range: [-39.0741, 11.1389]\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04, Loss=44.4466\n          R¬≤=-0.0879, MAE=4.7355, LLL=-5.4512\nüéØ NEW BEST! R¬≤: -0.0879\n--------------------------------------------------\nEpoch 2: LR=1.00e-04, Loss=31.6251\n          R¬≤=0.0341, MAE=4.4529, LLL=-5.0508\nüéØ NEW BEST! R¬≤: 0.0341\n--------------------------------------------------\nEpoch 3: LR=1.00e-04, Loss=30.3141\n          R¬≤=0.0676, MAE=4.0962, LLL=-4.4645\nüéØ NEW BEST! R¬≤: 0.0676\n--------------------------------------------------\nEpoch 4: LR=1.00e-04, Loss=28.8795\n          R¬≤=-0.1188, MAE=4.9008, LLL=-5.4449\n--------------------------------------------------\nEpoch 5: LR=1.00e-04, Loss=26.3504\n          R¬≤=0.1368, MAE=4.1628, LLL=-4.6197\nüéØ NEW BEST! R¬≤: 0.1368\n--------------------------------------------------\nEpoch 6: LR=1.00e-04, Loss=23.6187\n          R¬≤=-0.3164, MAE=4.8404, LLL=-5.0773\n--------------------------------------------------\nEpoch 7: LR=1.00e-04, Loss=23.5852\n          R¬≤=-0.1820, MAE=4.9847, LLL=-5.3387\n--------------------------------------------------\nEpoch 8: LR=1.00e-04, Loss=23.2749\n          R¬≤=-0.6902, MAE=5.8079, LLL=-5.9496\n--------------------------------------------------\nEpoch 9: LR=1.00e-04, Loss=21.7599\n          R¬≤=0.0052, MAE=4.3910, LLL=-4.6630\n--------------------------------------------------\nEpoch 10: LR=1.00e-04, Loss=20.7933\n          R¬≤=0.1594, MAE=4.0934, LLL=-4.3832\nüéØ NEW BEST! R¬≤: 0.1594\n--------------------------------------------------\nEpoch 11: LR=1.00e-04, Loss=19.4025\n          R¬≤=-0.3083, MAE=5.1816, LLL=-5.3498\n--------------------------------------------------\nEpoch 12: LR=1.00e-04, Loss=19.3403\n          R¬≤=-0.2395, MAE=4.8799, LLL=-5.0472\n--------------------------------------------------\nEpoch 13: LR=1.00e-04, Loss=19.4564\n          R¬≤=0.3093, MAE=3.7938, LLL=-4.1159\nüéØ NEW BEST! R¬≤: 0.3093\n--------------------------------------------------\nEpoch 14: LR=1.00e-04, Loss=17.6277\n          R¬≤=0.1344, MAE=4.2387, LLL=-4.5031\n--------------------------------------------------\nEpoch 15: LR=1.00e-04, Loss=16.4257\n          R¬≤=0.0735, MAE=4.6670, LLL=-4.8540\n--------------------------------------------------\nEpoch 16: LR=1.00e-04, Loss=15.8074\n          R¬≤=-0.0579, MAE=4.7505, LLL=-4.9686\n--------------------------------------------------\nEpoch 17: LR=1.00e-04, Loss=16.4557\n          R¬≤=-0.2297, MAE=5.2795, LLL=-5.3853\n--------------------------------------------------\nEpoch 18: LR=1.00e-04, Loss=15.1317\n          R¬≤=0.0946, MAE=4.2520, LLL=-4.5294\n--------------------------------------------------\nEpoch 19: LR=1.00e-04, Loss=16.1214\n          R¬≤=-0.1602, MAE=4.8386, LLL=-5.0044\n--------------------------------------------------\nEpoch 20: LR=5.00e-05, Loss=13.8324\n          R¬≤=-0.3185, MAE=5.0784, LLL=-5.2310\n--------------------------------------------------\nEpoch 21: LR=5.00e-05, Loss=7.4635\n          R¬≤=0.1679, MAE=4.0467, LLL=-4.3364\n--------------------------------------------------\nEpoch 22: LR=5.00e-05, Loss=7.1439\n          R¬≤=0.1073, MAE=4.6018, LLL=-4.8112\n--------------------------------------------------\nEpoch 23: LR=5.00e-05, Loss=7.3737\n          R¬≤=0.0680, MAE=4.2934, LLL=-4.5404\nEarly stopping at epoch 23\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.3093\nBest MAE = 3.7938\nBest LLL = -4.1159\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# LLL\nimport os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"üöÄ Optimized OSIC Model - LLL as Main Loss\")\nprint(\"=\"*60)\nprint(f\"üì± Device: {DEVICE}\")\n\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([\n        (age - 50)/30,\n        age / 100,\n    ])\n    vector.append(1.0 if df_row['Sex']=='Male' else 0.0)\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1,0,0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0,1,0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0,0,1])\n    else:\n        vector.extend([0,0,0])\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,\n            (fvc - 2500)/1000,\n        ])\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112*age) if age>0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101*age) if age>0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    return np.array(vector)\n\ndef calculate_lll_loss(mean_pred, log_var, targets):\n    # Numerically stable programmatic LLL negative for loss minimization\n    var = torch.exp(log_var)\n    delta = torch.abs(mean_pred - targets)\n    lll = - ( - torch.sqrt(torch.tensor(2.0)) * delta / (var.sqrt() + 1e-6) - torch.log(var.sqrt() * torch.sqrt(torch.tensor(2.0))) )\n    return lll.mean()\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2)*delta/sigma - np.log(sigma*np.sqrt(2))\n\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating decays ...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient']==patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) >=2:\n        try:\n            if len(weeks)==2:\n                slope = (fvc[1]-fvc[0])/(weeks[1]-weeks[0])\n            else:\n                slopes=[]\n                for i in range(len(weeks)):\n                    for j in range(i+1,len(weeks)):\n                        if weeks[j]!=weeks[i]:\n                            slopes.append((fvc[j]-fvc[i])/(weeks[j]-weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            A[patient] = slope\n        except:\n            A[patient]=0.0\n    else:\n        A[patient]=0.0\n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients.\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10,p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0,20.0), p=0.3),\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        for i,param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,64), nn.ReLU(),\n            nn.Linear(64,1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256,32), nn.ReLU(),\n            nn.Linear(32,1), nn.Tanh()\n        )\n        self._initialize_weights()\n    def _initialize_weights(self):\n        for m in [self.mean_head,self.log_var_head]:\n            if isinstance(m,nn.Linear):\n                nn.init.normal_(m.weight,0,0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias,0.0)\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(b,-1)\n        tab_features = self.tabular_processor(tabular)\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused)\n        log_var = self.log_var_head(fused)\n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184','ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower()=='.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split=='train':\n            return len(self.valid_patients)*8\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        if self.split=='train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n        patient = self.valid_patients[patient_idx]\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_features, target, patient\n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape)==3:\n                img = img[img.shape[0]//2]\n            img = cv2.resize(img,(384,384))\n            img_min,img_max = img.min(), img.max()\n            if img_max>img_min:\n                img = (img-img_min)/(img_max-img_min)*255\n            else:\n                img = np.zeros_like(img)\n            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img,img,img],axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384,384,3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                # Use negative LLL as loss\n                var = torch.exp(log_var)\n                delta = torch.abs(mean_pred - targets)\n                # Calculate negative log likelihood loss (Laplace)\n                loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                loss = loss.mean()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                train_batches += 1\n            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n            self.model.eval()\n            val_loss_sum = 0.0\n            val_batches = 0\n            val_predictions, val_targets, val_log_vars = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    var = torch.exp(log_var)\n                    delta = torch.abs(mean_pred - targets)\n                    val_loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                    val_loss = val_loss.mean()\n                    val_loss_sum += val_loss.item()\n                    val_batches += 1\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    if mean_pred_np.ndim == 0:\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else 0\n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                rmse = np.sqrt(np.mean((val_pred_np - val_target_np) ** 2))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}\")\n                print(f\"          Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n                print(f\"          R¬≤={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, LLL={avg_lll:.4f}\")\n                scheduler.step(r2)\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'Oct_14_best_LLL_6_optimized_model.pth')\n                    print(f\"üéØ NEW BEST! R¬≤: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                print(\"-\"*50)\n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"üîÑ Creating optimized data loaders...\")\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        print(\"‚úÖ Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"üíæ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        return\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    print(\"\\nüî• FINAL RESULTS:\")\n    print(f\"Best R¬≤ = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    return best_r2, best_mae, best_lll\n\nif __name__==\"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:19:48.068846Z","iopub.execute_input":"2025-10-14T11:19:48.069079Z","iopub.status.idle":"2025-10-14T11:28:00.413491Z","shell.execute_reply.started":"2025-10-14T11:19:48.069058Z","shell.execute_reply":"2025-10-14T11:28:00.412479Z"}},"outputs":[{"name":"stdout","text":"üöÄ Optimized OSIC Model - LLL as Main Loss\n============================================================\nüì± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating decays ...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 1176.32it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nüîÑ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nüìä Model parameters: 7,827,138\n‚úÖ Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nüíæ GPU memory: 0.11 GB\nEpoch 1: LR=1.00e-04\n          Train Loss=5.8783, Val Loss=4.8014\n          R¬≤=-0.0665, MAE=4.5679, RMSE=5.6201, LLL=-4.8577\nüéØ NEW BEST! R¬≤: -0.0665\n--------------------------------------------------\nEpoch 2: LR=1.00e-04\n          Train Loss=4.7018, Val Loss=4.6248\n          R¬≤=0.1047, MAE=4.2391, RMSE=5.1495, LLL=-4.6033\nüéØ NEW BEST! R¬≤: 0.1047\n--------------------------------------------------\nEpoch 3: LR=1.00e-04\n          Train Loss=4.5227, Val Loss=4.0017\n          R¬≤=0.1232, MAE=4.0894, RMSE=5.0958, LLL=-4.4151\nüéØ NEW BEST! R¬≤: 0.1232\n--------------------------------------------------\nEpoch 4: LR=1.00e-04\n          Train Loss=4.4788, Val Loss=4.4055\n          R¬≤=0.1043, MAE=4.4388, RMSE=5.1505, LLL=-4.6828\n--------------------------------------------------\nEpoch 5: LR=1.00e-04\n          Train Loss=4.4111, Val Loss=4.8372\n          R¬≤=0.1518, MAE=4.0624, RMSE=5.0122, LLL=-4.3654\nüéØ NEW BEST! R¬≤: 0.1518\n--------------------------------------------------\nEpoch 6: LR=1.00e-04\n          Train Loss=4.2549, Val Loss=4.7694\n          R¬≤=0.0465, MAE=4.6319, RMSE=5.3142, LLL=-4.8401\n--------------------------------------------------\nEpoch 7: LR=1.00e-04\n          Train Loss=4.2205, Val Loss=3.9851\n          R¬≤=0.1400, MAE=4.0696, RMSE=5.0470, LLL=-4.3458\n--------------------------------------------------\nEpoch 8: LR=1.00e-04\n          Train Loss=4.1032, Val Loss=4.6429\n          R¬≤=-0.0335, MAE=4.6766, RMSE=5.5325, LLL=-4.8760\n--------------------------------------------------\nEpoch 9: LR=1.00e-04\n          Train Loss=4.1091, Val Loss=3.8627\n          R¬≤=0.2561, MAE=3.9363, RMSE=4.6940, LLL=-4.2337\nüéØ NEW BEST! R¬≤: 0.2561\n--------------------------------------------------\nEpoch 10: LR=1.00e-04\n          Train Loss=4.0197, Val Loss=4.4939\n          R¬≤=0.0579, MAE=4.4540, RMSE=5.2822, LLL=-4.6818\n--------------------------------------------------\nEpoch 11: LR=1.00e-04\n          Train Loss=3.9590, Val Loss=4.6888\n          R¬≤=0.1288, MAE=4.3334, RMSE=5.0798, LLL=-4.5714\n--------------------------------------------------\nEpoch 12: LR=1.00e-04\n          Train Loss=3.8488, Val Loss=5.2498\n          R¬≤=-0.0754, MAE=4.8825, RMSE=5.6436, LLL=-5.0372\n--------------------------------------------------\nEpoch 13: LR=1.00e-04\n          Train Loss=3.8244, Val Loss=4.5665\n          R¬≤=-0.0236, MAE=4.7080, RMSE=5.5059, LLL=-4.8946\n--------------------------------------------------\nEpoch 14: LR=1.00e-04\n          Train Loss=3.7439, Val Loss=4.4771\n          R¬≤=0.0394, MAE=4.2944, RMSE=5.3340, LLL=-4.5385\n--------------------------------------------------\nEpoch 15: LR=1.00e-04\n          Train Loss=3.6238, Val Loss=4.7373\n          R¬≤=0.0726, MAE=4.4270, RMSE=5.2409, LLL=-4.6481\n--------------------------------------------------\nEpoch 16: LR=5.00e-05\n          Train Loss=3.5693, Val Loss=4.3921\n          R¬≤=0.0683, MAE=4.3882, RMSE=5.2529, LLL=-4.6171\n--------------------------------------------------\nEpoch 17: LR=5.00e-05\n          Train Loss=3.4897, Val Loss=5.4490\n          R¬≤=-0.3532, MAE=5.1006, RMSE=6.3308, LLL=-5.2243\n--------------------------------------------------\nEpoch 18: LR=5.00e-05\n          Train Loss=3.5337, Val Loss=4.5793\n          R¬≤=0.0944, MAE=4.3256, RMSE=5.1789, LLL=-4.5609\n--------------------------------------------------\nEpoch 19: LR=5.00e-05\n          Train Loss=3.3984, Val Loss=3.8960\n          R¬≤=0.2566, MAE=3.9003, RMSE=4.6922, LLL=-4.1957\nüéØ NEW BEST! R¬≤: 0.2566\n--------------------------------------------------\nEpoch 20: LR=5.00e-05\n          Train Loss=3.4184, Val Loss=4.0215\n          R¬≤=-0.0287, MAE=4.3480, RMSE=5.5198, LLL=-4.5788\n--------------------------------------------------\nEpoch 21: LR=5.00e-05\n          Train Loss=3.3772, Val Loss=4.0234\n          R¬≤=0.2970, MAE=3.7972, RMSE=4.5630, LLL=-4.1074\nüéØ NEW BEST! R¬≤: 0.2970\n--------------------------------------------------\nEpoch 22: LR=5.00e-05\n          Train Loss=3.3793, Val Loss=4.8199\n          R¬≤=-0.0627, MAE=4.7916, RMSE=5.6102, LLL=-4.9589\n--------------------------------------------------\nEpoch 23: LR=5.00e-05\n          Train Loss=3.3052, Val Loss=5.1048\n          R¬≤=-0.0038, MAE=4.2902, RMSE=5.4526, LLL=-4.5286\n--------------------------------------------------\nEpoch 24: LR=5.00e-05\n          Train Loss=3.2727, Val Loss=4.6424\n          R¬≤=0.0873, MAE=4.5031, RMSE=5.1992, LLL=-4.7140\n--------------------------------------------------\nEpoch 25: LR=5.00e-05\n          Train Loss=3.2964, Val Loss=5.4112\n          R¬≤=-0.2751, MAE=5.2280, RMSE=6.1454, LLL=-5.3327\n--------------------------------------------------\nEpoch 26: LR=5.00e-05\n          Train Loss=3.2634, Val Loss=6.0935\n          R¬≤=-0.2976, MAE=5.2072, RMSE=6.1993, LLL=-5.3143\n--------------------------------------------------\nEpoch 27: LR=5.00e-05\n          Train Loss=3.2472, Val Loss=5.4519\n          R¬≤=-0.0758, MAE=5.0595, RMSE=5.6448, LLL=-5.1874\n--------------------------------------------------\nEpoch 28: LR=2.50e-05\n          Train Loss=3.1868, Val Loss=4.0206\n          R¬≤=0.2731, MAE=3.8357, RMSE=4.6401, LLL=-4.1377\n--------------------------------------------------\nEpoch 29: LR=2.50e-05\n          Train Loss=3.1789, Val Loss=4.5659\n          R¬≤=0.0344, MAE=4.2712, RMSE=5.3476, LLL=-4.5114\n--------------------------------------------------\nEpoch 30: LR=2.50e-05\n          Train Loss=3.0293, Val Loss=4.6408\n          R¬≤=-0.0273, MAE=4.6590, RMSE=5.5160, LLL=-4.8461\n--------------------------------------------------\nEpoch 31: LR=2.50e-05\n          Train Loss=3.0432, Val Loss=4.4216\n          R¬≤=0.1056, MAE=4.2202, RMSE=5.1469, LLL=-4.4684\nEarly stopping at epoch 31\n\nüî• FINAL RESULTS:\nBest R¬≤ = 0.2970\nBest MAE = 3.7972\nBest LLL = -4.1074\n","output_type":"stream"}],"execution_count":14}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DICOM compression support packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "print(\"Installing DICOM compression support...\")\n",
    "packages = [\n",
    "    'pylibjpeg',\n",
    "    'pylibjpeg-libjpeg', \n",
    "    'gdcm'\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    if install_package(pkg):\n",
    "        print(f\"✅ {pkg} installed\")\n",
    "    else:\n",
    "        print(f\"⚠️ {pkg} installation failed (may already be installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V2 Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from datetime import timedelta, datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import pickle\n",
    "import glob\n",
    "from math import sqrt, log\n",
    "\n",
    "# Image processing\n",
    "from skimage import measure, morphology, segmentation\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Albumentations for medical augmentations\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Pulmonary Fibrosis Progression Analysis - FIXED VERSION\")\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 1: DATA LOADING AND EDA\n",
    "# =============================================================================\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "try:\n",
    "    test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "    print(f'Train: {train_df.shape[0]} rows, Test: {test_df.shape[0]} rows')\n",
    "except:\n",
    "    print(f'Train: {train_df.shape[0]} rows, Test: file not found')\n",
    "    test_df = None\n",
    "\n",
    "print(\"\\nTrain data sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTrain data statistics:\")\n",
    "print(train_df.describe())\n",
    "\n",
    "# Basic EDA\n",
    "print(f'\\nUnique patients in training data: {train_df[\"Patient\"].nunique()}')\n",
    "print(f'Total observations: {len(train_df)}')\n",
    "print(f'Average observations per patient: {len(train_df)/train_df[\"Patient\"].nunique():.2f}')\n",
    "\n",
    "# Check for missing values\n",
    "print(f'\\nMissing values:')\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Basic visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Correlation matrix\n",
    "plt.subplot(2, 3, 1)\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# FVC distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(train_df['FVC'], bins=30, alpha=0.7)\n",
    "plt.title('FVC Distribution')\n",
    "plt.xlabel('FVC')\n",
    "\n",
    "# Age vs FVC\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(train_df['Age'], train_df['FVC'], alpha=0.6)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('FVC')\n",
    "plt.title('Age vs FVC')\n",
    "\n",
    "# Smoking status distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "train_df['SmokingStatus'].value_counts().plot(kind='bar')\n",
    "plt.title('Smoking Status Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Sex distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "train_df['Sex'].value_counts().plot(kind='bar')\n",
    "plt.title('Sex Distribution')\n",
    "\n",
    "# Weeks distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist(train_df['Weeks'], bins=30, alpha=0.7)\n",
    "plt.title('Weeks Distribution')\n",
    "plt.xlabel('Weeks')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample patient trajectories\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sample_patients = train_df['Patient'].unique()[:3]\n",
    "\n",
    "for i, patient in enumerate(sample_patients):\n",
    "    patient_data = train_df[train_df['Patient'] == patient]\n",
    "    axes[i].plot(patient_data['Weeks'], patient_data['FVC'], 'o-')\n",
    "    axes[i].set_title(f\"Patient: {patient[:10]}...\")\n",
    "    axes[i].set_xlabel('Weeks')\n",
    "    axes[i].set_ylabel('FVC')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 2: FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Calculating linear decay coefficients...\")\n",
    "\n",
    "A = {}  # Decay coefficients\n",
    "TAB = {}  # Tabular features\n",
    "P = []  # Patient list\n",
    "\n",
    "def get_tab_features(df_row):\n",
    "    \"\"\"Extract tabular features (returns 4 features)\"\"\"\n",
    "    vector = [(df_row['Age'] - 30) / 30] \n",
    "    \n",
    "    # Sex encoding\n",
    "    if df_row['Sex'] == 'Male':\n",
    "        vector.append(0)\n",
    "    else:\n",
    "        vector.append(1)\n",
    "    \n",
    "    # Smoking status encoding\n",
    "    smoking_status = df_row['SmokingStatus']\n",
    "    if smoking_status == 'Never smoked':\n",
    "        vector.extend([0, 0])\n",
    "    elif smoking_status == 'Ex-smoker':\n",
    "        vector.extend([1, 1])\n",
    "    elif smoking_status == 'Currently smokes':\n",
    "        vector.extend([0, 1])\n",
    "    else:\n",
    "        vector.extend([1, 0])\n",
    "    return np.array(vector, dtype=np.float32)\n",
    "\n",
    "# Calculate slopes for each patient\n",
    "decay_rates = []\n",
    "for patient in tqdm(train_df['Patient'].unique(), desc=\"Processing patients\"):\n",
    "    sub = train_df[train_df['Patient'] == patient].copy()\n",
    "    fvc = sub['FVC'].values\n",
    "    weeks = sub['Weeks'].values\n",
    "    \n",
    "    if len(weeks) > 1:\n",
    "        # Fit linear regression: FVC = a * weeks + b\n",
    "        c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
    "        try:\n",
    "            a, b = np.linalg.lstsq(c, fvc, rcond=None)[0]\n",
    "            A[patient] = float(a)  # Ensure float\n",
    "            TAB[patient] = get_tab_features(sub.iloc[0])\n",
    "            P.append(patient)\n",
    "            decay_rates.append(a)\n",
    "        except Exception as e:\n",
    "            print(f\"Linear fit failed for patient {patient}: {e}\")\n",
    "            # Fallback calculation\n",
    "            a = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0]) if len(weeks) > 1 else 0.0\n",
    "            A[patient] = float(a)\n",
    "            TAB[patient] = get_tab_features(sub.iloc[0])\n",
    "            P.append(patient)\n",
    "            decay_rates.append(a)\n",
    "    else:\n",
    "        # Single observation\n",
    "        A[patient] = 0.0\n",
    "        TAB[patient] = get_tab_features(sub.iloc[0])\n",
    "        P.append(patient)\n",
    "        decay_rates.append(0.0)\n",
    "\n",
    "print(f\"Processed {len(P)} patients with decay coefficients\")\n",
    "\n",
    "# Analyze decay rates\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(decay_rates, bins=30, alpha=0.7)\n",
    "plt.title(\"Distribution of FVC Decay Rates\")\n",
    "plt.xlabel(\"Decay Rate (FVC/Week)\")\n",
    "plt.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(decay_rates)\n",
    "plt.title(\"Decay Rate Box Plot\")\n",
    "plt.ylabel(\"Decay Rate (FVC/Week)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Decay rate statistics:\")\n",
    "print(f\"  Mean: {np.mean(decay_rates):.3f}\")\n",
    "print(f\"  Std: {np.std(decay_rates):.3f}\")\n",
    "print(f\"  Range: [{np.min(decay_rates):.3f}, {np.max(decay_rates):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 3: ENHANCED DICOM IMAGE PROCESSING WITH COMPRESSION HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "# Install required packages for DICOM compression handling\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_dicom_dependencies():\n",
    "    \"\"\"Install required dependencies for DICOM compression handling\"\"\"\n",
    "    packages = [\n",
    "        'pydicom[gdcm]',  # GDCM support for DICOM compression\n",
    "        'pylibjpeg',      # JPEG support\n",
    "        'pylibjpeg-libjpeg',  # JPEG Lossless support\n",
    "        'pillow-simd',    # Faster image processing\n",
    "        'opencv-python'   # OpenCV for image operations\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])\n",
    "            print(f\"✅ Installed: {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"⚠️ Failed to install: {package} (may already be installed)\")\n",
    "\n",
    "# Try to install dependencies\n",
    "print(\"Installing DICOM compression dependencies...\")\n",
    "install_dicom_dependencies()\n",
    "\n",
    "# Import with compression support\n",
    "try:\n",
    "    import gdcm  # GDCM for DICOM compression\n",
    "    print(\"✅ GDCM loaded successfully\")\n",
    "    GDCM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ GDCM not available - using fallback methods\")\n",
    "    GDCM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import pylibjpeg  # JPEG support\n",
    "    print(\"✅ PyLibJPEG loaded successfully\")\n",
    "    PYLIBJPEG_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ PyLibJPEG not available - using fallback methods\")\n",
    "    PYLIBJPEG_AVAILABLE = False\n",
    "\n",
    "def load_and_preprocess_dicom_enhanced(path, verbose=False):\n",
    "    \"\"\"Enhanced DICOM loading with compression handling and multiple fallback methods\"\"\"\n",
    "    try:\n",
    "        # Method 1: Try standard pydicom with auto decompression\n",
    "        try:\n",
    "            dcm = pydicom.dcmread(str(path), force=True)\n",
    "            \n",
    "            # Check if decompression is needed and handle \"already uncompressed\" case\n",
    "            if hasattr(dcm, 'decompress'):\n",
    "                try:\n",
    "                    dcm.decompress()\n",
    "                except RuntimeError as e:\n",
    "                    # \"The dataset is already uncompressed\" is actually a success case\n",
    "                    if \"already uncompressed\" not in str(e):\n",
    "                        raise e\n",
    "                    # Continue - the file is fine\n",
    "            \n",
    "            if hasattr(dcm, 'pixel_array'):\n",
    "                img = dcm.pixel_array.astype(np.float32)\n",
    "                return process_dicom_image(img, path, verbose=verbose)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Method 1 failed for {path}: {e}\")\n",
    "        \n",
    "        # Method 2: Try with specific transfer syntax handling\n",
    "        try:\n",
    "            dcm = pydicom.dcmread(str(path), force=True, defer_size=\"1 KB\")\n",
    "            \n",
    "            # Handle different transfer syntaxes\n",
    "            if hasattr(dcm, 'file_meta') and hasattr(dcm.file_meta, 'TransferSyntaxUID'):\n",
    "                ts = dcm.file_meta.TransferSyntaxUID\n",
    "                if verbose:\n",
    "                    print(f\"Transfer Syntax: {ts}\")\n",
    "                \n",
    "                # Force pixel array extraction\n",
    "                img = dcm.pixel_array.astype(np.float32)\n",
    "                return process_dicom_image(img, path, verbose=verbose)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Method 2 failed for {path}: {e}\")\n",
    "        \n",
    "        # Method 3: Try raw pixel data extraction\n",
    "        try:\n",
    "            dcm = pydicom.dcmread(str(path), force=True, stop_before_pixels=False)\n",
    "            \n",
    "            if hasattr(dcm, 'PixelData') and dcm.PixelData is not None:\n",
    "                # Try to extract raw pixel data\n",
    "                rows = dcm.Rows\n",
    "                cols = dcm.Columns\n",
    "                \n",
    "                # Attempt different pixel data interpretations\n",
    "                pixel_bytes = dcm.PixelData\n",
    "                \n",
    "                # Try as uint16 (most common for CT)\n",
    "                if len(pixel_bytes) >= rows * cols * 2:\n",
    "                    img = np.frombuffer(pixel_bytes[:rows*cols*2], dtype=np.uint16)\n",
    "                    img = img.reshape((rows, cols)).astype(np.float32)\n",
    "                    return process_dicom_image(img, path, verbose=verbose)\n",
    "                \n",
    "                # Try as uint8\n",
    "                elif len(pixel_bytes) >= rows * cols:\n",
    "                    img = np.frombuffer(pixel_bytes[:rows*cols], dtype=np.uint8)\n",
    "                    img = img.reshape((rows, cols)).astype(np.float32)\n",
    "                    return process_dicom_image(img, path, verbose=verbose)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Method 3 failed for {path}: {e}\")\n",
    "        \n",
    "        # Method 4: Try with PIL/OpenCV as fallback\n",
    "        try:\n",
    "            from PIL import Image\n",
    "            \n",
    "            # Try to open as regular image file (some DICOM can be read this way)\n",
    "            with Image.open(path) as pil_img:\n",
    "                img = np.array(pil_img.convert('L')).astype(np.float32)\n",
    "                return process_dicom_image(img, path, verbose=verbose)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Method 4 failed for {path}: {e}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"All methods failed for {path} - returning dummy image\")\n",
    "        return create_dummy_image()\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Critical error loading {path}: {e}\")\n",
    "        return create_dummy_image()\n",
    "\n",
    "def process_dicom_image(img, path, verbose=False):\n",
    "    \"\"\"Process DICOM image array into standardized format\"\"\"\n",
    "    try:\n",
    "        # Handle different image dimensions\n",
    "        if len(img.shape) == 3:\n",
    "            img = img[img.shape[0]//2]  # Take middle slice if 3D\n",
    "        elif len(img.shape) == 4:\n",
    "            img = img[0, img.shape[1]//2]  # Handle 4D case\n",
    "        \n",
    "        # Ensure 2D\n",
    "        if len(img.shape) != 2:\n",
    "            if verbose:\n",
    "                print(f\"Unexpected image shape {img.shape} for {path}\")\n",
    "            return create_dummy_image()\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = cv2.resize(img, (512, 512))\n",
    "        \n",
    "        # Handle Hounsfield Units (HU) for CT scans\n",
    "        # Typical HU range: -1000 (air) to +3000 (bone)\n",
    "        # Clip extreme values\n",
    "        img = np.clip(img, -1000, 3000)\n",
    "        \n",
    "        # Normalize to 0-255 with better handling\n",
    "        img_min, img_max = img.min(), img.max()\n",
    "        if img_max > img_min:\n",
    "            img = (img - img_min) / (img_max - img_min) * 255\n",
    "        else:\n",
    "            # Handle constant images\n",
    "            img = np.full_like(img, 128)  # Gray instead of black\n",
    "        \n",
    "        img = np.clip(img, 0, 255)\n",
    "        \n",
    "        # Convert to 3-channel RGB\n",
    "        img = np.stack([img, img, img], axis=2).astype(np.uint8)\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error processing image from {path}: {e}\")\n",
    "        return create_dummy_image()\n",
    "\n",
    "def create_dummy_image():\n",
    "    \"\"\"Create a dummy image when DICOM loading fails\"\"\"\n",
    "    # Create a gray image with some noise to simulate medical imaging\n",
    "    dummy = np.random.normal(128, 20, (512, 512))\n",
    "    dummy = np.clip(dummy, 0, 255).astype(np.uint8)\n",
    "    dummy = np.stack([dummy, dummy, dummy], axis=2)\n",
    "    return dummy\n",
    "\n",
    "# Legacy functions for backward compatibility\n",
    "def load_scan_fixed(path):\n",
    "    \"\"\"Load DICOM slices from a folder - ENHANCED with compression handling\"\"\"\n",
    "    try:\n",
    "        files = [f for f in os.listdir(path) if f.endswith('.dcm')]\n",
    "        slices = []\n",
    "        \n",
    "        for f in files:\n",
    "            try:\n",
    "                dcm_path = os.path.join(path, f)\n",
    "                dcm = pydicom.dcmread(dcm_path, force=True)\n",
    "                \n",
    "                # Try to decompress if compressed\n",
    "                if hasattr(dcm, 'decompress'):\n",
    "                    try:\n",
    "                        dcm.decompress()\n",
    "                    except RuntimeError as e:\n",
    "                        # \"Already uncompressed\" is fine\n",
    "                        if \"already uncompressed\" not in str(e):\n",
    "                            raise e\n",
    "                \n",
    "                slices.append(dcm)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {f}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not slices:\n",
    "            return None\n",
    "            \n",
    "        slices.sort(key=lambda x: int(x.InstanceNumber) if hasattr(x, 'InstanceNumber') else 0)\n",
    "        \n",
    "        # Set slice thickness\n",
    "        try:\n",
    "            if len(slices) > 1:\n",
    "                slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "            else:\n",
    "                slice_thickness = 1.0\n",
    "        except:\n",
    "            try:\n",
    "                slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "            except:\n",
    "                slice_thickness = 1.0  # Default\n",
    "        \n",
    "        for s in slices:\n",
    "            s.SliceThickness = slice_thickness\n",
    "            \n",
    "        return slices\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading scan from {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_pixels_hu_fixed(scans):\n",
    "    \"\"\"Convert DICOM to Hounsfield Units (HU) - Enhanced with compression handling\"\"\"\n",
    "    try:\n",
    "        valid_slices = []\n",
    "        \n",
    "        for s in scans:\n",
    "            try:\n",
    "                # Try to access pixel array\n",
    "                if hasattr(s, 'pixel_array'):\n",
    "                    pixel_array = s.pixel_array\n",
    "                    valid_slices.append(s)\n",
    "                elif hasattr(s, 'PixelData'):\n",
    "                    # Try raw pixel data extraction\n",
    "                    rows, cols = s.Rows, s.Columns\n",
    "                    pixel_bytes = s.PixelData\n",
    "                    \n",
    "                    if len(pixel_bytes) >= rows * cols * 2:\n",
    "                        pixel_array = np.frombuffer(pixel_bytes[:rows*cols*2], dtype=np.uint16)\n",
    "                        pixel_array = pixel_array.reshape((rows, cols))\n",
    "                        s.pixel_array = pixel_array  # Add it for consistency\n",
    "                        valid_slices.append(s)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not extract pixels from slice: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not valid_slices:\n",
    "            return None\n",
    "            \n",
    "        image = np.stack([s.pixel_array for s in valid_slices])\n",
    "        image = image.astype(np.int16)\n",
    "        \n",
    "        # Set out-of-scan pixels to 0\n",
    "        image[image == -2000] = 0\n",
    "        \n",
    "        # Convert to HU\n",
    "        first_slice = valid_slices[0]\n",
    "        intercept = getattr(first_slice, 'RescaleIntercept', 0)\n",
    "        slope = getattr(first_slice, 'RescaleSlope', 1)\n",
    "        \n",
    "        if slope != 1:\n",
    "            image = slope * image.astype(np.float64)\n",
    "            image = image.astype(np.int16)\n",
    "            \n",
    "        image += np.int16(intercept)\n",
    "        \n",
    "        return np.array(image, dtype=np.int16)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to HU: {e}\")\n",
    "        return None\n",
    "\n",
    "# Update the main loading function\n",
    "def load_and_preprocess_dicom_fixed(path):\n",
    "    \"\"\"Main DICOM loading function - now uses enhanced loading with reduced verbosity\"\"\"\n",
    "    return load_and_preprocess_dicom_enhanced(path, verbose=False)  # Turn off verbose by default\n",
    "\n",
    "# Test DICOM loading on a sample\n",
    "if TRAIN_DIR.exists():\n",
    "    print(\"\\nTesting enhanced DICOM loading...\")\n",
    "    sample_patient = P[0] if P else None\n",
    "    if sample_patient:\n",
    "        sample_path = TRAIN_DIR / sample_patient\n",
    "        if sample_path.exists():\n",
    "            dcm_files = list(sample_path.glob('*.dcm'))\n",
    "            if dcm_files:\n",
    "                print(f\"Testing with {len(dcm_files)} DICOM files from {sample_patient}\")\n",
    "                \n",
    "                # Test first file with verbose output\n",
    "                test_img = load_and_preprocess_dicom_enhanced(dcm_files[0], verbose=True)\n",
    "                print(f\"Loaded image shape: {test_img.shape}, dtype: {test_img.dtype}\")\n",
    "                print(f\"Pixel value range: [{test_img.min()}, {test_img.max()}]\")\n",
    "                \n",
    "                # Test a few more files without verbose output\n",
    "                success_count = 0\n",
    "                for i, dcm_file in enumerate(dcm_files[:5]):\n",
    "                    try:\n",
    "                        img = load_and_preprocess_dicom_fixed(dcm_file)\n",
    "                        if img.shape == (512, 512, 3):\n",
    "                            success_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ File {i+1}: {dcm_file.name} - Error: {e}\")\n",
    "                \n",
    "                print(f\"✅ Successfully loaded {success_count}/{min(5, len(dcm_files))} test files\")\n",
    "            else:\n",
    "                print(\"No DICOM files found for sample patient\")\n",
    "        else:\n",
    "            print(f\"Sample patient directory not found: {sample_path}\")\n",
    "    else:\n",
    "        print(\"No patients available for testing\")\n",
    "else:\n",
    "    print(\"Training directory not found - using dummy data for testing\")\n",
    "    dummy_img = create_dummy_image()\n",
    "    print(f\"Created dummy image: {dummy_img.shape}, dtype: {dummy_img.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 4: MEDICAL AUGMENTATIONS\n",
    "# =============================================================================\n",
    "\n",
    "class MedicalAugmentation:\n",
    "    def __init__(self, augment=True):\n",
    "        if augment:\n",
    "            self.transform = albu.Compose([\n",
    "                # Geometric augmentations - conservative for medical images\n",
    "                albu.Rotate(limit=10, p=0.5),  # Reduced rotation\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
    "                \n",
    "                # Medical-specific augmentations\n",
    "                albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                albu.GaussNoise(var_limit=(5.0, 25.0), p=0.3),\n",
    "                albu.RandomGamma(gamma_limit=(90, 110), p=0.3),\n",
    "                \n",
    "                # Light distortions\n",
    "                albu.GridDistortion(num_steps=3, distort_limit=0.1, p=0.2),\n",
    "                albu.OpticalDistortion(distort_limit=0.1, shift_limit=0.1, p=0.2),\n",
    "                \n",
    "                # Dropout\n",
    "                albu.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.2),\n",
    "                \n",
    "                # Normalization\n",
    "                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = albu.Compose([\n",
    "                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        return self.transform(image=image)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 5: FIXED DATASET CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class OSICDenseNetDataset(Dataset):\n",
    "    \"\"\"FIXED dataset with proper error handling and debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True, verbose=True):\n",
    "        self.patients = patients\n",
    "        self.A_dict = A_dict\n",
    "        self.TAB_dict = TAB_dict\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.augmentor = MedicalAugmentation(augment=augment)\n",
    "        \n",
    "        # Comprehensive patient validation\n",
    "        self.patient_images = {}\n",
    "        missing_dirs = []\n",
    "        no_images = []\n",
    "        valid_patients = []\n",
    "        \n",
    "        for patient in self.patients:\n",
    "            patient_dir = self.data_dir / patient\n",
    "            \n",
    "            if not patient_dir.exists():\n",
    "                missing_dirs.append(patient)\n",
    "                continue\n",
    "                \n",
    "            image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n",
    "            \n",
    "            if not image_files:\n",
    "                no_images.append(patient)\n",
    "                continue\n",
    "                \n",
    "            # Test load first image to ensure it's valid\n",
    "            try:\n",
    "                test_img = load_and_preprocess_dicom_fixed(image_files[0])\n",
    "                if test_img is not None and test_img.shape == (512, 512, 3):\n",
    "                    self.patient_images[patient] = image_files\n",
    "                    valid_patients.append(patient)\n",
    "                else:\n",
    "                    no_images.append(patient)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Failed to load test image for {patient}: {e}\")\n",
    "                no_images.append(patient)\n",
    "        \n",
    "        self.valid_patients = valid_patients\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nDataset {split} validation:\")\n",
    "            print(f\"  Input patients: {len(self.patients)}\")\n",
    "            print(f\"  Missing directories: {len(missing_dirs)}\")\n",
    "            print(f\"  No valid images: {len(no_images)}\")\n",
    "            print(f\"  Valid patients: {len(self.valid_patients)}\")\n",
    "            \n",
    "            if missing_dirs and len(missing_dirs) <= 5:\n",
    "                print(f\"  Missing dirs: {missing_dirs}\")\n",
    "            elif missing_dirs:\n",
    "                print(f\"  Missing dirs: {missing_dirs[:3]}... and {len(missing_dirs)-3} more\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.split == 'train':\n",
    "            return len(self.valid_patients) * 4  # Reduced multiplier\n",
    "        else:\n",
    "            return len(self.valid_patients)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            if self.split == 'train':\n",
    "                patient_idx = idx % len(self.valid_patients)\n",
    "            else:\n",
    "                patient_idx = idx\n",
    "                \n",
    "            patient = self.valid_patients[patient_idx]\n",
    "            \n",
    "            # Get random image for this patient\n",
    "            available_images = self.patient_images[patient]\n",
    "            if len(available_images) > 1:\n",
    "                selected_image = np.random.choice(available_images)\n",
    "            else:\n",
    "                selected_image = available_images[0]\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            img = load_and_preprocess_dicom_fixed(selected_image)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to load image for {patient}\")\n",
    "            \n",
    "            # Apply augmentations\n",
    "            img_tensor = self.augmentor(img)\n",
    "            \n",
    "            # Get tabular features\n",
    "            if patient not in self.TAB_dict:\n",
    "                raise ValueError(f\"No tabular features for {patient}\")\n",
    "            tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n",
    "            \n",
    "            # Get target (decay coefficient)\n",
    "            if patient not in self.A_dict:\n",
    "                raise ValueError(f\"No decay coefficient for {patient}\")\n",
    "            target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n",
    "            \n",
    "            return img_tensor, tab_features, target, patient\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx} (patient {patient if 'patient' in locals() else 'unknown'}): {e}\")\n",
    "            # Return dummy data to prevent training crash\n",
    "            dummy_img = torch.zeros((3, 512, 512), dtype=torch.float32)\n",
    "            dummy_tab = torch.zeros(4, dtype=torch.float32)\n",
    "            dummy_target = torch.tensor(0.0, dtype=torch.float32)\n",
    "            return dummy_img, dummy_tab, dummy_target, \"dummy_patient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 6: FIXED MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        x_cat = self.conv1(x_cat)\n",
    "        return x * self.sigmoid(x_cat)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return x * self.sigmoid(out)\n",
    "\n",
    "class WorkingDenseNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    COMPLETELY FIXED DenseNet model with proper cross-modal attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tabular_dim=4, dropout_rate=0.3):\n",
    "        super(WorkingDenseNetModel, self).__init__()\n",
    "        \n",
    "        # DenseNet121 backbone\n",
    "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        self.features = densenet.features\n",
    "        \n",
    "        # Get the number of features from DenseNet\n",
    "        self.num_image_features = 1024  # DenseNet121 output features\n",
    "        \n",
    "        # Attention mechanisms\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "        self.channel_attention = ChannelAttention(self.num_image_features)\n",
    "        \n",
    "        # Enhanced tabular processing\n",
    "        self.tabular_processor = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # FIX 2: MAJOR BUG FIX - Trainable cross-modal projection\n",
    "        self.tab_to_img_projection = nn.Linear(512, self.num_image_features)\n",
    "        self.img_to_tab_projection = nn.Linear(self.num_image_features, 512)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.num_image_features, \n",
    "            num_heads=8, \n",
    "            dropout=0.1, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Multi-modal fusion with proper dimensions\n",
    "        fusion_input_dim = self.num_image_features + 512  # 1024 + 512 = 1536\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate/2),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Regression heads for uncertainty quantification\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Log variance head (for uncertainty)\n",
    "        self.log_var_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Proper weight initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, images, tabular):\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Extract image features\n",
    "        img_features = self.features(images)  # [B, 1024, H, W]\n",
    "        \n",
    "        # Apply attention mechanisms\n",
    "        img_features = self.channel_attention(img_features)\n",
    "        img_features = self.spatial_attention(img_features)\n",
    "        \n",
    "        # Global average pooling\n",
    "        img_features = F.adaptive_avg_pool2d(img_features, (1, 1))\n",
    "        img_features = img_features.view(batch_size, -1)  # [B, 1024]\n",
    "        \n",
    "        # Process tabular data\n",
    "        tab_features = self.tabular_processor(tabular)  # [B, 512]\n",
    "        \n",
    "        # FIX 2: CRITICAL FIX - Proper cross-modal attention\n",
    "        # Prepare for attention\n",
    "        img_expanded = img_features.unsqueeze(1)  # [B, 1, 1024]\n",
    "        \n",
    "        # Project tabular to same dimension as image features (TRAINABLE!)\n",
    "        tab_proj = self.tab_to_img_projection(tab_features)  # [B, 512] -> [B, 1024]\n",
    "        tab_expanded = tab_proj.unsqueeze(1)  # [B, 1, 1024]\n",
    "        \n",
    "        # Cross-modal attention: let image attend to tabular\n",
    "        attended_img, attention_weights = self.cross_attention(\n",
    "            img_expanded,  # Query\n",
    "            tab_expanded,  # Key\n",
    "            tab_expanded   # Value\n",
    "        )\n",
    "        attended_img = attended_img.squeeze(1)  # [B, 1024]\n",
    "        \n",
    "        # Also project image to tabular space for bidirectional fusion\n",
    "        img_to_tab = self.img_to_tab_projection(img_features)  # [B, 1024] -> [B, 512]\n",
    "        enhanced_tab = tab_features + img_to_tab  # Residual connection\n",
    "        \n",
    "        # Multi-modal fusion\n",
    "        combined_features = torch.cat([attended_img, enhanced_tab], dim=1)  # [B, 1536]\n",
    "        fused_features = self.fusion_layer(combined_features)  # [B, 256]\n",
    "        \n",
    "        # Predictions\n",
    "        mean_pred = self.mean_head(fused_features).squeeze(-1)  # [B]\n",
    "        log_var = self.log_var_head(fused_features).squeeze(-1)  # [B]\n",
    "        \n",
    "        # Clamp log_var to reasonable range to prevent numerical issues\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        \n",
    "        return mean_pred, log_var\n",
    "\n",
    "# FIX 6: Properly defined ModelWithConfidence\n",
    "class ModelWithConfidence(nn.Module):\n",
    "    \"\"\"Wrapper to convert log_var to confidence for evaluation/submission\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model):\n",
    "        super(ModelWithConfidence, self).__init__()\n",
    "        self.base_model = base_model\n",
    "    \n",
    "    def forward(self, images, tabular):\n",
    "        mean_pred, log_var = self.base_model(images, tabular)\n",
    "        # FIX 3: Consistent sigma handling - no arbitrary multipliers\n",
    "        confidence = torch.exp(log_var / 2.0)  # Convert log_var to standard deviation\n",
    "        return mean_pred, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = WorkingDenseNetModel(tabular_dim=4).to(DEVICE)\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7: Data Split and Loaders\n",
    "# -----------------------------\n",
    "\n",
    "# Split patients into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "patients_list = list(P)\n",
    "train_patients, val_patients = train_test_split(\n",
    "    patients_list, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train patients: {len(train_patients)}\")\n",
    "print(f\"Validation patients: {len(val_patients)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = OSICDenseNetDataset(\n",
    "    patients=train_patients,\n",
    "    A_dict=A,\n",
    "    TAB_dict=TAB,\n",
    "    data_dir=TRAIN_DIR,\n",
    "    split='train',\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = OSICDenseNetDataset(\n",
    "    patients=val_patients,\n",
    "    A_dict=A,\n",
    "    TAB_dict=TAB,\n",
    "    data_dir=TRAIN_DIR,\n",
    "    split='val',\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Data loaders created: {len(train_loader)} training batches, {len(val_loader)} validation batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 8: Training with Uncertainty\n",
    "# --------------------------------\n",
    "\n",
    "# FIX 3: Corrected trainer with consistent sigma handling\n",
    "class CorrectedSimpleTrainer:\n",
    "    \"\"\"\n",
    "    Trainer with FIXED sigma consistency\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, lr=1e-4):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.best_val_mae = float('inf')\n",
    "        self.best_val_lll = float('-inf')\n",
    "        \n",
    "    def uncertainty_loss(self, mean_pred, log_var, targets, reduction='mean'):\n",
    "        \"\"\"FIXED uncertainty loss with consistent sigma handling\"\"\"\n",
    "        # FIX 3: Use natural units - no artificial scaling during training\n",
    "        var = torch.exp(log_var)\n",
    "        mse_loss = (mean_pred - targets) ** 2\n",
    "        \n",
    "        # Uncertainty loss: 0.5 * (MSE/var + log(var))\n",
    "        loss = 0.5 * (mse_loss / var + log_var)\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        return loss.sum()\n",
    "        \n",
    "    def laplace_log_likelihood(self, y_true, y_pred, log_var, sigma_floor=70.0):\n",
    "        \"\"\"\n",
    "        FIXED Laplace Log Likelihood with proper sigma floor handling\n",
    "        \"\"\"\n",
    "        # Convert log variance to standard deviation\n",
    "        sigma = torch.exp(log_var / 2.0)\n",
    "        \n",
    "        # FIX 3: Apply sigma floor only for evaluation, not training scaling\n",
    "        sigma = torch.clamp(sigma, min=sigma_floor)  # Use contest floor here\n",
    "        \n",
    "        abs_errors = torch.abs(y_true - y_pred)\n",
    "        \n",
    "        # Actual log-likelihood: -log(√2 * σ) - |y-μ|/σ\n",
    "        log_likelihood = -torch.log(np.sqrt(2.0) * sigma) - abs_errors / sigma\n",
    "        \n",
    "        return torch.mean(log_likelihood)\n",
    "        \n",
    "    def train(self, train_loader, val_loader, epochs=30, patience=8):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=5e-5,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=4, verbose=True\n",
    "        )\n",
    "        \n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_mae = 0.0\n",
    "            train_lll = 0.0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n",
    "                try:\n",
    "                    images = images.to(self.device)\n",
    "                    tabular = tabular.to(self.device) \n",
    "                    targets = targets.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    mean_pred, log_var = self.model(images, tabular)\n",
    "                    \n",
    "                    # Calculate losses and metrics\n",
    "                    loss = self.uncertainty_loss(mean_pred, log_var, targets)\n",
    "                    mae = F.l1_loss(mean_pred, targets)\n",
    "                    lll = self.laplace_log_likelihood(targets, mean_pred, log_var)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    train_mae += mae.item()\n",
    "                    train_lll += lll.item()\n",
    "                    train_batches += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_mae = 0.0\n",
    "            val_lll = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "            val_log_vars = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (images, tabular, targets, _) in enumerate(val_loader):\n",
    "                    try:\n",
    "                        images = images.to(self.device)\n",
    "                        tabular = tabular.to(self.device)\n",
    "                        targets = targets.to(self.device)\n",
    "                        \n",
    "                        mean_pred, log_var = self.model(images, tabular)\n",
    "                        \n",
    "                        loss = self.uncertainty_loss(mean_pred, log_var, targets)\n",
    "                        mae = F.l1_loss(mean_pred, targets)\n",
    "                        lll = self.laplace_log_likelihood(targets, mean_pred, log_var)\n",
    "                        \n",
    "                        val_loss += loss.item()\n",
    "                        val_mae += mae.item()\n",
    "                        val_lll += lll.item()\n",
    "                        \n",
    "                        val_predictions.extend(mean_pred.cpu().numpy())\n",
    "                        val_targets.extend(targets.cpu().numpy())\n",
    "                        val_log_vars.extend(log_var.cpu().numpy())\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if train_batches > 0 and len(val_predictions) > 0:\n",
    "                avg_train_loss = train_loss / train_batches\n",
    "                avg_train_mae = train_mae / train_batches\n",
    "                avg_train_lll = train_lll / train_batches\n",
    "                \n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                avg_val_mae = val_mae / len(val_loader)\n",
    "                avg_val_lll = val_lll / len(val_loader)\n",
    "                \n",
    "                # Convert to numpy for additional metrics\n",
    "                val_predictions = np.array(val_predictions)\n",
    "                val_targets = np.array(val_targets)\n",
    "                val_log_vars = np.array(val_log_vars)\n",
    "                \n",
    "                # FIX 3: Show raw sigma values (no arbitrary scaling)\n",
    "                val_sigmas = np.sqrt(np.exp(val_log_vars))\n",
    "                val_rmse = np.sqrt(np.mean((val_predictions - val_targets) ** 2))\n",
    "                \n",
    "                # Calculate R²\n",
    "                ss_res = np.sum((val_targets - val_predictions) ** 2)\n",
    "                ss_tot = np.sum((val_targets - np.mean(val_targets)) ** 2)\n",
    "                r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else -float('inf')\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                print(f\"Train Loss: {avg_train_loss:.6f} | Train LLL: {avg_train_lll:.6f} | Train MAE: {avg_train_mae:.6f}\")\n",
    "                print(f\"Val Loss: {avg_val_loss:.6f} | Val LLL: {avg_val_lll:.6f} | MAE: {avg_val_mae:.6f} | RMSE: {val_rmse:.6f} | R²: {r2:.6f}\")\n",
    "                print(f\"Raw Sigma Stats: Avg={np.mean(val_sigmas):.2f}, Range=[{np.min(val_sigmas):.2f}, {np.max(val_sigmas):.2f}]\")\n",
    "                \n",
    "                scheduler.step(avg_val_lll)\n",
    "                \n",
    "                if avg_val_lll > self.best_val_lll:\n",
    "                    self.best_val_lll = avg_val_lll\n",
    "                    self.best_val_mae = avg_val_mae\n",
    "                    torch.save(self.model.state_dict(), 'best_model_fixed.pth')\n",
    "                    print(\"✅ New best model saved!\")\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "                    \n",
    "                print(\"-\" * 70)\n",
    "        \n",
    "        return self.best_val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 9: Submission Generation with Confidence\n",
    "# -------------------------------------------\n",
    "\n",
    "class TTAPredictor:\n",
    "    \"\"\"Test-time augmentation for more robust predictions\"\"\"\n",
    "    def __init__(self, model, num_augmentations=5):\n",
    "        self.model = model\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.augmentor = MedicalAugmentation(augment=True)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, image, tabular):\n",
    "        # Original prediction\n",
    "        with torch.no_grad():\n",
    "            mean_pred, log_var = self.model(image.unsqueeze(0), tabular.unsqueeze(0))\n",
    "            mean_preds = [mean_pred.item()]\n",
    "            log_vars = [log_var.item()]\n",
    "        \n",
    "        # Augmented predictions\n",
    "        for _ in range(self.num_augmentations):\n",
    "            try:\n",
    "                # Apply augmentation\n",
    "                aug_img = self.augmentor(image.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "                aug_img = aug_img.to(image.device)\n",
    "                \n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    mean_pred, log_var = self.model(aug_img.unsqueeze(0), tabular.unsqueeze(0))\n",
    "                    mean_preds.append(mean_pred.item())\n",
    "                    log_vars.append(log_var.item())\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in TTA: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Ensemble predictions\n",
    "        mean_ensemble = np.median(mean_preds)\n",
    "        log_var_ensemble = np.median(log_vars)\n",
    "        \n",
    "        # Calculate uncertainty (standard deviation)\n",
    "        std = np.sqrt(np.exp(log_var_ensemble))\n",
    "        \n",
    "        return mean_ensemble, std\n",
    "\n",
    "def create_submission_with_confidence(model, test_dir, output_file='submission.csv'):\n",
    "    \"\"\"Create submission with confidence intervals (no artificial clipping).\"\"\"\n",
    "    print(f\"📝 Creating submission with confidence intervals...\")\n",
    "\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "    print(f\"✅ Loaded test data: {len(test_df)} samples\")\n",
    "\n",
    "    submissions = []\n",
    "    model.eval()\n",
    "\n",
    "    # Create augmentor for test time augmentation\n",
    "    test_augmentor = MedicalAugmentation(augment=False)\n",
    "\n",
    "    print(\"🔄 Processing test patients...\")\n",
    "\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n",
    "        patient_id = row['Patient']\n",
    "        weeks = row['Weeks']\n",
    "\n",
    "        try:\n",
    "            # Load patient image dir\n",
    "            patient_dir = Path(test_dir) / patient_id\n",
    "\n",
    "            # Default fallback predictions\n",
    "            fvc_pred = 2000.0\n",
    "            confidence_val = 200.0\n",
    "\n",
    "            if patient_dir.exists():\n",
    "                image_files = list(patient_dir.glob('*.dcm'))\n",
    "                if image_files:\n",
    "                    # Load and preprocess image\n",
    "                    img = load_and_preprocess_dicom_fixed(image_files[0])\n",
    "                    img_tensor = test_augmentor(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "                    # Prepare tabular features\n",
    "                    tab_features = get_tab_features(row)\n",
    "                    tab_tensor = torch.tensor(tab_features).float().unsqueeze(0).to(DEVICE)\n",
    "\n",
    "                    # Predict with uncertainty\n",
    "                    with torch.no_grad():\n",
    "                        mean_pred, log_var = model(img_tensor, tab_tensor)\n",
    "                        fvc_pred = mean_pred.item()\n",
    "                        confidence_val = max(torch.exp(log_var / 2).item() * 70, 70)\n",
    "\n",
    "            # Create submission rows for required weeks\n",
    "            for week in range(-12, 134):\n",
    "                patient_week = f\"{patient_id}_{week}\"\n",
    "\n",
    "                # Linear progression adjustment\n",
    "                if patient_id in A:\n",
    "                    time_adjusted_fvc = fvc_pred + (week - weeks) * A[patient_id]\n",
    "                else:\n",
    "                    time_adjusted_fvc = fvc_pred + (week - weeks) * (-7)\n",
    "\n",
    "                # ⚠️ No clipping — keep raw predictions\n",
    "                submissions.append({\n",
    "                    'Patient_Week': patient_week,\n",
    "                    'FVC': time_adjusted_fvc,\n",
    "                    'Confidence': confidence_val\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing patient {patient_id}: {e}\")\n",
    "            # Fallback rows for failed patient\n",
    "            for week in range(-12, 134):\n",
    "                patient_week = f\"{patient_id}_{week}\"\n",
    "                submissions.append({\n",
    "                    'Patient_Week': patient_week,\n",
    "                    'FVC': 2000.0,\n",
    "                    'Confidence': 200.0\n",
    "                })\n",
    "\n",
    "    # Build dataframe\n",
    "    submission_df = pd.DataFrame(submissions)\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"✅ Submission saved to {output_file}\")\n",
    "    print(f\"📊 Submission stats:\")\n",
    "    print(f\"   Total rows: {len(submission_df)}\")\n",
    "    print(f\"   FVC raw range: {submission_df['FVC'].min():.1f} - {submission_df['FVC'].max():.1f}\")\n",
    "    print(f\"   Confidence range: {submission_df['Confidence'].min():.1f} - {submission_df['Confidence'].max():.1f}\")\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "# Helper function for DICOM loading (for submission)\n",
    "def load_and_preprocess_dicom(path):\n",
    "    \"\"\"Simplified DICOM loading for submission\"\"\"\n",
    "    try:\n",
    "        dcm = pydicom.dcmread(str(path))\n",
    "        img = dcm.pixel_array.astype(np.float32)\n",
    "        \n",
    "        if len(img.shape) == 3:\n",
    "            img = img[img.shape[0]//2]\n",
    "        \n",
    "        img = cv2.resize(img, (512, 512))\n",
    "        \n",
    "        # Normalize to 0-255\n",
    "        img_min, img_max = img.min(), img.max()\n",
    "        if img_max > img_min:\n",
    "            img = (img - img_min) / (img_max - img_min) * 255\n",
    "        else:\n",
    "            img = np.zeros_like(img)\n",
    "        \n",
    "        # Convert to 3-channel\n",
    "        img = np.stack([img, img, img], axis=2).astype(np.uint8)\n",
    "        return img\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return black image as fallback\n",
    "        return np.zeros((512, 512, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 10: Execute Training with Enhanced Error Handling\n",
    "# -----------------------------------------------------\n",
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "print(\"📊 Dataset Summary:\")\n",
    "print(f\"   Total patients: {len(P)}\")\n",
    "print(f\"   Training patients: {len(train_dataset.valid_patients)}\")\n",
    "print(f\"   Validation patients: {len(val_dataset.valid_patients)}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test one batch to ensure everything works\n",
    "print(\"\\n🧪 Testing data loading...\")\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    images, tabular, targets, patient_ids = test_batch\n",
    "    print(f\"✅ Successfully loaded test batch:\")\n",
    "    print(f\"   Images shape: {images.shape}\")\n",
    "    print(f\"   Tabular shape: {tabular.shape}\")\n",
    "    print(f\"   Targets shape: {targets.shape}\")\n",
    "    print(f\"   Sample patient: {patient_ids[0]}\")\n",
    "    \n",
    "    # Test model forward pass\n",
    "    with torch.no_grad():\n",
    "        test_pred, test_log_var = model(images.to(DEVICE), tabular.to(DEVICE))\n",
    "        print(f\"   Model output shapes: pred={test_pred.shape}, log_var={test_log_var.shape}\")\n",
    "        print(f\"   Sample prediction: {test_pred[0].item():.3f} ± {torch.exp(test_log_var[0]/2).item():.3f}\")\n",
    "    \n",
    "    print(\"✅ All systems ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in data loading test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\n🛠️ Please check data paths and DICOM loading issues above.\")\n",
    "\n",
    "# Start actual training\n",
    "try:\n",
    "    trainer = CorrectedSimpleTrainer(model, DEVICE, lr=5e-5)\n",
    "    print(f\"\\n🎯 Training started with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    best_val_mae = trainer.train(train_loader, val_loader, epochs=30, patience=8)\n",
    "    print(f\"🎯 Training completed! Best validation MAE: {best_val_mae:.6f}\")\n",
    "    \n",
    "    # Generate submission (skipping as per user's request)\n",
    "    print(\"📝 Skipping submission generation as requested...\")\n",
    "    # final_submission = create_submission_with_confidence(model, TEST_DIR, 'enhanced_submission.csv')\n",
    "    # print(\"✅ Submission ready!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️ Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\n💡 Try running individual cells to debug the issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# LLL evaluation utilities\n",
    "# -------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from math import sqrt, log\n",
    "from tqdm import tqdm\n",
    "\n",
    "SQRT2 = np.sqrt(2.0)\n",
    "\n",
    "def laplace_score_per_sample(y_true, y_pred, sigma, sigma_floor=70.0):\n",
    "    \"\"\"\n",
    "    Per-sample Laplace Log-Likelihood (as used in OSIC).\n",
    "    Inputs are numpy arrays or scalars.\n",
    "    sigma is clipped to a minimum of sigma_floor.\n",
    "    Returns per-sample score (not averaged).\n",
    "    \"\"\"\n",
    "    sigma = np.maximum(sigma, sigma_floor)\n",
    "    delta = np.abs(y_true - y_pred)\n",
    "    term1 = - (SQRT2 * delta) / sigma\n",
    "    term2 = - np.log(SQRT2 * sigma)\n",
    "    return term1 + term2\n",
    "\n",
    "def mean_laplace_score(y_true, y_pred, sigma, sigma_floor=70.0):\n",
    "    arr = laplace_score_per_sample(np.array(y_true), np.array(y_pred), np.array(sigma), sigma_floor=sigma_floor)\n",
    "    return float(np.mean(arr))\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate on a DataLoader\n",
    "# -------------------------\n",
    "def evaluate_lll_from_loader(model, loader, device, mode='log_var', tta_predictor=None, sigma_floor=70.0, save_csv=True, out_dir=None):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - 'log_var' : model(images, tabular) -> (mean_pred, log_var). sigma = sqrt(exp(log_var))\n",
    "      - 'confidence' : model(images, tabular) -> (mean_pred, confidence). confidence used as sigma directly\n",
    "      - 'tta' : use tta_predictor.predict(image, tabular) -> (mean, std). (std used directly as sigma)\n",
    "    tta_predictor: instance of TTAPredictor if mode == 'tta'\n",
    "    Returns: (mean_lll, df) and writes CSV if save_csv True.\n",
    "    CSV columns: ['patient'(if available), 'y_true', 'y_pred', 'sigma', 'lll']\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    sigmas = []\n",
    "    patients = []\n",
    "\n",
    "    device = torch.device(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating LLL\"):\n",
    "            images, tabular, targets, patient_ids = batch\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            images = images.to(device)\n",
    "            tabular = tabular.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if mode == 'log_var':\n",
    "                mean_pred, log_var = model(images, tabular)\n",
    "                mean_np = mean_pred.detach().cpu().numpy().astype(float)\n",
    "                log_var_np = log_var.detach().cpu().numpy().astype(float)\n",
    "                sigma_np = np.sqrt(np.exp(log_var_np))\n",
    "            elif mode == 'confidence':\n",
    "                mean_pred, confidence = model(images, tabular)\n",
    "                mean_np = mean_pred.detach().cpu().numpy().astype(float)\n",
    "                sigma_np = confidence.detach().cpu().numpy().astype(float)\n",
    "            elif mode == 'tta':\n",
    "                mean_list = []\n",
    "                sigma_list = []\n",
    "                for i in range(batch_size):\n",
    "                    img = images[i].cpu()\n",
    "                    tab = tabular[i].cpu()\n",
    "                    mean_i, sigma_i = tta_predictor.predict(img, tab)\n",
    "                    mean_list.append(float(mean_i))\n",
    "                    sigma_list.append(float(sigma_i))\n",
    "                mean_np = np.array(mean_list, dtype=float)\n",
    "                sigma_np = np.array(sigma_list, dtype=float)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown mode for evaluate_lll_from_loader\")\n",
    "\n",
    "            targets_np = targets.detach().cpu().numpy().astype(float)\n",
    "\n",
    "            preds.extend(mean_np.tolist())\n",
    "            trues.extend(targets_np.tolist())\n",
    "            sigmas.extend(sigma_np.tolist())\n",
    "            patients.extend([p if isinstance(p, str) else (p.item().decode('utf-8') if hasattr(p, 'item') else str(p)) for p in patient_ids])\n",
    "\n",
    "    preds = np.array(preds, dtype=float)\n",
    "    trues = np.array(trues, dtype=float)\n",
    "    sigmas = np.array(sigmas, dtype=float)\n",
    "    lll_per_sample = laplace_score_per_sample(trues, preds, sigmas, sigma_floor=sigma_floor)\n",
    "    mean_lll = float(np.mean(lll_per_sample))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'patient': patients,\n",
    "        'y_true': trues,\n",
    "        'y_pred': preds,\n",
    "        'sigma': sigmas,\n",
    "        'lll': lll_per_sample\n",
    "    })\n",
    "\n",
    "    if save_csv:\n",
    "        if out_dir is None:\n",
    "            out_dir = globals().get('auto_save_dir', '.')\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        outpath = os.path.join(out_dir, 'lll_predictions.csv')\n",
    "        df.to_csv(outpath, index=False)\n",
    "        print(f\"Saved per-sample predictions + lll to: {outpath}\")\n",
    "\n",
    "    print(f\"Mean Laplace Log-Likelihood (LLL): {mean_lll:.6f}\")\n",
    "    return mean_lll, df\n",
    "\n",
    "# -------------------------\n",
    "# Helper: Convert slope -> FVC predictions and compute LLL per patient-week\n",
    "# -------------------------\n",
    "def compute_lll_from_slope_predictions(slope_df, cur_fvc_map, cur_week_map, weeks_to_predict=None, sigma_floor=70.0, save_csv=True, out_dir=None):\n",
    "    if weeks_to_predict is None:\n",
    "        weeks_to_predict = np.arange(-12, 134)\n",
    "\n",
    "    rows = []\n",
    "    for idx, r in slope_df.iterrows():\n",
    "        patient = r['Patient']\n",
    "        slope = float(r['pred_slope'])\n",
    "        sigma_slope = float(r.get('sigma_slope', 0.0))\n",
    "        if patient not in cur_fvc_map or patient not in cur_week_map:\n",
    "            continue\n",
    "        cur_fvc = float(cur_fvc_map[patient])\n",
    "        cur_week = int(cur_week_map[patient])\n",
    "\n",
    "        intercept = cur_fvc - slope * cur_week\n",
    "\n",
    "        for w in weeks_to_predict:\n",
    "            pred_fvc = intercept + slope * w\n",
    "            sigma_fvc = max(1e-6, sigma_slope * abs(w - cur_week))\n",
    "            rows.append({\n",
    "                'Patient': patient,\n",
    "                'Week': w,\n",
    "                'y_true': None,\n",
    "                'y_pred': pred_fvc,\n",
    "                'sigma': sigma_fvc\n",
    "            })\n",
    "\n",
    "    df_expanded = pd.DataFrame(rows)\n",
    "    if save_csv:\n",
    "        out_dir = out_dir or globals().get('auto_save_dir', '.')\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        df_expanded.to_csv(os.path.join(out_dir, 'slope_to_fvc_expanded.csv'), index=False)\n",
    "        print(f\"Saved expanded slope->FVC predictions to {out_dir}/slope_to_fvc_expanded.csv\")\n",
    "    return df_expanded\n",
    "\n",
    "# -------------------------\n",
    "# Example usage after training (run these cells)\n",
    "# -------------------------\n",
    "# 1) If your trained model returns (mean_pred, log_var) -> use mode='log_var'\n",
    "auto_save_dir = \"./auto_save_data\"\n",
    "import os\n",
    "os.makedirs(auto_save_dir, exist_ok=True)\n",
    "mean_lll, df = evaluate_lll_from_loader(model, val_loader, DEVICE, mode='log_var', out_dir=auto_save_dir)\n",
    "#\n",
    "# 2) If you wrapped your model with ModelWithConfidence and it returns (mean_pred, confidence) -> mode='confidence'\n",
    "wrapped = ModelWithConfidence(model)  # load weights as needed\n",
    "wrapped.load_state_dict(torch.load('model_with_confidence.pth'))  # if saved\n",
    "wrapped.to(DEVICE).eval()\n",
    "mean_lll, df = evaluate_lll_from_loader(wrapped, val_loader, DEVICE, mode='confidence', out_dir=auto_save_dir)\n",
    "#\n",
    "# 3) If you want to do TTA (slower) using TTAPredictor:\n",
    "tta = TTAPredictor(model, num_augmentations=5)\n",
    "mean_lll, df = evaluate_lll_from_loader(model, val_loader, DEVICE, mode='tta', tta_predictor=tta, out_dir=auto_save_dir)\n",
    "#\n",
    "# 4) If your model predicts slope, and you have anchor cur_fvc & cur_week (per-patient), create a slope_df:\n",
    "slope_df = pd.DataFrame([\n",
    "    {'Patient': p, 'pred_slope': s, 'sigma_slope': ssize}\n",
    "    for p, s, ssize in zip(patient_list, slope_list, sigma_list)\n",
    "])\n",
    "expanded = compute_lll_from_slope_predictions(slope_df, cur_fvc_map, cur_week_map, weeks_to_predict=np.arange(-12,134))\n",
    "#    # fill expanded['y_true'] with true FVCs if you have them, then compute mean_laplace_score:\n",
    "mean_lll = mean_laplace_score(expanded['y_true'], expanded['y_pred'], expanded['sigma'], sigma_floor=70.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "packages = [\n",
    "    'pydicom',\n",
    "    'pylibjpeg',\n",
    "    'pylibjpeg-libjpeg', \n",
    "    'gdcm',\n",
    "    'opencv-python-headless',\n",
    "    'scikit-learn',\n",
    "    'albumentations',\n",
    "    'tqdm',\n",
    "    'seaborn',\n",
    "    'torch_xla',\n",
    "    'cloud-tpu-client'\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    if install_package(pkg):\n",
    "        print(f\"✅ {pkg} installed\")\n",
    "    else:\n",
    "        print(f\"⚠️ {pkg} installation failed (may already be installed)\")\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from datetime import timedelta, datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import pickle\n",
    "import glob\n",
    "from math import sqrt, log\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Image processing\n",
    "from skimage import measure, morphology, segmentation\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Albumentations for medical augmentations\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Check for TPU/GPU acceleration\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    HAS_TPU = True\n",
    "except ImportError:\n",
    "    HAS_TPU = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Configuration - Detect and use best available accelerator\n",
    "if HAS_TPU:\n",
    "    DEVICE = xm.xla_device()\n",
    "    print(\"Using TPU accelerator\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using GPU accelerator\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "DATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "\n",
    "print(\"Pulmonary Fibrosis Progression Analysis - OPTIMIZED VERSION\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 1: DATA LOADING AND EDA\n",
    "# =============================================================================\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "try:\n",
    "    test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "    print(f'Train: {train_df.shape[0]} rows, Test: {test_df.shape[0]} rows')\n",
    "except:\n",
    "    print(f'Train: {train_df.shape[0]} rows, Test: file not found')\n",
    "    test_df = None\n",
    "\n",
    "print(\"\\nTrain data sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTrain data statistics:\")\n",
    "print(train_df.describe())\n",
    "\n",
    "# Basic EDA\n",
    "print(f'\\nUnique patients in training data: {train_df[\"Patient\"].nunique()}')\n",
    "print(f'Total observations: {len(train_df)}')\n",
    "print(f'Average observations per patient: {len(train_df)/train_df[\"Patient\"].nunique():.2f}')\n",
    "\n",
    "# Check for missing values\n",
    "print(f'\\nMissing values:')\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2: FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Processing tabular features...\")\n",
    "\n",
    "# Create baseline features for each patient\n",
    "baseline_features = {}\n",
    "patient_slopes = {}\n",
    "patient_intercepts = {}\n",
    "\n",
    "for patient in train_df['Patient'].unique():\n",
    "    patient_data = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n",
    "    \n",
    "    # Get baseline measurement (first visit)\n",
    "    baseline = patient_data.iloc[0]\n",
    "    baseline_features[patient] = {\n",
    "        'Age': baseline['Age'],\n",
    "        'Sex': baseline['Sex'],\n",
    "        'SmokingStatus': baseline['SmokingStatus'],\n",
    "        'BaselineFVC': baseline['FVC'],\n",
    "        'BaselineWeeks': baseline['Weeks'],\n",
    "        'Percent': baseline['Percent'] if 'Percent' in baseline else 50.0  # Fallback\n",
    "    }\n",
    "    \n",
    "    # Calculate slope and intercept if multiple measurements\n",
    "    if len(patient_data) > 1:\n",
    "        weeks = patient_data['Weeks'].values\n",
    "        fvc = patient_data['FVC'].values\n",
    "        \n",
    "        # Linear regression: FVC = slope * weeks + intercept\n",
    "        A = np.vstack([weeks, np.ones(len(weeks))]).T\n",
    "        slope, intercept = np.linalg.lstsq(A, fvc, rcond=None)[0]\n",
    "        \n",
    "        patient_slopes[patient] = slope\n",
    "        patient_intercepts[patient] = intercept\n",
    "    else:\n",
    "        patient_slopes[patient] = 0.0\n",
    "        patient_intercepts[patient] = baseline['FVC']\n",
    "\n",
    "# Create enhanced tabular features\n",
    "def get_enhanced_tabular_features(patient_id, row=None):\n",
    "    \"\"\"Get enhanced tabular features with proper encoding\"\"\"\n",
    "    features = baseline_features[patient_id].copy()\n",
    "    \n",
    "    # Standardize age\n",
    "    features['Age'] = (features['Age'] - 50) / 20  # Standardize around mean\n",
    "    \n",
    "    # Encode sex (0 for Male, 1 for Female)\n",
    "    features['Sex'] = 1 if features['Sex'] == 'Female' else 0\n",
    "    \n",
    "    # One-hot encode smoking status\n",
    "    smoking_status = features['SmokingStatus']\n",
    "    features['Smoking_Never'] = 1 if smoking_status == 'Never smoked' else 0\n",
    "    features['Smoking_Ex'] = 1 if smoking_status == 'Ex-smoker' else 0\n",
    "    features['Smoking_Current'] = 1 if smoking_status == 'Currently smokes' else 0\n",
    "    \n",
    "    # Standardize baseline FVC\n",
    "    features['BaselineFVC'] = (features['BaselineFVC'] - 2500) / 1000\n",
    "    \n",
    "    # Standardize baseline weeks\n",
    "    features['BaselineWeeks'] = features['BaselineWeeks'] / 100\n",
    "    \n",
    "    # Standardize Percent\n",
    "    features['Percent'] = (features['Percent'] - 50) / 20\n",
    "    \n",
    "    # Add slope and intercept\n",
    "    features['Slope'] = patient_slopes[patient_id] / 10  # Scale slope\n",
    "    features['Intercept'] = (patient_intercepts[patient_id] - 2500) / 1000\n",
    "    \n",
    "    # Remove the original smoking status\n",
    "    del features['SmokingStatus']\n",
    "    \n",
    "    # If we have a row with current week, add week delta\n",
    "    if row is not None:\n",
    "        week_delta = (row['Weeks'] - features['BaselineWeeks'] * 100) / 50\n",
    "        features['WeekDelta'] = week_delta\n",
    "    \n",
    "    return np.array(list(features.values()), dtype=np.float32)\n",
    "\n",
    "# Calculate slopes for each patient\n",
    "A = patient_slopes\n",
    "TAB = {patient: get_enhanced_tabular_features(patient) for patient in baseline_features.keys()}\n",
    "P = list(baseline_features.keys())\n",
    "\n",
    "print(f\"Processed {len(P)} patients with enhanced features\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 3: ENHANCED DICOM PROCESSING WITH HU PRESERVATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_pixels_hu(dcm):\n",
    "    \"\"\"Convert DICOM pixel array to Hounsfield Units\"\"\"\n",
    "    try:\n",
    "        # Get pixel array\n",
    "        pixel_array = dcm.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Apply rescale intercept and slope if available\n",
    "        intercept = getattr(dcm, 'RescaleIntercept', 0)\n",
    "        slope = getattr(dcm, 'RescaleSlope', 1)\n",
    "        \n",
    "        pixel_array = pixel_array * slope + intercept\n",
    "        \n",
    "        return pixel_array\n",
    "    except:\n",
    "        return np.zeros((512, 512), dtype=np.float32)\n",
    "\n",
    "def load_dicom_with_hu(path):\n",
    "    \"\"\"Load DICOM and preserve Hounsfield Units\"\"\"\n",
    "    try:\n",
    "        dcm = pydicom.dcmread(str(path), force=True)\n",
    "        hu_image = get_pixels_hu(dcm)\n",
    "        \n",
    "        # Window to lung range [-1200, 600]\n",
    "        hu_image = np.clip(hu_image, -1200, 600)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        hu_image = (hu_image + 300) / 900  # Center around -300, scale by 900\n",
    "        \n",
    "        return hu_image\n",
    "    except Exception as e:\n",
    "        return np.zeros((512, 512), dtype=np.float32) - 1  # Return -1 filled array\n",
    "\n",
    "def load_three_slices(patient_dir, slice_idx=None, target_size=(256, 256)):\n",
    "    \"\"\"Load three adjacent slices for 2.5D input with consistent sizing\"\"\"\n",
    "    try:\n",
    "        # Get all DICOM files for patient\n",
    "        dicom_files = sorted(list(patient_dir.glob('*.dcm')))\n",
    "        if not dicom_files:\n",
    "            return None\n",
    "            \n",
    "        # Use middle slice if not specified\n",
    "        if slice_idx is None:\n",
    "            slice_idx = len(dicom_files) // 2\n",
    "            \n",
    "        # Get three slices around the index\n",
    "        slices = []\n",
    "        for i in range(max(0, slice_idx-1), min(len(dicom_files), slice_idx+2)):\n",
    "            slice_img = load_dicom_with_hu(dicom_files[i])\n",
    "            \n",
    "            # Resize to target size\n",
    "            if slice_img.shape != target_size:\n",
    "                slice_img = cv2.resize(slice_img, target_size, interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "            slices.append(slice_img)\n",
    "            \n",
    "        # If we couldn't get 3 slices, duplicate existing ones\n",
    "        while len(slices) < 3:\n",
    "            slices.append(slices[-1] if slices else np.zeros(target_size, dtype=np.float32) - 1)\n",
    "            \n",
    "        # Stack slices as channels\n",
    "        stacked = np.stack(slices, axis=-1)\n",
    "        return stacked\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return dummy 3-slice image with correct size\n",
    "        dummy_slice = np.zeros(target_size, dtype=np.float32) - 1\n",
    "        return np.stack([dummy_slice] * 3, axis=-1)\n",
    "\n",
    "# =============================================================================\n",
    "# PART 4: MEDICAL AUGMENTATIONS FOR HU IMAGES\n",
    "# =============================================================================\n",
    "\n",
    "class MedicalAugmentation:\n",
    "    def __init__(self, augment=True, target_size=(256, 256)):\n",
    "        self.target_size = target_size\n",
    "        if augment:\n",
    "            self.transform = albu.Compose([\n",
    "                albu.Rotate(limit=5, p=0.3),  # Reduced rotation for medical images\n",
    "                albu.HorizontalFlip(p=0.3),\n",
    "                albu.ShiftScaleRotate(shift_limit=0.03, scale_limit=0.05, rotate_limit=5, p=0.3),\n",
    "                albu.GaussNoise(var_limit=(0.01, 0.05), p=0.2),  # Reduced noise for HU\n",
    "                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = albu.Compose([\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        # Ensure image is the correct size\n",
    "        if image.shape[:2] != self.target_size:\n",
    "            image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_AREA)\n",
    "        return self.transform(image=image)['image']\n",
    "\n",
    "# =============================================================================\n",
    "# PART 5: DATASET CLASS WITH 2.5D INPUT\n",
    "# =============================================================================\n",
    "\n",
    "class OSICDenseNetDataset(Dataset):\n",
    "    \"\"\"Dataset class with 2.5D input and robust error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True, target_size=(256, 256)):\n",
    "        self.patients = patients\n",
    "        self.A_dict = A_dict\n",
    "        self.TAB_dict = TAB_dict\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.target_size = target_size\n",
    "        self.augmentor = MedicalAugmentation(augment=augment, target_size=target_size)\n",
    "        \n",
    "        # Preload patient directories and file lists\n",
    "        self.patient_dirs = {}\n",
    "        valid_patients = []\n",
    "        \n",
    "        for patient in self.patients:\n",
    "            patient_dir = self.data_dir / patient\n",
    "            \n",
    "            if not patient_dir.exists():\n",
    "                continue\n",
    "                \n",
    "            dicom_files = list(patient_dir.glob('*.dcm'))\n",
    "            \n",
    "            if dicom_files:\n",
    "                self.patient_dirs[patient] = patient_dir\n",
    "                valid_patients.append(patient)\n",
    "        \n",
    "        self.valid_patients = valid_patients\n",
    "        print(f\"Dataset {split}: {len(self.valid_patients)} valid patients\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_patients) * (4 if self.split == 'train' else 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            patient_idx = idx % len(self.valid_patients)\n",
    "            patient = self.valid_patients[patient_idx]\n",
    "            \n",
    "            # Load 2.5D image (3 slices)\n",
    "            img = load_three_slices(self.patient_dirs[patient], target_size=self.target_size)\n",
    "            \n",
    "            # Apply augmentations\n",
    "            img_tensor = self.augmentor(img)\n",
    "            \n",
    "            # Get features\n",
    "            tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n",
    "            target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n",
    "            \n",
    "            return img_tensor, tab_features, target, patient\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Return dummy data with consistent sizes\n",
    "            dummy_img = torch.zeros((3, self.target_size[0], self.target_size[1]), dtype=torch.float32)\n",
    "            dummy_tab = torch.zeros(len(self.TAB_dict[patient]), dtype=torch.float32)\n",
    "            dummy_target = torch.tensor(0.0, dtype=torch.float32)\n",
    "            return dummy_img, dummy_tab, dummy_target, \"dummy_patient\"\n",
    "\n",
    "# =============================================================================\n",
    "# PART 6: IMPROVED MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class EfficientNetModel(nn.Module):\n",
    "    \"\"\"More efficient model using EfficientNet backbone\"\"\"\n",
    "    \n",
    "    def __init__(self, tabular_dim=10):\n",
    "        super(EfficientNetModel, self).__init__()\n",
    "        \n",
    "        # EfficientNet backbone\n",
    "        self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        # Modify first convolution to accept 3 channels properly\n",
    "        original_first_conv = self.backbone.features[0][0]\n",
    "        self.backbone.features[0][0] = nn.Conv2d(\n",
    "            3, original_first_conv.out_channels, \n",
    "            kernel_size=original_first_conv.kernel_size,\n",
    "            stride=original_first_conv.stride,\n",
    "            padding=original_first_conv.padding,\n",
    "            bias=original_first_conv.bias\n",
    "        )\n",
    "        \n",
    "        # Initialize with pretrained weights (average across RGB channels)\n",
    "        with torch.no_grad():\n",
    "            self.backbone.features[0][0].weight[:, :3] = original_first_conv.weight.clone()\n",
    "            if original_first_conv.bias is not None:\n",
    "                self.backbone.features[0][0].bias = original_first_conv.bias.clone()\n",
    "        \n",
    "        # Get number of features from backbone\n",
    "        self.num_image_features = self.backbone.classifier[1].in_features\n",
    "        \n",
    "        # Tabular processor\n",
    "        self.tabular_processor = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Fusion and prediction\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.num_image_features + 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mean_head = nn.Linear(128, 1)\n",
    "        self.log_var_head = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, images, tabular):\n",
    "        # Image features\n",
    "        img_features = self.backbone.features(images)\n",
    "        img_features = F.adaptive_avg_pool2d(img_features, (1, 1))\n",
    "        img_features = img_features.view(img_features.size(0), -1)\n",
    "        \n",
    "        # Tabular features\n",
    "        tab_features = self.tabular_processor(tabular)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([img_features, tab_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Predictions\n",
    "        mean_pred = self.mean_head(fused).squeeze(-1)\n",
    "        log_var = self.log_var_head(fused).squeeze(-1)\n",
    "        \n",
    "        return mean_pred, log_var\n",
    "\n",
    "# Initialize model\n",
    "tabular_dim = len(TAB[list(TAB.keys())[0]])\n",
    "model = EfficientNetModel(tabular_dim=tabular_dim).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 7: DATA SPLIT AND LOADERS\n",
    "# =============================================================================\n",
    "\n",
    "# Split patients using GroupKFold for better validation\n",
    "patients_list = list(P)\n",
    "kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# For simplicity, use the first fold\n",
    "train_idx, val_idx = next(kfold.split(patients_list, groups=patients_list))\n",
    "train_patients = [patients_list[i] for i in train_idx]\n",
    "val_patients = [patients_list[i] for i in val_idx]\n",
    "\n",
    "print(f\"Train patients: {len(train_patients)}, Validation patients: {len(val_patients)}\")\n",
    "\n",
    "# Create datasets with consistent target size (smaller for faster training)\n",
    "TARGET_SIZE = (256, 256)\n",
    "train_dataset = OSICDenseNetDataset(\n",
    "    patients=train_patients, A_dict=A, TAB_dict=TAB, \n",
    "    data_dir=TRAIN_DIR, split='train', augment=True, target_size=TARGET_SIZE\n",
    ")\n",
    "\n",
    "val_dataset = OSICDenseNetDataset(\n",
    "    patients=val_patients, A_dict=A, TAB_dict=TAB,\n",
    "    data_dir=TRAIN_DIR, split='val', augment=False, target_size=TARGET_SIZE\n",
    ")\n",
    "\n",
    "# Create data loaders with appropriate batch size\n",
    "BATCH_SIZE = 16 if HAS_TPU or torch.cuda.is_available() else 4\n",
    "NUM_WORKERS = 4 if HAS_TPU or torch.cuda.is_available() else 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created: {len(train_loader)} train, {len(val_loader)} val batches\")\n",
    "print(f\"Using batch size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 8: TRAINING WITH IMPROVED LOSS AND METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def laplace_log_likelihood(y_true, y_pred, sigma, sigma_min=70):\n",
    "    \"\"\"Compute Laplace Log Likelihood with sigma clipping\"\"\"\n",
    "    sigma = np.maximum(sigma, sigma_min)\n",
    "    delta = np.abs(y_true - y_pred)\n",
    "    return -np.sqrt(2) * delta / sigma - np.log(np.sqrt(2) * sigma)\n",
    "\n",
    "class ImprovedTrainer:\n",
    "    def __init__(self, model, device, lr=1e-4):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', patience=5, factor=0.5, verbose=True\n",
    "        )\n",
    "        self.best_loss = float('inf')\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # For TPU\n",
    "        self.is_tpu = hasattr(device, 'type') and device.type == 'xla'\n",
    "    \n",
    "    def gaussian_nll_loss(self, mean_pred, log_var, targets):\n",
    "        \"\"\"Gaussian negative log likelihood loss\"\"\"\n",
    "        return 0.5 * (torch.mean(torch.exp(-log_var) * (mean_pred - targets)**2 + log_var))\n",
    "    \n",
    "    def compute_metrics(self, mean_pred, log_var, targets):\n",
    "        \"\"\"Compute various metrics for evaluation\"\"\"\n",
    "        # Convert to numpy for metric calculation\n",
    "        mean_pred_np = mean_pred.detach().cpu().numpy()\n",
    "        log_var_np = log_var.detach().cpu().numpy()\n",
    "        targets_np = targets.detach().cpu().numpy()\n",
    "        \n",
    "        # Calculate sigma\n",
    "        sigma_np = np.sqrt(np.exp(log_var_np))\n",
    "        \n",
    "        # Metrics\n",
    "        mse = mean_squared_error(targets_np, mean_pred_np)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(targets_np, mean_pred_np)\n",
    "        \n",
    "        # Laplace Log Likelihood\n",
    "        lll = np.mean(laplace_log_likelihood(targets_np, mean_pred_np, sigma_np))\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'lll': lll\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_metrics = {'mse': 0, 'rmse': 0, 'r2': 0, 'lll': 0}\n",
    "        \n",
    "        for images, tabular, targets, _ in tqdm(loader, desc=\"Training\"):\n",
    "            if self.is_tpu:\n",
    "                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n",
    "            else:\n",
    "                images, tabular, targets = images.to(self.device, non_blocking=True), \\\n",
    "                                         tabular.to(self.device, non_blocking=True), \\\n",
    "                                         targets.to(self.device, non_blocking=True)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.is_tpu:\n",
    "                # TPU doesn't support AMP, use regular training\n",
    "                mean_pred, log_var = self.model(images, tabular)\n",
    "                loss = self.gaussian_nll_loss(mean_pred, log_var, targets)\n",
    "                loss.backward()\n",
    "                xm.optimizer_step(self.optimizer)\n",
    "            else:\n",
    "                # Use AMP for GPU\n",
    "                with autocast():\n",
    "                    mean_pred, log_var = self.model(images, tabular)\n",
    "                    loss = self.gaussian_nll_loss(mean_pred, log_var, targets)\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics = self.compute_metrics(mean_pred, log_var, targets)\n",
    "            for k in all_metrics:\n",
    "                all_metrics[k] += metrics[k]\n",
    "        \n",
    "        # Average metrics\n",
    "        for k in all_metrics:\n",
    "            all_metrics[k] /= len(loader)\n",
    "        \n",
    "        return total_loss / len(loader), all_metrics\n",
    "    \n",
    "    def validate(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_metrics = {'mse': 0, 'rmse': 0, 'r2': 0, 'lll': 0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, tabular, targets, _ in tqdm(loader, desc=\"Validation\"):\n",
    "                if self.is_tpu:\n",
    "                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n",
    "                else:\n",
    "                    images, tabular, targets = images.to(self.device, non_blocking=True), \\\n",
    "                                             tabular.to(self.device, non_blocking=True), \\\n",
    "                                             targets.to(self.device, non_blocking=True)\n",
    "                \n",
    "                mean_pred, log_var = self.model(images, tabular)\n",
    "                loss = self.gaussian_nll_loss(mean_pred, log_var, targets)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Compute metrics\n",
    "                metrics = self.compute_metrics(mean_pred, log_var, targets)\n",
    "                for k in all_metrics:\n",
    "                    all_metrics[k] += metrics[k]\n",
    "        \n",
    "        # Average metrics\n",
    "        for k in all_metrics:\n",
    "            all_metrics[k] /= len(loader)\n",
    "        \n",
    "        return total_loss / len(loader), all_metrics\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=20):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_metrics = self.train_epoch(train_loader)\n",
    "            val_loss, val_metrics = self.validate(val_loader)\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            print(\"Train Metrics - MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, LLL: {lll:.4f}\".format(**train_metrics))\n",
    "            print(\"Val Metrics   - MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, LLL: {lll:.4f}\".format(**val_metrics))\n",
    "            \n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                if self.is_tpu:\n",
    "                    xm.save(model.state_dict(), 'best_model.pth')\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(\"✅ New best model saved!\")\n",
    "\n",
    "# Start training\n",
    "print(\"🚀 Starting training...\")\n",
    "trainer = ImprovedTrainer(model, DEVICE, lr=1e-4)\n",
    "trainer.train(train_loader, val_loader, epochs=15)\n",
    "\n",
    "print(\"🎯 Training completed!\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 9: BASELINE MODEL (LightGBM)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training LightGBM baseline for comparison...\")\n",
    "\n",
    "# Prepare data for LightGBM\n",
    "X = []\n",
    "y = []\n",
    "groups = []\n",
    "\n",
    "for patient in P:\n",
    "    features = TAB[patient]\n",
    "    X.append(features)\n",
    "    y.append(A[patient])\n",
    "    groups.append(patient)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X, y)\n",
    "\n",
    "# Evaluate\n",
    "lgb_preds = lgb_model.predict(X)\n",
    "lgb_mae = np.mean(np.abs(lgb_preds - y))\n",
    "lgb_mse = mean_squared_error(y, lgb_preds)\n",
    "lgb_rmse = np.sqrt(lgb_mse)\n",
    "lgb_r2 = r2_score(y, lgb_preds)\n",
    "\n",
    "print(f\"LightGBM Baseline - MAE: {lgb_mae:.4f}, MSE: {lgb_mse:.4f}, RMSE: {lgb_rmse:.4f}, R²: {lgb_r2:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 10: IMPROVED SUBMISSION GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_improved_submission(model, test_dir, output_file='submission.csv'):\n",
    "    \"\"\"Create improved submission file with proper uncertainty handling\"\"\"\n",
    "    print(\"Creating improved submission...\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "    submissions = []\n",
    "    \n",
    "    model.eval()\n",
    "    augmentor = MedicalAugmentation(augment=False, target_size=TARGET_SIZE)\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        patient_id = row['Patient']\n",
    "        weeks = row['Weeks']\n",
    "        \n",
    "        try:\n",
    "            patient_dir = Path(test_dir) / patient_id\n",
    "            \n",
    "            # Load 2.5D image\n",
    "            img = load_three_slices(patient_dir, target_size=TARGET_SIZE)\n",
    "            if img is None:\n",
    "                raise ValueError(\"No DICOM files found\")\n",
    "                \n",
    "            img_tensor = augmentor(img).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # Get tabular features\n",
    "            tab_features = get_enhanced_tabular_features(patient_id, row)\n",
    "            tab_tensor = torch.tensor(tab_features).float().unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mean_pred, log_var = model(img_tensor, tab_tensor)\n",
    "                fvc_pred = mean_pred.item()\n",
    "                sigma = np.sqrt(np.exp(log_var.item()))\n",
    "                \n",
    "                # Apply competition sigma floor (70) only at submission time\n",
    "                confidence = max(sigma, 70.0)\n",
    "            \n",
    "            # For each required week in the test set\n",
    "            patient_week = f\"{patient_id}_{weeks}\"\n",
    "            \n",
    "            # Use the model's prediction directly (no slope adjustment)\n",
    "            submissions.append({\n",
    "                'Patient_Week': patient_week,\n",
    "                'FVC': fvc_pred,\n",
    "                'Confidence': confidence\n",
    "            })\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Fallback to baseline prediction\n",
    "            if patient_id in TAB:\n",
    "                tab_features = TAB[patient_id]\n",
    "                fvc_pred = lgb_model.predict(tab_features.reshape(1, -1))[0]\n",
    "            else:\n",
    "                fvc_pred = 2500  # Average FVC\n",
    "                \n",
    "            submissions.append({\n",
    "                'Patient_Week': f\"{patient_id}_{weeks}\",\n",
    "                'FVC': fvc_pred,\n",
    "                'Confidence': 200.0  # Conservative uncertainty\n",
    "            })\n",
    "    \n",
    "    # Create submission file\n",
    "    submission_df = pd.DataFrame(submissions)\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    return submission_df\n",
    "\n",
    "# Generate submission\n",
    "if test_df is not None and TEST_DIR.exists():\n",
    "    submission = create_improved_submission(model, TEST_DIR, 'submission.csv')\n",
    "    print(\"✅ Submission ready!\")\n",
    "    print(submission.head())\n",
    "else:\n",
    "    print(\"No test data found - skipping submission\")\n",
    "\n",
    "print(\"🎉 All done!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1357052,
     "isSourceIdPinned": false,
     "sourceId": 20604,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# DenseNet V2: Enhanced Medical Imaging Model for OSIC Pulmonary Fibrosis Progression\n# Production-Ready Single-Flow Notebook with Advanced Uncertainty Quantification\n\nimport os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom PIL import Image\nimport json\nfrom pathlib import Path\nimport joblib\nimport warnings\nimport pickle\n\n# Albumentations for medical augmentations\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    \"\"\"Ensure reproducibility across all random operations\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nTEST_DIR = DATA_DIR / \"test\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"🚀 DenseNet V2 - Enhanced Medical Imaging Model\")\nprint(\"=\" * 60)\nprint(f\"📱 Device: {DEVICE}\")\nif torch.cuda.is_available():\n    print(f\"🔥 GPU: {torch.cuda.get_device_name()}\")\n    print(f\"💾 Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(\"=\" * 60)\n\n\n# QUICK RECOVERY: Load Auto-Saved Data (Run this after restarting kernel)\ndef quick_recovery():\n    \"\"\"Quick recovery of auto-saved data after kernel restart\"\"\"\n    global train_df, A, TAB, P, train_patients, val_patients\n    \n    print(\"🔄 QUICK RECOVERY MODE\")\n    print(\"=\" * 40)\n    \n    # Check Kaggle vs local environment\n    if os.path.exists('/kaggle/working/auto_save_data'):\n        auto_save_dir = \"/kaggle/working/auto_save_data\"\n        print(\"🐰 Using Kaggle persistent auto-save data\")\n    elif os.path.exists('auto_save_data'):\n        auto_save_dir = \"auto_save_data\"\n        print(\"🏠 Using local auto-save data\")\n    else:\n        print(\"❌ No auto-saved data found. Run full notebook first.\")\n        return False\n    \n    try:\n        # Load core data\n        print(\"📊 Loading core data...\")\n        train_df = pd.read_csv(f\"{auto_save_dir}/train_df_backup.csv\")\n        \n        with open(f\"{auto_save_dir}/decay_coefficients_A_backup.pkl\", 'rb') as f:\n            A = pickle.load(f)\n        \n        with open(f\"{auto_save_dir}/tabular_features_TAB_backup.pkl\", 'rb') as f:\n            TAB = pickle.load(f)\n        \n        with open(f\"{auto_save_dir}/patient_list_P_backup.pkl\", 'rb') as f:\n            P = pickle.load(f)\n        \n        print(f\"✅ Loaded: train_df ({train_df.shape}), A ({len(A)}), TAB ({len(TAB)}), P ({len(P)})\")\n        \n        # Load splits if available\n        if os.path.exists(f\"{auto_save_dir}/train_patients_backup.pkl\"):\n            print(\"🔄 Loading train/val splits...\")\n            \n            with open(f\"{auto_save_dir}/train_patients_backup.pkl\", 'rb') as f:\n                train_patients = pickle.load(f)\n            \n            with open(f\"{auto_save_dir}/val_patients_backup.pkl\", 'rb') as f:\n                val_patients = pickle.load(f)\n            \n            print(f\"✅ Loaded: train_patients ({len(train_patients)}), val_patients ({len(val_patients)})\")\n        \n        # Show metadata\n        if os.path.exists(f\"{auto_save_dir}/processing_metadata.json\"):\n            with open(f\"{auto_save_dir}/processing_metadata.json\", 'r') as f:\n                metadata = json.load(f)\n            print(f\"📅 Data from: {metadata.get('processing_timestamp', 'Unknown')}\")\n        \n        # Load model if available\n        if os.path.exists(f\"{auto_save_dir}/model_weights_backup.pth\"):\n            print(\"🏗️ Loading model...\")\n            try:\n                global model\n                model = WorkingDenseNetModel(tabular_dim=4).to(DEVICE)\n                model.load_state_dict(torch.load(f\"{auto_save_dir}/model_weights_backup.pth\", map_location=DEVICE))\n                print(\"✅ Model weights loaded\")\n            except:\n                print(\"⚠️ Model loading failed (need to run model definition cells first)\")\n        \n        # Show training results if available\n        if os.path.exists(f\"{auto_save_dir}/training_results_backup.json\"):\n            with open(f\"{auto_save_dir}/training_results_backup.json\", 'r') as f:\n                results = json.load(f)\n            print(f\"📈 Previous training: MAE = {results.get('best_val_mae', 'N/A')}\")\n        \n        print(\"🎉 Quick recovery complete! Core variables restored.\")\n        print(\"💡 Tip: If model loading failed, run model definition cells first, then call quick_recovery() again\")\n        return True\n        \n    except Exception as e:\n        print(f\"❌ Recovery failed: {e}\")\n        return False\n\nprint(\"✅ Quick recovery system ready!\")\nprint(\"💡 Usage after kernel restart:\")\nprint(\"   quick_recovery()  # Restore all auto-saved data\")\n# Uncomment the line below to auto-recover on kernel restart\n# quick_recovery()\n\n\n# Cell 2: Load Data and Create Tabular Features\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_tab_features(df_row):\n    \"\"\"Extract tabular features (returns 4 features)\"\"\"\n    vector = [(df_row['Age'] - 30) / 30] \n    \n    # Sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(0)\n    else:\n        vector.append(1)\n    \n    # Smoking status encoding\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([1, 1])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 1])\n    else:\n        vector.extend([1, 0])\n    return np.array(vector)\n\n# Calculate linear decay coefficients for each patient\nA = {} \nTAB = {} \nP = [] \n\nprint(\"Calculating linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy()\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, b = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n            TAB[patient] = get_tab_features(sub.iloc[0])\n            P.append(patient)\n        except:\n            # Use fallback method for patients with insufficient data\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0]) if len(weeks) > 1 else 0.0\n            TAB[patient] = get_tab_features(sub.iloc[0])\n            P.append(patient)\n    else:\n        A[patient] = 0.0\n        TAB[patient] = get_tab_features(sub.iloc[0])\n        P.append(patient)\n\nprint(f\"Processed {len(P)} patients with decay coefficients\")\n\n\n# Auto-Save: Critical Data After Processing\nprint(\"💾 Auto-saving critical data...\")\nimport os\nimport pickle\nimport json\nfrom datetime import datetime\n\n# Create auto-save directory (Kaggle-aware)\nif os.path.exists('/kaggle/working'):\n    auto_save_dir = \"/kaggle/working/auto_save_data\"\n    print(\"🐰 Using Kaggle persistent storage\")\nelse:\n    auto_save_dir = \"auto_save_data\"\n    print(\"🏠 Using local storage\")\n\nos.makedirs(auto_save_dir, exist_ok=True)\n\n# Save immediately after processing\ntry:\n    # Save core dataframes and dictionaries\n    train_df.to_csv(f\"{auto_save_dir}/train_df_backup.csv\", index=False)\n    \n    with open(f\"{auto_save_dir}/decay_coefficients_A_backup.pkl\", 'wb') as f:\n        pickle.dump(A, f)\n    \n    with open(f\"{auto_save_dir}/tabular_features_TAB_backup.pkl\", 'wb') as f:\n        pickle.dump(TAB, f)\n    \n    with open(f\"{auto_save_dir}/patient_list_P_backup.pkl\", 'wb') as f:\n        pickle.dump(P, f)\n    \n    # Save processing metadata\n    metadata = {\n        'processed_patients': len(P),\n        'total_decay_coefficients': len(A),\n        'tabular_features_dim': len(list(TAB.values())[0]) if TAB else 0,\n        'processing_timestamp': datetime.now().isoformat()\n    }\n    \n    with open(f\"{auto_save_dir}/processing_metadata.json\", 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(f\"✅ Auto-saved to {auto_save_dir}/\")\n    print(f\"   - train_df_backup.csv\")\n    print(f\"   - decay_coefficients_A_backup.pkl\") \n    print(f\"   - tabular_features_TAB_backup.pkl\")\n    print(f\"   - patient_list_P_backup.pkl\")\n    print(f\"   - processing_metadata.json\")\n    \nexcept Exception as e:\n    print(f\"⚠️ Auto-save failed: {e}\")\n\n\n\n# Cell 3: Medical-Specific Augmentations (FIXED)\nclass MedicalAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                # Geometric augmentations\n                albu.Rotate(limit=15, p=0.7),\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                \n                # Medical-specific augmentations\n                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                albu.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                \n                # Lung-specific augmentations for robustness\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                \n                # Cutout for robustness\n                albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                \n                # Normalization\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\n\n\n\n# Cell 4: Enhanced DenseNet Model with ALL Improvements\nclass UltraAdvancedDenseNetModel(nn.Module):\n    \"\"\"\n    Ultra-Enhanced DenseNet model with ALL improvements:\n    - Multi-scale feature extraction\n    - Cross-modal attention\n    - Uncertainty quantification\n    - Spatial attention\n    - Channel attention\n    - Feature pyramid network\n    \"\"\"\n    \n    def __init__(self, tabular_dim=4, dropout_rate=0.5):\n        super(UltraAdvancedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone with pretrained weights\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Multi-scale processing branches\n        self.scale_branches = nn.ModuleList([\n            self._create_scale_branch(kernel_size=7, stride=2, padding=3),  # Large scale\n            self._create_scale_branch(kernel_size=5, stride=2, padding=2),  # Medium scale\n            self._create_scale_branch(kernel_size=3, stride=2, padding=1),  # Small scale\n        ])\n        \n        # Feature pyramid network\n        self.fpn_conv1 = nn.Conv2d(1024, 512, 1)\n        self.fpn_conv2 = nn.Conv2d(1024, 512, 1)\n        self.fpn_conv3 = nn.Conv2d(1024, 512, 1)\n        \n        # Spatial attention mechanism\n        self.spatial_attention = SpatialAttention()\n        \n        # Channel attention mechanism\n        self.channel_attention = ChannelAttention(512 * 3)\n        \n        # Cross-modal attention\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=1536, num_heads=12, dropout=0.2, batch_first=True\n        )\n        \n        # Enhanced tabular processing with residual connections\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU()\n        )\n        \n        # Multi-modal fusion with attention\n        self.fusion_attention = nn.MultiheadAttention(\n            embed_dim=1568, num_heads=8, dropout=0.1, batch_first=True\n        )\n        \n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1568, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate/2)\n        )\n        \n        # Uncertainty quantification heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        # Initialize weights\n        self._initialize_weights()\n        \n    def _create_scale_branch(self, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Multi-scale processing\n        scale_features = []\n        for i, branch in enumerate(self.scale_branches):\n            if i == 0:\n                scale_feat = branch(images)\n            elif i == 1:\n                downsampled = F.avg_pool2d(images, kernel_size=2)\n                scale_feat = branch(downsampled)\n                scale_feat = F.interpolate(scale_feat, scale_factor=2, mode='bilinear', align_corners=False)\n            else:\n                downsampled = F.avg_pool2d(images, kernel_size=4)\n                scale_feat = branch(downsampled)\n                scale_feat = F.interpolate(scale_feat, scale_factor=4, mode='bilinear', align_corners=False)\n            \n            scale_features.append(scale_feat)\n        \n        # Concatenate multi-scale features\n        multi_scale = torch.cat(scale_features, dim=1)\n        \n        # Pass through DenseNet features\n        img_features = self.features(multi_scale)\n        \n        # Apply spatial attention\n        img_features = self.spatial_attention(img_features)\n        \n        # Feature pyramid processing\n        fpn1 = self.fpn_conv1(img_features)\n        fpn2 = self.fpn_conv2(img_features)\n        fpn3 = self.fpn_conv3(img_features)\n        \n        # Global pooling for each FPN level\n        fpn1_pool = F.adaptive_avg_pool2d(fpn1, (1, 1)).view(batch_size, -1)\n        fpn2_pool = F.adaptive_avg_pool2d(fpn2, (1, 1)).view(batch_size, -1)\n        fpn3_pool = F.adaptive_avg_pool2d(fpn3, (1, 1)).view(batch_size, -1)\n        \n        # Concatenate FPN features\n        fpn_combined = torch.cat([fpn1_pool, fpn2_pool, fpn3_pool], dim=1)\n        \n        # Apply channel attention\n        fpn_combined = self.channel_attention(fpn_combined.unsqueeze(-1).unsqueeze(-1))\n        fpn_combined = fpn_combined.view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Cross-modal attention\n        img_expanded = fpn_combined.unsqueeze(1)\n        tab_expanded = tab_features.unsqueeze(1)\n        \n        # Attention between image and tabular features\n        attended_img, _ = self.cross_attention(\n            img_expanded, tab_expanded, tab_expanded\n        )\n        attended_img = attended_img.squeeze(1)\n        \n        # Fusion with attention\n        combined_features = torch.cat([attended_img, tab_features], dim=1)\n        combined_expanded = combined_features.unsqueeze(1)\n        \n        fused_features, _ = self.fusion_attention(\n            combined_expanded, combined_expanded, combined_expanded\n        )\n        fused_features = fused_features.squeeze(1)\n        \n        # Final fusion\n        final_features = self.fusion_layer(fused_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(final_features)\n        log_var = self.log_var_head(final_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        x_cat = self.conv1(x_cat)\n        return x * self.sigmoid(x_cat)\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return x * self.sigmoid(out)\n        \n# Cell 5: Enhanced Dataset Class (FIXED)\nclass OSICDenseNetDataset(Dataset):\n    \"\"\"Enhanced dataset with medical augmentations and robust loading\"\"\"\n    \n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augment = augment\n        self.augmentor = MedicalAugmentation(augment=augment)\n        \n        # Prepare image paths for each patient\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        # Filter patients with available images\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        # For training, use multiple samples per patient\n        if self.split == 'train':\n            return len(self.valid_patients) * 6  # More augmented samples\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image for this patient\n        available_images = self.patient_images[patient]\n        if len(available_images) > 1:\n            selected_image = np.random.choice(available_images)\n        else:\n            selected_image = available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_and_preprocess_dicom(selected_image)\n        \n        # Apply augmentations\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (decay coefficient)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_and_preprocess_dicom(self, path):\n        \"\"\"Enhanced DICOM loading with better preprocessing\"\"\"\n        try:\n            # Load DICOM\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            # Handle different DICOM formats\n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]  # Take middle slice if 3D\n            \n            # Resize to target size\n            img = cv2.resize(img, (512, 512))\n            \n            # Normalize to 0-255 range\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            # Return a black image as fallback\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\n\n# Cell 5.1: Define PICP Loss and helper\nimport torch.nn.functional as F\n\ndef picp_loss(y_true, y_lower, y_upper, target_coverage=0.95):\n    \"\"\"\n    Prediction Interval Coverage Probability loss.\n    Penalizes intervals that cover less than target_coverage of the data.\n    \"\"\"\n    # Boolean mask of which targets lie inside the interval\n    in_interval = ((y_true >= y_lower) & (y_true <= y_upper)).float()\n    picp = in_interval.mean()           # actual coverage\n    penalty = F.relu(target_coverage - picp)\n    return penalty, picp.item()\n\n\n\n# Cell 6: CORRECTED Working Model with Fixed Dimensions\nclass WorkingDenseNetModel(nn.Module):\n    \"\"\"\n    CORRECTED model with proper dimension matching\n    \"\"\"\n    \n    def __init__(self, tabular_dim=4, dropout_rate=0.4):\n        super(WorkingDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Spatial attention\n        self.spatial_attention = SpatialAttention()\n        \n        # Enhanced tabular processing\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 256),  # Increased to 256\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 512),  # Final tabular features: 512\n            nn.BatchNorm1d(512),\n            nn.ReLU()\n        )\n        \n        # Cross-modal attention (fixed dimensions)\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=1024, num_heads=8, dropout=0.2, batch_first=True\n        )\n        \n        # Multi-modal fusion (corrected input size)\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 512, 768),  # 1024 (img) + 512 (tab) = 1536 -> 768\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate/2)\n        )\n        \n        # Uncertainty quantification heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 1)\n        )\n        \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)  # [B, 1024, H, W]\n        \n        # Apply spatial attention\n        img_features = self.spatial_attention(img_features)\n        \n        # Global average pooling\n        img_features = F.adaptive_avg_pool2d(img_features, (1, 1))\n        img_features = img_features.view(batch_size, -1)  # [B, 1024]\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)  # [B, 512]\n        \n        # Cross-modal attention\n        img_expanded = img_features.unsqueeze(1)  # [B, 1, 1024]\n        tab_expanded = tab_features.unsqueeze(1)  # [B, 1, 512]\n        \n        # Project tabular to same dimension for attention\n        tab_proj = F.linear(tab_expanded, \n                           torch.randn(1024, 512).to(images.device))  # [B, 1, 1024]\n        \n        attended_img, _ = self.cross_attention(\n            img_expanded, tab_proj, tab_proj\n        )\n        attended_img = attended_img.squeeze(1)  # [B, 1024]\n        \n        # Fusion\n        combined_features = torch.cat([attended_img, tab_features], dim=1)  # [B, 1536]\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nprint(\"✅ CORRECTED Working model defined!\")\n\n# Cell 6.5: Data Preparation and Loaders\nprint(\"🔄 Creating data loaders...\")\n\n# Split patients into train and validation\nfrom sklearn.model_selection import train_test_split\n\npatients_list = list(P)\ntrain_patients, val_patients = train_test_split(\n    patients_list, \n    test_size=0.2, \n    random_state=42,\n    shuffle=True\n)\n\nprint(f\"Train patients: {len(train_patients)}\")\nprint(f\"Validation patients: {len(val_patients)}\")\n\n# Create datasets\ntrain_dataset = OSICDenseNetDataset(\n    patients=train_patients,\n    A_dict=A,\n    TAB_dict=TAB,\n    data_dir=TRAIN_DIR,\n    split='train',\n    augment=True\n)\n\nval_dataset = OSICDenseNetDataset(\n    patients=val_patients,\n    A_dict=A,\n    TAB_dict=TAB,\n    data_dir=TRAIN_DIR,\n    split='val',\n    augment=False\n)\n\n# Create data loaders\n# Cell 6.5: DataLoaders (Optimized)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=True,\n    num_workers=6,\n    pin_memory=True,\n    drop_last=True,\n    persistent_workers=True\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    drop_last=False,\n    persistent_workers=True\n)\n\nprint(f\"✅ Data loaders created!\")\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n\n\n\n\n\n# Cell 7: Final Trainer with AMP, LLL, Full Metrics -------------------\n\nfrom torch.cuda.amp import autocast, GradScaler\n\n\n# Cell 7: Enhanced Trainer with PICP integrated\n\nclass PicpTrainer:\n    def __init__(self, model, device, lr=1e-4, lambda_picp=0.5):\n        self.model = model.to(device)\n        self.device = device\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        self.scaler = torch.cuda.amp.GradScaler()\n        self.lambda_picp = lambda_picp\n\n    def laplace_nll(self, y_true, y_pred, log_var):\n        sigma = torch.exp(log_var / 2.0)\n        sigma = torch.clamp(sigma, min=1.0, max=500.0)\n        abs_errors = torch.abs(y_true - y_pred)\n        log_likelihood = -torch.log(2.0 * sigma) - abs_errors / sigma\n        return -log_likelihood.mean()  # <--- do NOT use .item() here\n\n    def picp_loss(self, y_true, y_pred, log_var):\n        sigma = torch.exp(log_var / 2.0)\n        lower = y_pred - 1.96 * sigma\n        upper = y_pred + 1.96 * sigma\n        inside = ((y_true >= lower) & (y_true <= upper)).float()\n        picp = inside.mean()\n        return torch.abs(picp - 0.95)  # Encourages PICP ~ 0.95\n\n    def train(self, train_loader, val_loader, epochs=30, patience=5):\n        best_val_mae = float('inf')\n        patience_counter = 0\n\n        # 💡 Ensure all model params require grad\n        for p in self.model.parameters():\n            p.requires_grad = True\n\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n\n            for batch in train_loader:\n                self.optimizer.zero_grad()\n                with torch.cuda.amp.autocast():\n                    images, targets = batch\n                    images = images.to(self.device)\n                    targets = targets.to(self.device)\n\n                    outputs = self.model(images)\n                    preds, log_var = outputs[:, 0], outputs[:, 1]\n\n                    nll = self.laplace_nll(targets, preds, log_var)\n                    p_loss = self.picp_loss(targets, preds, log_var)\n                    loss = nll + self.lambda_picp * p_loss\n\n                if not loss.requires_grad:\n                    print(\"❌ Loss has no grad!\")\n                    continue\n\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n\n                train_loss += loss.item()\n\n            # 💡 You can log loss here\n            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n\n        return best_val_mae\n\n\n\nclass SimpleTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_mae = float('inf')\n        self.best_val_lll = float('inf')\n        self.scaler = GradScaler()\n\n    def uncertainty_loss(self, mean_pred, log_var, targets, reduction='mean'):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        uncertainty_penalty = torch.mean(torch.abs(log_var - torch.log(mse_loss + 1e-6)))\n        loss = 0.5 * (mse_loss / var + log_var) + 0.1 * uncertainty_penalty\n        return loss.mean() if reduction == 'mean' else loss.sum()\n\n    def laplace_log_likelihood(self, y_true, y_pred, log_var):\n        sigma = torch.exp(log_var / 2.0)\n        sigma = torch.clamp(sigma, min=5.0, max=500.0)\n        abs_errors = torch.abs(y_true - y_pred)\n        log_likelihood = -torch.log(2.0 * sigma) - abs_errors / sigma\n        return torch.mean(log_likelihood)\n\n    def train(self, train_loader, val_loader, epochs=30, patience=8):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, verbose=True)\n        patience_counter = 0\n\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss, train_mae, train_lll, train_batches = 0.0, 0.0, 0.0, 0\n\n            for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n                images = images.to(self.device, non_blocking=True)\n                tabular = tabular.to(self.device, non_blocking=True)\n                targets = targets.to(self.device, non_blocking=True)\n\n                optimizer.zero_grad()\n                with autocast():\n                    mean_pred, log_var = self.model(images, tabular)\n                    loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                    mae = F.l1_loss(mean_pred, targets)\n                    lll = self.laplace_log_likelihood(targets, mean_pred, log_var)\n\n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                self.scaler.step(optimizer)\n                self.scaler.update()\n\n                train_loss += loss.item()\n                train_mae += mae.item()\n                train_lll += lll.item()\n                train_batches += 1\n\n            self.model.eval()\n            val_loss, val_mae, val_lll = 0.0, 0.0, 0.0\n            val_predictions, val_targets, val_sigmas = [], [], []\n\n            with torch.no_grad():\n                for batch_idx, (images, tabular, targets, _) in enumerate(val_loader):\n                    images = images.to(self.device)\n                    tabular = tabular.to(self.device)\n                    targets = targets.to(self.device)\n\n                    with autocast():\n                        mean_pred, log_var = self.model(images, tabular)\n                        loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                        mae = F.l1_loss(mean_pred, targets)\n                        lll = self.laplace_log_likelihood(targets, mean_pred, log_var)\n\n                    sigma = torch.exp(log_var / 2.0)\n                    sigma = torch.clamp(sigma, min=50.0, max=500.0)\n\n                    val_loss += loss.item()\n                    val_mae += mae.item()\n                    val_lll += lll.item()\n                    val_predictions.extend(mean_pred.cpu().numpy())\n                    val_targets.extend(targets.cpu().numpy())\n                    val_sigmas.extend(sigma.cpu().numpy())\n\n            # Metrics and logging\n            if train_batches > 0 and len(val_predictions) > 0:\n                avg_train_loss = train_loss / train_batches\n                avg_train_mae = train_mae / train_batches\n                avg_train_lll = train_lll / train_batches\n\n                avg_val_loss = val_loss / len(val_loader)\n                avg_val_mae = val_mae / len(val_loader)\n                avg_val_lll = val_lll / len(val_loader)\n\n                val_predictions = np.array(val_predictions)\n                val_targets = np.array(val_targets)\n                val_sigmas = np.array(val_sigmas)\n\n                val_rmse = np.sqrt(np.mean((val_predictions - val_targets) ** 2))\n                ss_res = np.sum((val_targets - val_predictions) ** 2)\n                ss_tot = np.sum((val_targets - np.mean(val_targets)) ** 2)\n                r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else -float('inf')\n                avg_sigma = np.mean(val_sigmas)\n\n                print(f\"\\nEpoch {epoch+1}/{epochs}\")\n                print(f\"Train Loss: {avg_train_loss:.6f} | Train LLL: {avg_train_lll:.6f} | Train MAE: {avg_train_mae:.6f}\")\n                print(f\"Val Loss: {avg_val_loss:.6f} | Val LLL: {avg_val_lll:.6f} | MAE: {avg_val_mae:.6f} | RMSE: {val_rmse:.6f} | R²: {r2:.6f} | Sigma: {avg_sigma:.2f}\")\n\n                scheduler.step(avg_val_mae)\n\n                if avg_val_lll < self.best_val_lll:\n                    self.best_val_lll = avg_val_lll\n                    self.best_val_mae = avg_val_mae\n                    torch.save(self.model.state_dict(), 'best_working_model.pth')\n                    print(\"✅ New best model saved! (Best Laplace Log Likelihood)\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    if patience_counter >= patience:\n                        print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n                        break\n                print(\"-\" * 80)\n        return self.best_val_mae\n\nprint(\"✅ Enhanced trainer with AMP + LLL defined!\")\n\n\n\n\n\n\n\n\n\n# CORRECTED SimpleTrainer with FIXED sigma learning\nclass CorrectedSimpleTrainer:\n    \"\"\"\n    CORRECTED trainer with proper sigma learning and debugging\n    \"\"\"\n    \n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_mae = float('inf')\n        self.best_val_lll = float('inf')  # For actual log-likelihood (negative values)\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets, reduction='mean'):\n        \"\"\"Uncertainty-aware loss function\"\"\"\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        \n        # Add penalty for poor uncertainty estimation\n        uncertainty_penalty = torch.mean(torch.abs(log_var - torch.log(mse_loss + 1e-6)))\n        \n        loss = 0.5 * (mse_loss / var + log_var) + 0.05 * uncertainty_penalty\n        \n        if reduction == 'mean':\n            return loss.mean()\n        return loss.sum()\n        \n    def laplace_log_likelihood(self, y_true, y_pred, log_var):\n        \"\"\"\n        Calculate ACTUAL Laplace Log Likelihood\n        Returns actual log-likelihood (negative values, higher is better)\n        \"\"\"\n        # Convert log variance to standard deviation (sigma)\n        sigma = torch.exp(log_var / 2.0)\n        \n        # Much smaller bounds - allow dynamic learning!\n        sigma = torch.clamp(sigma, min=2.0, max=200.0)\n        \n        abs_errors = torch.abs(y_true - y_pred)\n        \n        # ACTUAL log-likelihood: log(1/(2σ)) - |y-μ|/σ\n        # = -log(2σ) - |y-μ|/σ\n        log_likelihood = -torch.log(2.0 * sigma) - abs_errors / sigma\n        \n        # Return ACTUAL log-likelihood (negative values, higher is better)\n        return torch.mean(log_likelihood)\n        \n    def train(self, train_loader, val_loader, epochs=30, patience=8):\n        # Better optimizer settings for uncertainty learning\n        optimizer = torch.optim.AdamW(\n            self.model.parameters(), \n            lr=5e-5,  # Lower learning rate for better uncertainty learning\n            weight_decay=1e-5\n        )\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=4, verbose=True  # mode='max' for log-likelihood\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training phase\n            self.model.train()\n            train_loss = 0.0\n            train_mae = 0.0\n            train_lll = 0.0\n            train_batches = 0\n            \n            for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n                try:\n                    images = images.to(self.device)\n                    tabular = tabular.to(self.device) \n                    targets = targets.to(self.device)\n                    \n                    optimizer.zero_grad()\n                    \n                    # Forward pass\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Calculate losses and metrics\n                    loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                    mae = F.l1_loss(mean_pred, targets)\n                    lll = self.laplace_log_likelihood(targets, mean_pred, log_var)\n                    \n                    # Backward pass\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                    optimizer.step()\n                    \n                    train_loss += loss.item()\n                    train_mae += mae.item()\n                    train_lll += lll.item()\n                    train_batches += 1\n                    \n                except Exception as e:\n                    print(f\"Error in training batch {batch_idx}: {e}\")\n                    continue\n            \n            # Validation phase\n            self.model.eval()\n            val_loss = 0.0\n            val_mae = 0.0\n            val_lll = 0.0\n            val_predictions = []\n            val_targets = []\n            val_sigmas = []\n            val_log_vars = []\n            \n            with torch.no_grad():\n                for batch_idx, (images, tabular, targets, _) in enumerate(val_loader):\n                    try:\n                        images = images.to(self.device)\n                        tabular = tabular.to(self.device)\n                        targets = targets.to(self.device)\n                        \n                        mean_pred, log_var = self.model(images, tabular)\n                        \n                        # Calculate all metrics\n                        loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                        mae = F.l1_loss(mean_pred, targets)\n                        lll = self.laplace_log_likelihood(targets, mean_pred, log_var)\n                        \n                        # Calculate sigma values for debugging\n                        sigma = torch.exp(log_var / 2.0)\n                        sigma = torch.clamp(sigma, min=2.0, max=200.0)\n                        \n                        val_loss += loss.item()\n                        val_mae += mae.item()\n                        val_lll += lll.item()\n                        \n                        val_predictions.extend(mean_pred.cpu().numpy())\n                        val_targets.extend(targets.cpu().numpy())\n                        val_sigmas.extend(sigma.cpu().numpy())\n                        val_log_vars.extend(log_var.cpu().numpy())\n                        \n                    except Exception as e:\n                        print(f\"Error in validation batch {batch_idx}: {e}\")\n                        continue\n            \n            # Calculate comprehensive metrics\n            if train_batches > 0 and len(val_predictions) > 0:\n                # Average training metrics\n                avg_train_loss = train_loss / train_batches\n                avg_train_mae = train_mae / train_batches\n                avg_train_lll = train_lll / train_batches\n                \n                # Average validation metrics\n                avg_val_loss = val_loss / len(val_loader)\n                avg_val_mae = val_mae / len(val_loader)\n                avg_val_lll = val_lll / len(val_loader)\n                \n                # Convert to numpy arrays for additional metrics\n                val_predictions = np.array(val_predictions)\n                val_targets = np.array(val_targets)\n                val_sigmas = np.array(val_sigmas)\n                val_log_vars = np.array(val_log_vars)\n                \n                # Calculate RMSE\n                val_rmse = np.sqrt(np.mean((val_predictions - val_targets) ** 2))\n                \n                # Calculate R²\n                ss_res = np.sum((val_targets - val_predictions) ** 2)\n                ss_tot = np.sum((val_targets - np.mean(val_targets)) ** 2)\n                r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else -float('inf')\n                \n                # Sigma statistics\n                avg_sigma = np.mean(val_sigmas)\n                min_sigma = np.min(val_sigmas)\n                max_sigma = np.max(val_sigmas)\n                \n                # Log variance statistics for debugging\n                avg_log_var = np.mean(val_log_vars)\n                \n                # Enhanced printing with all metrics and debugging info\n                print(f\"Epoch {epoch+1}/{epochs}\")\n                print(f\"Train Loss: {avg_train_loss:.6f} | Train LLL: {avg_train_lll:.6f} | Train MAE: {avg_train_mae:.6f}\")\n                print(f\"Val Loss: {avg_val_loss:.6f} | Val Laplace Log Likelihood: {avg_val_lll:.6f} | MAE: {avg_val_mae:.6f} | RMSE: {val_rmse:.6f} | R²: {r2:.6f}\")\n                print(f\"Sigma Stats: Avg={avg_sigma:.2f}, Range=[{min_sigma:.2f}, {max_sigma:.2f}] | Avg Log-Var: {avg_log_var:.4f}\")\n                \n                # Learning rate scheduling (using log-likelihood now)\n                scheduler.step(avg_val_lll)\n                \n                # Early stopping and model saving (using actual LLL - higher is better)\n                if avg_val_lll > self.best_val_lll:  # Higher log-likelihood is better\n                    self.best_val_lll = avg_val_lll\n                    self.best_val_mae = avg_val_mae\n                    torch.save(self.model.state_dict(), 'best_corrected_model.pth')\n                    print(\"✅ New best model saved! (Best Actual Log Likelihood)\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    \n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                    \n                print(\"-\" * 100)\n        \n        return self.best_val_mae\n\nprint(\"✅ CORRECTED SimpleTrainer defined with proper sigma learning!\")\n\n\n# Test and Retrain with CORRECTED Trainer\nprint(\"🔧 Testing CORRECTED trainer with proper sigma learning...\")\n\n# Load the best model if available\nif 'model' in globals():\n    try:\n        if os.path.exists('best_working_model.pth'):\n            model.load_state_dict(torch.load('best_working_model.pth'))\n            print(\"✅ Loaded best_working_model.pth as starting point\")\n        else:\n            print(\"⚠️ Starting with current model weights\")\n        \n        # Create CORRECTED trainer\n        corrected_trainer = CorrectedSimpleTrainer(model, DEVICE, lr=5e-5)\n        \n        # Test forward pass first\n        print(\"🔍 Testing model with corrected trainer...\")\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images = images.to(DEVICE)\n        tabular = tabular.to(DEVICE)\n        targets = targets.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n            sigma = torch.exp(log_var / 2.0)\n            sigma_clamped = torch.clamp(sigma, min=2.0, max=200.0)\n            lll = corrected_trainer.laplace_log_likelihood(targets, mean_pred, log_var)\n        \n        print(f\"✅ Test Results:\")\n        print(f\"   Raw log_var: {log_var[:5].detach().cpu().numpy()}\")\n        print(f\"   Raw sigma: {sigma[:5].detach().cpu().numpy()}\")\n        print(f\"   Clamped sigma: {sigma_clamped[:5].detach().cpu().numpy()}\")\n        print(f\"   Laplace Log Likelihood: {lll.item():.6f}\")\n        \n        print(\"\\n🚀 Starting CORRECTED training...\")\n        print(\"Expected improvements:\")\n        print(\"   - Dynamic sigma values (not fixed at 50.0)\")\n        print(\"   - Negative LLL values (better uncertainty)\")\n        print(\"   - Progress toward LLL = -6.0\")\n        print(\"=\" * 60)\n        \n        # Start corrected training\n        best_val_mae_corrected = corrected_trainer.train(\n            train_loader, \n            val_loader, \n            epochs=25, \n            patience=8\n        )\n        \n        print(f\"🎯 CORRECTED training completed!\")\n        print(f\"   Best validation MAE: {best_val_mae_corrected:.6f}\")\n        print(f\"   Best validation LLL: {corrected_trainer.best_val_lll:.6f}\")\n        \n    except Exception as e:\n        print(f\"❌ Error in corrected training: {e}\")\n        import traceback\n        traceback.print_exc()\n        \nelse:\n    print(\"❌ No model found. Run previous cells first!\")\n\n\n\n\n\n\n\n\n\n\n# Cell 8: Initialize Corrected Model and Test\nprint(\"🔄 Replacing with CORRECTED working model...\")\n\n# Delete old model\nif 'model' in globals():\n    del model\ntorch.cuda.empty_cache()\n\n# Initialize corrected model\nmodel = WorkingDenseNetModel(tabular_dim=4).to(DEVICE)\nprint(f\"✅ Corrected model initialized!\")\nprint(f\"📊 Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test model with actual batch\ntry:\n    if 'train_loader' in globals():\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images = images.to(DEVICE)\n        tabular = tabular.to(DEVICE)\n        \n        print(f\"🔍 Input shapes:\")\n        print(f\"   Images: {images.shape}\")\n        print(f\"   Tabular: {tabular.shape}\")\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n            print(f\"✅ Model forward pass successful!\")\n            print(f\"   Mean prediction: {mean_pred.shape} - {mean_pred[:3]}\")\n            print(f\"   Log variance: {log_var.shape} - {log_var[:3]}\")\n    else:\n        # Create a dummy test if data loaders aren't available\n        print(\"⚠️ Data loaders not found, creating dummy test...\")\n        dummy_images = torch.randn(2, 3, 512, 512).to(DEVICE)\n        dummy_tabular = torch.randn(2, 4).to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(dummy_images, dummy_tabular)\n            print(f\"✅ Model forward pass successful with dummy data!\")\n            print(f\"   Mean prediction: {mean_pred.shape} - {mean_pred}\")\n            print(f\"   Log variance: {log_var.shape} - {log_var}\")\n            \nexcept Exception as e:\n    print(f\"❌ Model test failed: {e}\")\n    import traceback\n    traceback.print_exc()\n\n\n\n# Cell 9: Start Training with PICP-aware Trainer\n# Cell: Start Training\nif 'model' in globals():\n    print(\"🚀 Starting training with CORRECTED model...\")\n    \n    trainer = PicpTrainer(model, DEVICE, lr=1e-4, lambda_picp=0.5)\n\n    best_val_mae = trainer.train(\n        train_loader, \n        val_loader, \n        epochs=30,\n        patience=8\n    )\n\n    print(f\"🎯 Training completed! Best validation MAE: {best_val_mae:.6f}\")\nelse:\n    print(\"❌ No model found. Run previous cells first!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T19:19:38.763819Z","iopub.execute_input":"2025-08-01T19:19:38.764462Z","iopub.status.idle":"2025-08-01T19:22:49.580214Z","shell.execute_reply.started":"2025-08-01T19:19:38.764432Z","shell.execute_reply":"2025-08-01T19:22:49.578955Z"}},"outputs":[{"name":"stdout","text":"🚀 DenseNet V2 - Enhanced Medical Imaging Model\n============================================================\n📱 Device: cuda\n🔥 GPU: Tesla P100-PCIE-16GB\n💾 Memory: 17.1 GB\n============================================================\n✅ Quick recovery system ready!\n💡 Usage after kernel restart:\n   quick_recovery()  # Restore all auto-saved data\nLoaded dataset with shape: (1549, 7)\nCalculating linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 176/176 [00:00<00:00, 1492.36it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with decay coefficients\n💾 Auto-saving critical data...\n🐰 Using Kaggle persistent storage\n✅ Auto-saved to /kaggle/working/auto_save_data/\n   - train_df_backup.csv\n   - decay_coefficients_A_backup.pkl\n   - tabular_features_TAB_backup.pkl\n   - patient_list_P_backup.pkl\n   - processing_metadata.json\n✅ CORRECTED Working model defined!\n🔄 Creating data loaders...\nTrain patients: 140\nValidation patients: 36\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 138 patients with images\nDataset val: 36 patients with images\n✅ Data loaders created!\nTrain batches: 103, Val batches: 5\n✅ Enhanced trainer with AMP + LLL defined!\n✅ CORRECTED SimpleTrainer defined with proper sigma learning!\n🔧 Testing CORRECTED trainer with proper sigma learning...\n✅ Loaded best_working_model.pth as starting point\n🔍 Testing model with corrected trainer...\n✅ Test Results:\n   Raw log_var: [1.4879323  0.8238814  1.240476   1.0700326  0.96476096]\n   Raw sigma: [2.1042647 1.509745  1.8593705 1.7074761 1.619926 ]\n   Clamped sigma: [2.1042647 2.        2.        2.        2.       ]\n   Laplace Log Likelihood: -3.345077\n\n🚀 Starting CORRECTED training...\nExpected improvements:\n   - Dynamic sigma values (not fixed at 50.0)\n   - Negative LLL values (better uncertainty)\n   - Progress toward LLL = -6.0\n============================================================\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 1: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 2: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 3: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 4: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 5: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 6: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 7: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 8: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 9: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 10: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 11: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 12: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 13: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 14: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 15: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 16: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 17: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 18: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 19: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 20: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 21: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 22: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 23: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 24: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 25: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 26: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 27: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 28: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 29: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 30: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 31: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 32: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 33: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 34: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 35: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 36: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 37: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 38: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 39: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 40: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 41: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 42: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 43: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 44: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 45: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 46: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 47: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 48: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 49: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 50: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 51: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 52: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 53: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 54: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 55: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 56: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 57: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 58: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 59: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 60: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 61: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 62: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 63: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 64: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 65: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 66: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 67: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 68: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 69: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 70: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 71: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 72: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 73: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 74: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 75: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 76: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 77: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 78: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 79: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 80: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 81: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 82: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 83: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 84: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 85: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 86: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 87: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 88: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 89: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 90: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 91: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 92: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 93: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 94: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 95: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 96: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 97: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 98: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 99: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 100: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 101: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 102: element 0 of tensors does not require grad and does not have a grad_fn\nError in training batch 0: element 0 of tensors does not require grad and does not have a grad_fn\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1808479210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m# Start corrected training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         best_val_mae_corrected = corrected_trainer.train(\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1808479210.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, epochs, patience)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m                     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                     \u001b[0mmean_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0;31m# Calculate losses and metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1803952276.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, tabular)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Extract image features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 1024, H, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Apply spatial attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"# Cell 10: Training Execution\nprint(\"Starting Progressive Training...\")\nbest_val_mae = trainer.train(\n    train_loader, \n    val_loader, \n    epochs=40, \n    patience=10\n)\n\n\n\n\n# Auto-Save: Model Training Results\nprint(\"💾 Auto-saving model training results...\")\n\ntry:\n    # Save training results\n    if 'best_val_mae' in globals():\n        training_results = {\n            'best_val_mae': float(best_val_mae),\n            'training_completed': True,\n            'training_timestamp': datetime.now().isoformat(),\n            'device_used': str(DEVICE),\n            'model_parameters': sum(p.numel() for p in model.parameters()) if 'model' in globals() else 0\n        }\n        \n        with open(f\"{auto_save_dir}/training_results_backup.json\", 'w') as f:\n            json.dump(training_results, f, indent=2)\n        \n        print(f\"✅ Training results saved: MAE = {best_val_mae:.6f}\")\n    \n    # Auto-save model weights if training completed\n    if 'model' in globals():\n        torch.save(model.state_dict(), f\"{auto_save_dir}/model_weights_backup.pth\")\n        print(\"✅ Model weights auto-saved\")\n    \n    # Save trainer state if available\n    if 'trainer' in globals():\n        trainer_state = {\n            'lr': trainer.lr,\n            'best_val_mae': float(trainer.best_val_mae) if hasattr(trainer, 'best_val_mae') else None,\n            'trainer_class': 'SimpleTrainer'\n        }\n        \n        with open(f\"{auto_save_dir}/trainer_state_backup.json\", 'w') as f:\n            json.dump(trainer_state, f, indent=2)\n        \n        print(\"✅ Trainer state auto-saved\")\n    \nexcept Exception as e:\n    print(f\"⚠️ Training auto-save failed: {e}\")\n\n\n\n# Cell 11: TTAPredictor for Enhanced Inference\nclass TTAPredictor:\n    def __init__(self, model, num_augmentations=5):\n        self.model = model\n        self.num_augmentations = num_augmentations\n        self.augmentor = MedicalAugmentation(augment=True)\n        self.model.eval()\n    \n    def predict(self, image, tabular):\n        # Original prediction\n        with torch.no_grad():\n            mean_pred, log_var = self.model(image.unsqueeze(0), tabular.unsqueeze(0))\n            mean_preds = [mean_pred.item()]\n            log_vars = [log_var.item()]\n        \n        # Augmented predictions\n        for _ in range(self.num_augmentations):\n            try:\n                # Apply augmentation\n                aug_img = self.augmentor(image.permute(1, 2, 0).numpy().astype(np.uint8))\n                aug_img = aug_img.to(DEVICE)\n                \n                # Predict\n                with torch.no_grad():\n                    mean_pred, log_var = self.model(aug_img.unsqueeze(0), tabular.unsqueeze(0))\n                    mean_preds.append(mean_pred.item())\n                    log_vars.append(log_var.item())\n                    \n            except Exception as e:\n                print(f\"Error in TTA: {e}\")\n                continue\n        \n        # Ensemble predictions\n        mean_ensemble = np.median(mean_preds)\n        log_var_ensemble = np.median(log_vars)\n        \n        # Calculate uncertainty (standard deviation)\n        std = np.sqrt(np.exp(log_var_ensemble))\n        \n        return mean_ensemble, std\n# Cell 13: Option 1 - Quick Confidence Head (Recommended - 5-10 minutes)\nprint(\"🔧 Adding Confidence Estimation to Existing Model...\")\n\nclass ConfidenceHead(nn.Module):\n    \"\"\"Simple confidence estimation head\"\"\"\n    def __init__(self, input_dim=256):\n        super(ConfidenceHead, self).__init__()\n        self.confidence_head = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Softplus()  # Ensures positive confidence values\n        )\n    \n    def forward(self, features):\n        return self.confidence_head(features)\n\nclass ModelWithConfidence(nn.Module):\n    \"\"\"Wrapper to add confidence to your existing model\"\"\"\n    def __init__(self, base_model):\n        super(ModelWithConfidence, self).__init__()\n        self.base_model = base_model\n        \n        # Freeze the base model\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        \n        # Add confidence head (takes features before final prediction)\n        self.confidence_head = ConfidenceHead(input_dim=256)  # Adjust based on your model\n        \n    def forward(self, images, tabular):\n        # Get features from your trained model (before final prediction)\n        with torch.no_grad():\n            # Access the fusion layer output from your model\n            batch_size = images.size(0)\n            img_features = self.base_model.features(images)\n            img_features = self.base_model.spatial_attention(img_features)\n            img_features = F.adaptive_avg_pool2d(img_features, (1, 1)).view(batch_size, -1)\n            tab_features = self.base_model.tabular_processor(tabular)\n            \n            # Get the fusion features (this is what we'll use for confidence)\n            combined_features = torch.cat([img_features, tab_features], dim=1)\n            fusion_features = self.base_model.fusion_layer(combined_features)\n        \n        # Get original FVC prediction\n        mean_pred, log_var = self.base_model(images, tabular)\n        \n        # Predict confidence using the fusion features\n        confidence = self.confidence_head(fusion_features.detach())\n        \n        return mean_pred, confidence.squeeze()\n\nprint(\"✅ ConfidenceHead and ModelWithConfidence classes defined!\")\n\n# Cell 14: Quick Confidence Trainer\nclass ConfidenceTrainer:\n    \"\"\"Quick trainer for confidence head only\"\"\"\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n        \n    def confidence_loss(self, fvc_pred, confidence, targets):\n        \"\"\"Loss that encourages reasonable confidence intervals\"\"\"\n        mse_loss = F.mse_loss(fvc_pred, targets)\n        \n        # Penalty for overconfident predictions (small confidence with large error)\n        errors = torch.abs(fvc_pred - targets)\n        confidence_penalty = torch.mean(errors / (confidence + 1e-6))\n        \n        # Penalty for underconfident predictions (very large confidence)\n        overconfidence_penalty = torch.mean(confidence) * 0.1\n        \n        return mse_loss + confidence_penalty + overconfidence_penalty\n    \n    def train_confidence(self, train_loader, val_loader, epochs=10):\n        \"\"\"Train only the confidence head - FAST!\"\"\"\n        # Only train the confidence head\n        optimizer = torch.optim.Adam(self.model.confidence_head.parameters(), lr=1e-3)\n        \n        print(f\"🚀 Training confidence head for {epochs} epochs...\")\n        \n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0\n            \n            for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n                try:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    \n                    optimizer.zero_grad()\n                    fvc_pred, confidence = self.model(images, tabular)\n                    loss = self.confidence_loss(fvc_pred, confidence, targets)\n                    loss.backward()\n                    optimizer.step()\n                    \n                    train_loss += loss.item()\n                except Exception as e:\n                    print(f\"Error in batch {batch_idx}: {e}\")\n                    continue\n            \n            # Validation\n            self.model.eval()\n            val_loss = 0\n            val_confidences = []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    try:\n                        images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                        fvc_pred, confidence = self.model(images, tabular)\n                        loss = self.confidence_loss(fvc_pred, confidence, targets)\n                        val_loss += loss.item()\n                        val_confidences.extend(confidence.cpu().numpy())\n                    except Exception as e:\n                        continue\n            \n            avg_train_loss = train_loss / len(train_loader)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_confidence = np.mean(val_confidences)\n            \n            print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Avg Confidence: {avg_confidence:.2f}\")\n        \n        torch.save(self.model.state_dict(), 'model_with_confidence.pth')\n        print(\"✅ Confidence model saved to 'model_with_confidence.pth'!\")\n        \n        return avg_val_loss\n\nprint(\"✅ ConfidenceTrainer class defined!\")\n\n\n\n# Cell 15: Option 2 - Quantile Regression Head (More Advanced)\nprint(\"📊 Defining Quantile Regression approach...\")\n\nclass QuantileRegressionHead(nn.Module):\n    \"\"\"Quantile regression for confidence intervals\"\"\"\n    def __init__(self, input_dim=256):\n        super(QuantileRegressionHead, self).__init__()\n        self.lower_quantile = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.upper_quantile = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n    \n    def forward(self, features):\n        lower = self.lower_quantile(features)\n        upper = self.upper_quantile(features)\n        return lower.squeeze(), upper.squeeze()\n\ndef quantile_loss(predictions, targets, quantile):\n    \"\"\"Quantile regression loss\"\"\"\n    errors = targets - predictions\n    return torch.mean(torch.max(quantile * errors, (quantile - 1) * errors))\n\nclass ModelWithQuantiles(nn.Module):\n    \"\"\"Model with quantile regression for confidence intervals\"\"\"\n    def __init__(self, base_model):\n        super(ModelWithQuantiles, self).__init__()\n        self.base_model = base_model\n        \n        # Freeze base model\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n            \n        self.quantile_head = QuantileRegressionHead()\n    \n    def forward(self, images, tabular):\n        # Get fusion features (same as confidence model)\n        batch_size = images.size(0)\n        with torch.no_grad():\n            img_features = self.base_model.features(images)\n            img_features = self.base_model.spatial_attention(img_features)\n            img_features = F.adaptive_avg_pool2d(img_features, (1, 1)).view(batch_size, -1)\n            tab_features = self.base_model.tabular_processor(tabular)\n            combined_features = torch.cat([img_features, tab_features], dim=1)\n            fusion_features = self.base_model.fusion_layer(combined_features)\n        \n        # Original prediction\n        mean_pred, _ = self.base_model(images, tabular)\n        \n        # Quantile predictions\n        lower_pred, upper_pred = self.quantile_head(fusion_features.detach())\n        \n        # Calculate confidence and final FVC\n        confidence = upper_pred - lower_pred\n        final_fvc = (lower_pred + upper_pred) / 2\n        \n        return final_fvc, confidence, lower_pred, upper_pred\n\nclass QuantileTrainer:\n    \"\"\"Trainer for quantile regression model\"\"\"\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n    \n    def train_quantiles(self, train_loader, val_loader, epochs=10):\n        optimizer = torch.optim.Adam(self.model.quantile_head.parameters(), lr=1e-3)\n        \n        print(f\"🚀 Training quantile regression for {epochs} epochs...\")\n        \n        best_val_loss = float('inf')\n        \n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0\n            \n            for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n                try:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    \n                    optimizer.zero_grad()\n                    final_fvc, confidence, lower_pred, upper_pred = self.model(images, tabular)\n                    \n                    # Quantile losses\n                    lower_loss = quantile_loss(lower_pred, targets, 0.2)  # 20th percentile\n                    upper_loss = quantile_loss(upper_pred, targets, 0.8)  # 80th percentile\n                    \n                    # Combined loss\n                    loss = lower_loss + upper_loss + F.mse_loss(final_fvc, targets)\n                    \n                    loss.backward()\n                    optimizer.step()\n                    \n                    train_loss += loss.item()\n                except Exception as e:\n                    continue\n            \n            # Validation\n            self.model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    try:\n                        images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                        final_fvc, confidence, lower_pred, upper_pred = self.model(images, tabular)\n                        \n                        lower_loss = quantile_loss(lower_pred, targets, 0.2)\n                        upper_loss = quantile_loss(upper_pred, targets, 0.8)\n                        loss = lower_loss + upper_loss + F.mse_loss(final_fvc, targets)\n                        val_loss += loss.item()\n                    except Exception as e:\n                        continue\n            \n            avg_val_loss = val_loss / len(val_loader)\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n            \n            print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {avg_val_loss:.4f}\")\n        \n        torch.save(self.model.state_dict(), 'quantile_model.pth')\n        print(\"✅ Quantile model saved!\")\n        \n        return best_val_loss\n\nprint(\"✅ Quantile Regression classes defined!\")\n\n\n# Cell 16: Execute Confidence Training\nprint(\"🚀 Setting up and training confidence estimation...\")\n\n# Check if we have a trained model\nif 'model' in globals() and 'train_loader' in globals():\n    try:\n        # Load the best model weights if available\n        import os\n        if os.path.exists('best_working_model.pth'):\n            model.load_state_dict(torch.load('best_working_model.pth'))\n            print(\"✅ Loaded best_working_model.pth\")\n        elif os.path.exists('best_densenet_model.pth'):\n            model.load_state_dict(torch.load('best_densenet_model.pth'))\n            print(\"✅ Loaded best_densenet_model.pth\")\n        else:\n            print(\"⚠️ No saved model found, using current model weights\")\n        \n        model.eval()\n        \n        # Create model with confidence (Option 1 - Recommended)\n        print(\"🔧 Creating ModelWithConfidence...\")\n        confidence_model = ModelWithConfidence(model).to(DEVICE)\n        \n        # Train confidence head (this is FAST - only 10 epochs!)\n        print(\"⚡ Training confidence head (Option 1)...\")\n        conf_trainer = ConfidenceTrainer(confidence_model, DEVICE)\n        conf_val_loss = conf_trainer.train_confidence(train_loader, val_loader, epochs=10)\n        \n        print(\"✅ Confidence training completed!\")\n        \n        # Optional: Also create quantile model for comparison\n        print(\"📊 Creating quantile model (Option 2)...\")\n        quantile_model = ModelWithQuantiles(model).to(DEVICE)\n        \n        print(\"⚡ Training quantile regression...\")\n        quant_trainer = QuantileTrainer(quantile_model, DEVICE)\n        quant_val_loss = quant_trainer.train_quantiles(train_loader, val_loader, epochs=8)\n        \n        # Compare models and save the better one\n        print(f\"\\n🏆 Model Comparison:\")\n        print(f\"   Confidence Model Val Loss: {conf_val_loss:.6f}\")\n        print(f\"   Quantile Model Val Loss: {quant_val_loss:.6f}\")\n        \n        if conf_val_loss <= quant_val_loss:\n            print(\"🥇 Confidence Model wins! Using Option 1\")\n            torch.save(confidence_model.state_dict(), 'best_confidence_model.pth')\n            best_model = confidence_model\n            best_model_type = \"confidence\"\n        else:\n            print(\"🥇 Quantile Model wins! Using Option 2\")\n            torch.save(quantile_model.state_dict(), 'best_confidence_model.pth')\n            best_model = quantile_model\n            best_model_type = \"quantile\"\n        \n        print(f\"✅ Best model saved as 'best_confidence_model.pth' (type: {best_model_type})\")\n        print(\"✅ Both confidence models ready!\")\n        \n    except Exception as e:\n        print(f\"❌ Error in confidence training: {e}\")\n        import traceback\n        traceback.print_exc()\n        \nelse:\n    print(\"⚠️ Model or data loaders not available. Run previous cells first!\")\n    print(\"Available variables:\", [var for var in globals().keys() if not var.startswith('_')])\n\n\n\n# Cell 17: Quick Submission Generator with Confidence\ndef create_submission_with_confidence(model, test_dir, output_file='enhanced_submission.csv'):\n    \"\"\"Create submission with confidence intervals\"\"\"\n    print(f\"📝 Creating submission with confidence intervals...\")\n    \n    # Load test data\n    try:\n        test_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n        print(f\"✅ Loaded test data: {len(test_df)} samples\")\n    except:\n        print(\"⚠️ Test data not found, creating sample submission format\")\n        # Create sample format for demonstration\n        test_df = pd.DataFrame({\n            'Patient': ['ID00000000000000000000000'] * 5,\n            'Weeks': [-12, -6, 0, 6, 12],\n            'FVC': [2000, 1950, 1900, 1850, 1800],\n            'Age': [65] * 5,\n            'Sex': ['Male'] * 5,\n            'SmokingStatus': ['Ex-smoker'] * 5\n        })\n    \n    submissions = []\n    model.eval()\n    \n    # Create augmentor for test time augmentation\n    test_augmentor = MedicalAugmentation(augment=False)\n    \n    print(\"🔄 Processing test patients...\")\n    \n    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n        patient_id = row['Patient']\n        weeks = row['Weeks']\n        \n        try:\n            # Load patient image\n            patient_dir = Path(test_dir) / patient_id\n            \n            # Default prediction values\n            fvc_pred = 2000.0  # Default FVC\n            confidence_val = 200.0  # Default confidence\n            \n            if patient_dir.exists():\n                image_files = list(patient_dir.glob('*.dcm'))\n                if image_files:\n                    # Load and preprocess image\n                    img = load_and_preprocess_dicom(image_files[0])\n                    img_tensor = test_augmentor(img).unsqueeze(0).to(DEVICE)\n                    \n                    # Prepare tabular features\n                    tab_features = get_tab_features(row)\n                    tab_tensor = torch.tensor(tab_features).float().unsqueeze(0).to(DEVICE)\n                    \n                    # Predict with confidence\n                    with torch.no_grad():\n                        if hasattr(model, 'confidence_head'):  # Option 1\n                            fvc_pred, confidence = model(img_tensor, tab_tensor)\n                            fvc_pred = fvc_pred.item()\n                            confidence_val = max(confidence.item(), 70)  # Minimum confidence\n                        elif hasattr(model, 'quantile_head'):  # Option 2\n                            final_fvc, confidence, lower_pred, upper_pred = model(img_tensor, tab_tensor)\n                            fvc_pred = final_fvc.item()\n                            confidence_val = max(confidence.item(), 70)\n                        else:\n                            # Fallback to base model\n                            mean_pred, log_var = model(img_tensor, tab_tensor)\n                            fvc_pred = mean_pred.item()\n                            confidence_val = max(torch.exp(log_var/2).item() * 100, 70)\n            \n            # Create submission rows for required weeks\n            for week in range(-12, 134):  # Standard competition range\n                patient_week = f\"{patient_id}_{week}\"\n                \n                # Adjust prediction based on time progression\n                if patient_id in A:\n                    time_adjusted_fvc = fvc_pred + (week - weeks) * A[patient_id]\n                else:\n                    time_adjusted_fvc = fvc_pred + (week - weeks) * (-7)  # Default decay\n                \n                # Ensure reasonable bounds\n                time_adjusted_fvc = max(time_adjusted_fvc, 800)  # Minimum FVC\n                time_adjusted_fvc = min(time_adjusted_fvc, 6000)  # Maximum FVC\n                \n                submissions.append({\n                    'Patient_Week': patient_week,\n                    'FVC': time_adjusted_fvc,\n                    'Confidence': confidence_val\n                })\n                \n        except Exception as e:\n            print(f\"⚠️ Error processing patient {patient_id}: {e}\")\n            # Use default values for this patient\n            for week in range(-12, 134):\n                patient_week = f\"{patient_id}_{week}\"\n                submissions.append({\n                    'Patient_Week': patient_week,\n                    'FVC': 2000.0,\n                    'Confidence': 200.0\n                })\n    \n    # Create submission dataframe\n    submission_df = pd.DataFrame(submissions)\n    submission_df.to_csv(output_file, index=False)\n    \n    print(f\"✅ Submission saved to {output_file}\")\n    print(f\"📊 Submission stats:\")\n    print(f\"   Total rows: {len(submission_df)}\")\n    print(f\"   FVC range: {submission_df['FVC'].min():.1f} - {submission_df['FVC'].max():.1f}\")\n    print(f\"   Confidence range: {submission_df['Confidence'].min():.1f} - {submission_df['Confidence'].max():.1f}\")\n    \n    return submission_df\n\n# Helper function for DICOM loading (simplified version)\ndef load_and_preprocess_dicom(path):\n    \"\"\"Simplified DICOM loading for submission\"\"\"\n    try:\n        dcm = pydicom.dcmread(str(path))\n        img = dcm.pixel_array.astype(np.float32)\n        \n        if len(img.shape) == 3:\n            img = img[img.shape[0]//2]\n        \n        img = cv2.resize(img, (512, 512))\n        \n        # Normalize to 0-255\n        img_min, img_max = img.min(), img.max()\n        if img_max > img_min:\n            img = (img - img_min) / (img_max - img_min) * 255\n        else:\n            img = np.zeros_like(img)\n        \n        # Convert to 3-channel\n        img = np.stack([img, img, img], axis=2).astype(np.uint8)\n        return img\n        \n    except Exception as e:\n        # Return black image as fallback\n        return np.zeros((512, 512, 3), dtype=np.uint8)\n\nprint(\"✅ Submission generator functions defined!\")\n\n\n# Cell 18: Generate Final Submission\nprint(\"🎯 Generating final submission with confidence intervals...\")\n\n# Generate submission using the best available model\ntry:\n    # First try to load the best confidence model\n    if 'best_model' in globals() and 'best_model_type' in globals():\n        print(f\"✅ Using best model: {best_model_type}\")\n        final_submission = create_submission_with_confidence(\n            best_model, \n            TEST_DIR, \n            f'enhanced_densenet_best_{best_model_type}_submission.csv'\n        )\n        chosen_model = f\"Best {best_model_type.title()} Model\"\n        \n    elif 'confidence_model' in globals():\n        print(\"✅ Using confidence model (Option 1)\")\n        final_submission = create_submission_with_confidence(\n            confidence_model, \n            TEST_DIR, \n            'enhanced_densenet_confidence_submission.csv'\n        )\n        chosen_model = \"Confidence Model\"\n        \n    elif 'quantile_model' in globals():\n        print(\"✅ Using quantile model (Option 2)\")\n        final_submission = create_submission_with_confidence(\n            quantile_model, \n            TEST_DIR, \n            'enhanced_densenet_quantile_submission.csv'\n        )\n        chosen_model = \"Quantile Model\"\n        \n    elif 'model' in globals():\n        print(\"✅ Using base model with uncertainty\")\n        final_submission = create_submission_with_confidence(\n            model, \n            TEST_DIR, \n            'enhanced_densenet_base_submission.csv'\n        )\n        chosen_model = \"Base Model\"\n        \n    else:\n        print(\"❌ No model available for submission\")\n        chosen_model = \"None\"\n        final_submission = None\n    \n    if final_submission is not None:\n        print(f\"\\n🎉 SUCCESS! Final submission created with {chosen_model}\")\n        print(f\"📁 File ready for competition upload!\")\n        \n        # Display final statistics\n        print(f\"\\n📊 Final Submission Statistics:\")\n        print(f\"   Model used: {chosen_model}\")\n        print(f\"   Total predictions: {len(final_submission)}\")\n        print(f\"   Unique patients: {len(set([p.split('_')[0] for p in final_submission['Patient_Week']]))}\")\n        print(f\"   FVC predictions range: {final_submission['FVC'].min():.1f} - {final_submission['FVC'].max():.1f}\")\n        print(f\"   Confidence range: {final_submission['Confidence'].min():.1f} - {final_submission['Confidence'].max():.1f}\")\n        print(f\"   Average confidence: {final_submission['Confidence'].mean():.1f}\")\n        \n        # Show sample predictions\n        print(f\"\\n📋 Sample predictions:\")\n        print(final_submission.head(10))\n        \n        # Save additional info\n        submission_info = {\n            'model_type': chosen_model,\n            'total_predictions': len(final_submission),\n            'fvc_range': [float(final_submission['FVC'].min()), float(final_submission['FVC'].max())],\n            'confidence_range': [float(final_submission['Confidence'].min()), float(final_submission['Confidence'].max())],\n            'avg_confidence': float(final_submission['Confidence'].mean())\n        }\n        \n        with open('submission_info.json', 'w') as f:\n            json.dump(submission_info, f, indent=2)\n        \n        print(f\"\\n💾 Additional files saved:\")\n        print(f\"   - submission_info.json (metadata)\")\n        print(f\"   - enhanced_densenet_*_submission.csv (main submission)\")\n        \nexcept Exception as e:\n    print(f\"❌ Error generating submission: {e}\")\n    import traceback\n    traceback.print_exc()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T19:22:49.581357Z","iopub.status.idle":"2025-08-01T19:22:49.581620Z","shell.execute_reply.started":"2025-08-01T19:22:49.581497Z","shell.execute_reply":"2025-08-01T19:22:49.581509Z"}},"outputs":[],"execution_count":null}]}
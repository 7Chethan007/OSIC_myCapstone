{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 12th Oct","metadata":{}},{"cell_type":"code","source":"    import os\n    import cv2\n    import pydicom\n    import pandas as pd\n    import numpy as np \n    import matplotlib.pyplot as plt \n    import random\n    from tqdm import tqdm \n    from sklearn.model_selection import train_test_split, StratifiedKFold\n    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader\n    import torchvision.models as models\n    from PIL import Image\n    import json\n    from pathlib import Path\n    import joblib\n    import warnings\n    import pickle\n    from typing import Dict, List, Tuple, Optional\n    from scipy import ndimage\n    from scipy.ndimage import binary_fill_holes, generate_binary_structure\n    from skimage import measure, morphology\n    from skimage.transform import resize\n    import albumentations as albu\n    from albumentations.pytorch import ToTensorV2\n\n    warnings.filterwarnings('ignore')\n\n    def seed_everything(seed=42):\n        \"\"\"Ensure reproducibility across all random operations\"\"\"\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n        \n    seed_everything(42)\n\n    # Configuration\n    DATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\n    TRAIN_DIR = DATA_DIR / \"train\"\n    TEST_DIR = DATA_DIR / \"test\"\n\n    # GPU Configuration for P100\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Enable optimizations for GPU training\n    if torch.cuda.is_available():\n        torch.backends.cudnn.benchmark = True  # Auto-tune for best performance on P100\n        torch.cuda.empty_cache()  # Clear cache before starting\n\n    print(\"ğŸš€ Fixed OSIC Model - Complete Working Version\")\n    print(\"=\" * 60)\n    print(f\"ğŸ“± Device: {DEVICE}\")\n    if torch.cuda.is_available():\n        print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name()}\")\n        print(f\"ğŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        print(f\"âš¡ CUDA Version: {torch.version.cuda}\")\n        print(f\"ğŸš€ cuDNN Benchmark: Enabled (P100 optimized)\")\n    print(\"=\" * 60)\n\n    # Load Data\n    train_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\n    print(f\"Loaded dataset with shape: {train_df.shape}\")\n\n    def get_tab_features(df_row):\n        \"\"\"Extract tabular features (returns 4 features)\"\"\"\n        vector = [(df_row['Age'] - 30) / 30] \n        \n        # Sex encoding\n        if df_row['Sex'] == 'Male':\n            vector.append(0)\n        else:\n            vector.append(1)\n        \n        # Smoking status encoding\n        smoking_status = df_row['SmokingStatus']\n        if smoking_status == 'Never smoked':\n            vector.extend([0, 0])\n        elif smoking_status == 'Ex-smoker':\n            vector.extend([1, 1])\n        elif smoking_status == 'Currently smokes':\n            vector.extend([0, 1])\n        else:\n            vector.extend([1, 0])\n        return np.array(vector)\n\n    # ğŸ”„ NEW APPROACH: Visit-level FVC prediction\n    # Instead of predicting one slope per patient, predict actual FVC at each visit\n    print(\"=\" * 60)\n    print(\"ğŸ“Š Creating Visit-Level Dataset\")\n    print(\"=\" * 60)\n\n    visit_data = []\n    patient_baselines = {}\n\n    # First pass: collect baseline information for each patient\n    for patient in train_df['Patient'].unique():\n        sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n        if len(sub) > 0:\n            # Get baseline (first measurement)\n            baseline_fvc = sub.iloc[0]['FVC']\n            baseline_percent = sub.iloc[0]['Percent']\n            baseline_week = sub.iloc[0]['Weeks']\n            \n            patient_baselines[patient] = {\n                'baseline_fvc': baseline_fvc,\n                'baseline_percent': baseline_percent,\n                'baseline_week': baseline_week,\n                'tabular': get_tab_features(sub.iloc[0])\n            }\n\n    # Second pass: create visit-level samples\n    print(\"Creating visit-level samples...\")\n    for patient in tqdm(train_df['Patient'].unique()):\n        sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n        \n        if patient not in patient_baselines:\n            continue\n        \n        baseline_info = patient_baselines[patient]\n        \n        # Create one sample for each visit\n        for idx, row in sub.iterrows():\n            visit_data.append({\n                'Patient': patient,\n                'Week': row['Weeks'],\n                'FVC': row['FVC'],  # Target: actual FVC at this visit\n                'Percent': row['Percent'],\n                'Age': row['Age'],\n                'Sex': row['Sex'],\n                'SmokingStatus': row['SmokingStatus'],\n                \n                # Temporal features (NEW!)\n                'WeeksFromBaseline': row['Weeks'] - baseline_info['baseline_week'],\n                'BaselineFVC': baseline_info['baseline_fvc'],\n                'BaselinePercent': baseline_info['baseline_percent'],\n                \n                # Tabular features\n                'tabular_features': baseline_info['tabular']\n            })\n\n    visit_df = pd.DataFrame(visit_data)\n\n    print(f\"\\nâœ… Dataset created:\")\n    print(f\"   Total visits: {len(visit_df)}\")\n    print(f\"   Unique patients: {visit_df['Patient'].nunique()}\")\n    print(f\"   Avg visits per patient: {len(visit_df) / visit_df['Patient'].nunique():.1f}\")\n    print(f\"   FVC range: [{visit_df['FVC'].min():.0f}, {visit_df['FVC'].max():.0f}] mL\")\n    print(f\"   Weeks range: [{visit_df['WeeksFromBaseline'].min():.0f}, {visit_df['WeeksFromBaseline'].max():.0f}]\")\n\n    class MedicalAugmentation:\n        def __init__(self, augment=True):\n            if augment:\n                self.transform = albu.Compose([\n                    albu.Rotate(limit=15, p=0.7),\n                    albu.HorizontalFlip(p=0.5),\n                    albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                    albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                    albu.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n                    albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                    albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                    albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                    albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                    ToTensorV2()\n                ])\n            else:\n                self.transform = albu.Compose([\n                    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                    ToTensorV2()\n                ])\n        \n        def __call__(self, image):\n            return self.transform(image=image)['image']\n\n    class SpatialAttention(nn.Module):\n        def __init__(self, kernel_size=7):\n            super(SpatialAttention, self).__init__()\n            self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n            self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n            avg_out = torch.mean(x, dim=1, keepdim=True)\n            max_out, _ = torch.max(x, dim=1, keepdim=True)\n            x_cat = torch.cat([avg_out, max_out], dim=1)\n            x_cat = self.conv1(x_cat)\n            return x * self.sigmoid(x_cat)\n\n    class WorkingDenseNetModel(nn.Module):\n        \"\"\"\n        ğŸ”„ UPDATED: Now predicts absolute FVC values (not slopes)\n        \n        Input:\n        - CT scan: 3 x 512 x 512\n        - Tabular: 7 features (Age, Sex, Smoking, WeeksFromBaseline, BaselineFVC, BaselinePercent)\n        \n        Output:\n        - FVC prediction (mL)\n        - Uncertainty (log variance)\n        \"\"\"\n        \n        def __init__(self, tabular_dim=7, dropout_rate=0.4):  # Changed from 4 to 7\n            super(WorkingDenseNetModel, self).__init__()\n            \n            # DenseNet121 backbone\n            densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n            self.features = densenet.features\n            \n            # Spatial attention\n            self.spatial_attention = SpatialAttention()\n            \n            # Enhanced tabular processing\n            self.tabular_processor = nn.Sequential(\n                nn.Linear(tabular_dim, 128),\n                nn.BatchNorm1d(128),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(128, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(256, 512),\n                nn.BatchNorm1d(512),\n                nn.ReLU()\n            )\n            \n            # Cross-modal attention\n            self.cross_attention = nn.MultiheadAttention(\n                embed_dim=1024, num_heads=8, dropout=0.2, batch_first=True\n            )\n            \n            # Initialize projection weight properly\n            self.tab_projection = nn.Linear(512, 1024)\n            \n            # Multi-modal fusion\n            self.fusion_layer = nn.Sequential(\n                nn.Linear(1024 + 512, 768),\n                nn.BatchNorm1d(768),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate),\n                nn.Linear(768, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate/2)\n            )\n            \n            # Uncertainty quantification heads\n            self.mean_head = nn.Sequential(\n                nn.Linear(256, 128),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 1)\n            )\n            \n            self.log_var_head = nn.Sequential(\n                nn.Linear(256, 64),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(64, 1)\n            )\n            \n        def forward(self, images, tabular):\n            batch_size = images.size(0)\n            \n            # Extract image features\n            img_features = self.features(images)\n            \n            # Apply spatial attention\n            img_features = self.spatial_attention(img_features)\n            \n            # Global average pooling\n            img_features = F.adaptive_avg_pool2d(img_features, (1, 1))\n            img_features = img_features.view(batch_size, -1)\n            \n            # Process tabular data\n            tab_features = self.tabular_processor(tabular)\n            \n            # Cross-modal attention\n            img_expanded = img_features.unsqueeze(1)\n            tab_expanded = tab_features.unsqueeze(1)\n            \n            # Project tabular to same dimension for attention\n            tab_proj = self.tab_projection(tab_expanded)\n            \n            attended_img, _ = self.cross_attention(\n                img_expanded, tab_proj, tab_proj\n            )\n            attended_img = attended_img.squeeze(1)\n            \n            # Fusion\n            combined_features = torch.cat([attended_img, tab_features], dim=1)\n            fused_features = self.fusion_layer(combined_features)\n            \n            # Predict mean and log variance\n            mean_pred = self.mean_head(fused_features)\n            log_var = self.log_var_head(fused_features)\n            \n            return mean_pred.squeeze(), log_var.squeeze()\n\n    class OSICVisitLevelDataset(Dataset):\n        \"\"\"\n        ğŸ”„ NEW: Visit-level dataset for FVC prediction\n        \n        Each sample represents ONE visit (patient at specific timepoint):\n        - Input: CT scan + tabular + temporal features\n        - Target: FVC value at that visit\n        \"\"\"\n        \n        def __init__(self, visit_dataframe, data_dir, split='train', augment=True):\n            self.visit_df = visit_dataframe.reset_index(drop=True)\n            self.data_dir = Path(data_dir)\n            self.split = split\n            self.augment = augment\n            self.augmentor = MedicalAugmentation(augment=augment)\n            \n            # Filter out problematic patients\n            problematic = ['ID00011637202177653955184', 'ID00052637202186188008618']\n            self.visit_df = self.visit_df[~self.visit_df['Patient'].isin(problematic)]\n            \n            # Prepare image paths for each patient (one CT per patient, used for all visits)\n            # ğŸ”„ IMPROVEMENT: Store ALL slices, will select based on visit context\n            self.patient_images = {}\n            for patient in self.visit_df['Patient'].unique():\n                patient_dir = self.data_dir / patient\n                if patient_dir.exists():\n                    image_files = sorted([f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm'])\n                    if image_files:\n                        # Store ALL slices instead of just middle one\n                        self.patient_images[patient] = image_files\n            \n            # Filter visits with available images\n            self.visit_df = self.visit_df[self.visit_df['Patient'].isin(self.patient_images.keys())]\n            \n            print(f\"Dataset {split}: {len(self.visit_df)} visits from {self.visit_df['Patient'].nunique()} patients\")\n        \n        def __len__(self):\n            return len(self.visit_df)\n        \n        def __getitem__(self, idx):\n            row = self.visit_df.iloc[idx]\n            patient = row['Patient']\n            \n            # ğŸ”„ IMPROVED: Select slice based on temporal context\n            # Later visits might show more severe disease â†’ use different slices\n            image_files = self.patient_images[patient]\n            \n            if self.augment:\n                # Training: Add variety by randomly selecting from middle 60% of slices\n                start_idx = len(image_files) // 5\n                end_idx = len(image_files) * 4 // 5\n                selected_image = image_files[np.random.randint(start_idx, max(start_idx + 1, end_idx))]\n            else:\n                # Validation: Use consistent middle slice\n                selected_image = image_files[len(image_files) // 2]\n            \n            # Load and preprocess image\n            img = self.load_and_preprocess_dicom(selected_image)\n            \n            # Apply augmentations\n            img_tensor = self.augmentor(img)\n            \n            # ğŸ”„ NEW: Enhanced tabular features with temporal information\n            tab_features = np.concatenate([\n                row['tabular_features'],  # Age, Sex, Smoking (4 features)\n                [(row['WeeksFromBaseline'] + 12) / 52],  # Normalized weeks (1 feature)\n                [row['BaselineFVC'] / 1000],  # Normalized baseline FVC (1 feature)\n                [row['BaselinePercent'] / 100]  # Baseline FVC % (1 feature)\n            ])  # Total: 7 features (was 4)\n            \n            tab_features = torch.tensor(tab_features, dtype=torch.float32)\n            \n            # ğŸ¯ NEW TARGET: Actual FVC value (not slope)\n            target_fvc = torch.tensor(row['FVC'], dtype=torch.float32)\n            \n            return img_tensor, tab_features, target_fvc, patient\n        \n        def load_and_preprocess_dicom(self, path):\n            \"\"\"Enhanced DICOM loading with better preprocessing\"\"\"\n            try:\n                # Load DICOM\n                dcm = pydicom.dcmread(str(path))\n                img = dcm.pixel_array.astype(np.float32)\n                \n                # Handle different DICOM formats\n                if len(img.shape) == 3:\n                    img = img[img.shape[0]//2]\n                \n                # Resize to target size\n                img = cv2.resize(img, (512, 512))\n                \n                # Normalize to 0-255 range\n                img_min, img_max = img.min(), img.max()\n                if img_max > img_min:\n                    img = (img - img_min) / (img_max - img_min) * 255\n                else:\n                    img = np.zeros_like(img)\n                \n                # Convert to 3-channel\n                img = np.stack([img, img, img], axis=2).astype(np.uint8)\n                \n                return img\n                \n            except Exception as e:\n                print(f\"Error loading DICOM {path}: {e}\")\n                return np.zeros((512, 512, 3), dtype=np.uint8)\n\n    class SimpleTrainer:\n        \"\"\"\n        Simple trainer that works with any model structure\n        \n        ğŸ¯ RESEARCH PAPER CONFIGURATION:\n        - Primary metric: MAE (Mean Absolute Error) - standard for regression papers\n        - Secondary metric: Laplace LL (Log Likelihood) - for competitive comparison\n        - Model saving: Best MAE (recommended for publication)\n        \n        ğŸ“Š TO TOGGLE TRAINING LOSS:\n        Set use_laplace_loss=True to train with Laplace LL instead of uncertainty loss\n        \"\"\"\n        \n        def __init__(self, model, device, lr=1e-4, use_laplace_loss=False):\n            self.model = model\n            self.device = device\n            self.lr = lr\n            self.use_laplace_loss = use_laplace_loss  # ğŸ¯ Toggle between MAE and LLL training\n            self.best_val_mae = float('inf')\n            self.best_laplace_ll = float('-inf')\n            \n            print(f\"\\n{'='*60}\")\n            print(f\"ğŸ¯ TRAINER CONFIGURATION\")\n            print(f\"{'='*60}\")\n            print(f\"Training Loss: {'Laplace Log Likelihood' if use_laplace_loss else 'Uncertainty Loss (MAE-based)'}\")\n            print(f\"Model Saving: Best MAE (Research Paper Standard)\")\n            print(f\"Monitoring: MAE, RMSE, RÂ², Correlation, Laplace LL\")\n            print(f\"{'='*60}\\n\")\n            \n        def uncertainty_loss(self, mean_pred, log_var, targets, reduction='mean'):\n            \"\"\"Uncertainty-aware loss function\"\"\"\n            var = torch.exp(log_var)\n            mse_loss = (mean_pred - targets) ** 2\n            loss = 0.5 * (mse_loss / var + log_var)\n            \n            if reduction == 'mean':\n                return loss.mean()\n            return loss.sum()\n        \n        def laplace_log_likelihood(self, mean_pred, log_var, targets, return_tensor=False):\n            \"\"\"\n            Calculate Laplace Log Likelihood (OSIC competition metric)\n            \n            ğŸ¯ DUAL USE:\n            - Monitoring: return_tensor=False (default) â†’ returns scalar for logging\n            - Training: return_tensor=True â†’ returns tensor for backprop\n            \n            Higher is better (closer to 0 from negative)\n            \n            Formula: -âˆš2 * |pred - actual| / Ïƒ - log(âˆš2 * Ïƒ)\n            where Ïƒ = sqrt(exp(log_var)), clipped to minimum 70\n            \n            Competition Rule: Minimum Ïƒ = 70mL (cannot be more confident than Â±70mL)\n            \"\"\"\n            sigma = torch.sqrt(torch.exp(log_var))\n            sigma_clipped = torch.clamp(sigma, min=70)  # Competition rule: min confidence = 70\n            \n            delta = torch.abs(mean_pred - targets)\n            \n            # Laplace log likelihood\n            ll = -np.sqrt(2) * delta / sigma_clipped - torch.log(np.sqrt(2) * sigma_clipped)\n            \n            if return_tensor:\n                return ll.mean()  # Return tensor for backprop\n            else:\n                return ll.mean().item()  # Return scalar for logging\n        \n        def calculate_r2_score(self, predictions, targets):\n            \"\"\"Calculate RÂ² score\"\"\"\n            predictions = np.array(predictions)\n            targets = np.array(targets)\n            ss_res = np.sum((targets - predictions) ** 2)\n            ss_tot = np.sum((targets - np.mean(targets)) ** 2)\n            r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n            return r2\n            \n        def train(self, train_loader, val_loader, epochs=30, patience=8):\n            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, mode='min', factor=0.5, patience=4, verbose=True\n            )\n            \n            # Mixed precision training for P100 speedup\n            scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n            use_amp = torch.cuda.is_available()\n            \n            if use_amp:\n                print(\"âš¡ Mixed Precision Training ENABLED for P100\")\n            \n            patience_counter = 0\n            \n            for epoch in range(epochs):\n                # Training phase\n                self.model.train()\n                train_loss = 0.0\n                train_mae = 0.0\n                train_batches = 0\n                \n                for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n                    try:\n                        images = images.to(self.device)\n                        tabular = tabular.to(self.device) \n                        targets = targets.to(self.device)\n                        \n                        optimizer.zero_grad()\n                        \n                        # Mixed precision forward pass\n                        if use_amp:\n                            with torch.cuda.amp.autocast():\n                                mean_pred, log_var = self.model(images, tabular)\n                                \n                                # ğŸ¯ AUTOMATIC LOSS SELECTION based on initialization flag\n                                if self.use_laplace_loss:\n                                    # Option A: Train with Laplace LL (competition metric)\n                                    loss = -self.laplace_log_likelihood(mean_pred, log_var, targets, return_tensor=True)\n                                else:\n                                    # Option B: Train with Uncertainty Loss (MAE-based, research standard)\n                                    loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                                \n                                mae = F.l1_loss(mean_pred, targets)\n                        else:\n                            mean_pred, log_var = self.model(images, tabular)\n                            \n                            # ğŸ¯ AUTOMATIC LOSS SELECTION based on initialization flag\n                            if self.use_laplace_loss:\n                                # Option A: Train with Laplace LL (competition metric)\n                                loss = -self.laplace_log_likelihood(mean_pred, log_var, targets, return_tensor=True)\n                            else:\n                                # Option B: Train with Uncertainty Loss (MAE-based, research standard)\n                                loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                            \n                            mae = F.l1_loss(mean_pred, targets)\n                        \n                        # Mixed precision backward pass\n                        if use_amp:\n                            scaler.scale(loss).backward()\n                            scaler.unscale_(optimizer)\n                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                            scaler.step(optimizer)\n                            scaler.update()\n                        else:\n                            loss.backward()\n                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                            optimizer.step()\n                        \n                        train_loss += loss.item()\n                        train_mae += mae.item()\n                        train_batches += 1\n                        \n                    except Exception as e:\n                        print(f\"Error in training batch {batch_idx}: {e}\")\n                        continue\n                \n                # Validation phase\n                self.model.eval()\n                val_loss = 0.0\n                val_mae = 0.0\n                val_predictions = []\n                val_targets = []\n                \n                with torch.no_grad():\n                    for batch_idx, (images, tabular, targets, _) in enumerate(val_loader):\n                        try:\n                            images = images.to(self.device)\n                            tabular = tabular.to(self.device)\n                            targets = targets.to(self.device)\n                            \n                            mean_pred, log_var = self.model(images, tabular)\n                            \n                            loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                            mae = F.l1_loss(mean_pred, targets)\n                            \n                            val_loss += loss.item()\n                            val_mae += mae.item()\n                            \n                            val_predictions.extend(mean_pred.cpu().numpy())\n                            val_targets.extend(targets.cpu().numpy())\n                            \n                        except Exception as e:\n                            print(f\"Error in validation batch {batch_idx}: {e}\")\n                            continue\n                \n                # Calculate metrics\n                if train_batches > 0 and len(val_predictions) > 0:\n                    avg_train_loss = train_loss / train_batches\n                    avg_train_mae = train_mae / train_batches\n                    avg_val_loss = val_loss / len(val_loader)\n                    avg_val_mae = val_mae / len(val_loader)\n                    \n                    # Calculate RÂ² score\n                    train_r2 = self.calculate_r2_score(\n                        [p.item() for p in self.model(images, tabular)[0][-8:]] if train_batches > 0 else [],\n                        [t.item() for t in targets[-8:]] if train_batches > 0 else []\n                    ) if train_batches > 0 else 0\n                    \n                    val_r2 = self.calculate_r2_score(val_predictions, val_targets)\n                    \n                    # Calculate Laplace Log Likelihood for validation\n                    val_laplace_ll = 0.0\n                    with torch.no_grad():\n                        for batch_idx, (images, tabular, targets, _) in enumerate(val_loader):\n                            images = images.to(self.device)\n                            tabular = tabular.to(self.device)\n                            targets = targets.to(self.device)\n                            mean_pred, log_var = self.model(images, tabular)\n                            val_laplace_ll += self.laplace_log_likelihood(mean_pred, log_var, targets)\n                    \n                    avg_val_laplace = val_laplace_ll / len(val_loader)\n                    \n                    # Calculate correlation\n                    val_corr = np.corrcoef(val_predictions, val_targets)[0, 1] if len(val_predictions) > 1 else 0\n                    \n                    print(f\"Epoch {epoch+1}/{epochs}\")\n                    print(f\"Train - Loss: {avg_train_loss:.6f} | MAE: {avg_train_mae:.1f} mL\")\n                    print(f\"Val   - Loss: {avg_val_loss:.6f} | MAE: {avg_val_mae:.1f} mL\")\n                    print(f\"Val   - RÂ²: {val_r2:.4f} | Correlation: {val_corr:.4f}\")\n                    print(f\"Val   - Laplace LL: {avg_val_laplace:.6f}\")\n                    print(f\"Val   - RMSE: {np.sqrt(mean_squared_error(val_targets, val_predictions)):.1f} mL\")\n                    \n                    # Learning rate scheduling\n                    scheduler.step(avg_val_mae)\n                    \n                    # ğŸ¯ RESEARCH PAPER APPROACH: Save based on MAE (primary metric)\n                    # Also track Laplace LL for competitive comparison\n                    \n                    improved = False\n                    \n                    # Primary: Save best MAE model (research paper standard)\n                    if avg_val_mae < self.best_val_mae:\n                        self.best_val_mae = avg_val_mae\n                        torch.save(self.model.state_dict(), 'best_working_model_mae.pth')\n                        print(f\"âœ… NEW BEST MAE! Model saved: {avg_val_mae:.1f} mL (Laplace LL: {avg_val_laplace:.4f})\")\n                        improved = True\n                        patience_counter = 0\n                    \n                    # Secondary: Also save best Laplace LL model (for comparison)\n                    if avg_val_laplace > self.best_laplace_ll:\n                        self.best_laplace_ll = avg_val_laplace\n                        torch.save(self.model.state_dict(), 'best_working_model_laplace.pth')\n                        print(f\"ğŸ’¾ Best Laplace LL model saved: {avg_val_laplace:.4f} (MAE: {avg_val_mae:.1f} mL)\")\n                        if not improved:\n                            patience_counter = 0\n                    \n                    if not improved and avg_val_laplace <= self.best_laplace_ll:\n                        patience_counter += 1\n                        \n                    if patience_counter >= patience:\n                        print(f\"Early stopping at epoch {epoch+1}\")\n                        break\n                        \n                    print(\"-\" * 60)\n            \n            print(f\"\\nğŸ† Training Summary:\")\n            print(f\"   Best Validation MAE: {self.best_val_mae:.6f}\")\n            print(f\"   Best Laplace Log Likelihood: {self.best_laplace_ll:.6f}\")\n            \n            return self.best_val_mae, self.best_laplace_ll\n\n    # Main execution\n    def main():\n        print(\"ğŸ”„ Creating data loaders...\")\n        print(\"=\" * 60)\n        \n        # ğŸ¯ PATIENT-LEVEL SPLIT (prevents data leakage!)\n        # All visits from same patient stay together\n        unique_patients = visit_df['Patient'].unique()\n        train_patients, val_patients = train_test_split(\n            unique_patients, \n            test_size=0.2, \n            random_state=42,\n            shuffle=True\n        )\n        \n        # Split visits by patient\n        train_visits = visit_df[visit_df['Patient'].isin(train_patients)]\n        val_visits = visit_df[visit_df['Patient'].isin(val_patients)]\n        \n        print(f\"ğŸ“Š Data Split:\")\n        print(f\"   Train: {len(train_patients)} patients, {len(train_visits)} visits\")\n        print(f\"   Val:   {len(val_patients)} patients, {len(val_visits)} visits\")\n        print(f\"   Train FVC: {train_visits['FVC'].mean():.0f} Â± {train_visits['FVC'].std():.0f} mL\")\n        print(f\"   Val FVC:   {val_visits['FVC'].mean():.0f} Â± {val_visits['FVC'].std():.0f} mL\")\n        print(\"=\" * 60)\n        \n        # Create datasets\n        train_dataset = OSICVisitLevelDataset(\n            visit_dataframe=train_visits,\n            data_dir=TRAIN_DIR,\n            split='train',\n            augment=True\n        )\n        \n        val_dataset = OSICVisitLevelDataset(\n            visit_dataframe=val_visits,\n            data_dir=TRAIN_DIR,\n            split='val',\n            augment=False\n        )\n        \n        # Create data loaders optimized for P100\n        # P100 has 16GB memory - can handle larger batches\n        batch_size = 16 if torch.cuda.is_available() else 8\n        num_workers = 4 if torch.cuda.is_available() else 2  # P100 benefits from more workers\n        \n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=num_workers,\n            pin_memory=True if torch.cuda.is_available() else False,\n            drop_last=True,\n            persistent_workers=True if num_workers > 0 else False  # Keep workers alive\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True if torch.cuda.is_available() else False,\n            drop_last=False,\n            persistent_workers=True if num_workers > 0 else False\n        )\n        \n        print(f\"âœ… Data loaders created!\")\n        print(f\"   Batch size: {batch_size} (P100 optimized)\")\n        print(f\"   Train batches: {len(train_loader)}\")\n        print(f\"   Val batches: {len(val_loader)}\")\n        print(f\"   Workers: {num_workers}\")\n        \n        # Initialize model\n        print(\"ğŸ”„ Initializing model...\")\n        model = WorkingDenseNetModel(tabular_dim=7).to(DEVICE)  # 7 features now\n        print(f\"âœ… Model initialized!\")\n        print(f\"ğŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n        print(f\"ğŸ“Š Output: FVC value (mL) + uncertainty\")\n        \n        # Test model with actual batch\n        try:\n            test_batch = next(iter(train_loader))\n            images, tabular, targets, _ = test_batch\n            images = images.to(DEVICE)\n            tabular = tabular.to(DEVICE)\n            \n            print(f\"ğŸ” Testing model...\")\n            with torch.no_grad():\n                mean_pred, log_var = model(images, tabular)\n                print(f\"âœ… Model forward pass successful!\")\n                \n        except Exception as e:\n            print(f\"âŒ Model test failed: {e}\")\n            return\n        \n        # ğŸ¯ TRAINING CONFIGURATION\n        # Toggle between MAE-based (uncertainty_loss) and LLL-based training\n        \n        # ============================================================\n        # ğŸ“Š OPTION 1: MAE-Based Training (RECOMMENDED FOR PAPERS)\n        # ============================================================\n        USE_LAPLACE_LOSS = False  # â† Set to True to train with Laplace LL instead\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"ğŸ¯ TRAINING CONFIGURATION FOR RESEARCH PAPER\")\n        print(\"=\"*60)\n        print(f\"Training Loss: {'Laplace Log Likelihood' if USE_LAPLACE_LOSS else 'Uncertainty Loss (MAE-based)'}\")\n        print(f\"Model Saving: Best MAE (primary) + Best Laplace LL (secondary)\")\n        print(f\"Monitoring: All metrics (MAE, RMSE, RÂ², Correlation, Laplace LL)\")\n        print(\"=\"*60 + \"\\n\")\n        \n        # Create trainer and start training\n        print(\"ğŸš€ Starting training...\")\n        trainer = SimpleTrainer(model, DEVICE, lr=1e-4, use_laplace_loss=USE_LAPLACE_LOSS)\n        \n        best_val_mae, best_laplace_ll = trainer.train(\n            train_loader, \n            val_loader, \n            epochs=30,\n            patience=8\n        )\n        \n        print(f\"\\nğŸ¯ Training completed!\")\n        print(f\"   Best MAE: {best_val_mae:.6f}\")\n        print(f\"   Best Laplace LL: {best_laplace_ll:.6f}\")\n        \n        return model, train_loader, val_loader, best_val_mae, best_laplace_ll\n\n    if __name__ == \"__main__\":\n        model, train_loader, val_loader, best_val_mae, best_laplace_ll = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T07:01:17.892885Z","iopub.execute_input":"2025-10-12T07:01:17.893635Z","iopub.status.idle":"2025-10-12T07:20:23.280185Z","shell.execute_reply.started":"2025-10-12T07:01:17.893610Z","shell.execute_reply":"2025-10-12T07:20:23.279159Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Fixed OSIC Model - Complete Working Version\n============================================================\nğŸ“± Device: cuda\nğŸ”¥ GPU: Tesla P100-PCIE-16GB\nğŸ’¾ Memory: 17.1 GB\nâš¡ CUDA Version: 12.4\nğŸš€ cuDNN Benchmark: Enabled (P100 optimized)\n============================================================\nLoaded dataset with shape: (1549, 7)\n============================================================\nğŸ“Š Creating Visit-Level Dataset\n============================================================\nCreating visit-level samples...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 946.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Dataset created:\n   Total visits: 1549\n   Unique patients: 176\n   Avg visits per patient: 8.8\n   FVC range: [827, 6399] mL\n   Weeks range: [0, 63]\nğŸ”„ Creating data loaders...\n============================================================\nğŸ“Š Data Split:\n   Train: 140 patients, 1233 visits\n   Val:   36 patients, 316 visits\n   Train FVC: 2744 Â± 836 mL\n   Val FVC:   2480 Â± 788 mL\n============================================================\nDataset train: 1217 visits from 138 patients\nDataset val: 316 visits from 36 patients\nâœ… Data loaders created!\n   Batch size: 16 (P100 optimized)\n   Train batches: 76\n   Val batches: 20\n   Workers: 4\nğŸ”„ Initializing model...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:00<00:00, 193MB/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… Model initialized!\nğŸ“Š Total parameters: 13,282,148\nğŸ“Š Output: FVC value (mL) + uncertainty\nğŸ” Testing model...\nâœ… Model forward pass successful!\n\n============================================================\nğŸ¯ TRAINING CONFIGURATION FOR RESEARCH PAPER\n============================================================\nTraining Loss: Uncertainty Loss (MAE-based)\nModel Saving: Best MAE (primary) + Best Laplace LL (secondary)\nMonitoring: All metrics (MAE, RMSE, RÂ², Correlation, Laplace LL)\n============================================================\n\nğŸš€ Starting training...\n\n============================================================\nğŸ¯ TRAINER CONFIGURATION\n============================================================\nTraining Loss: Uncertainty Loss (MAE-based)\nModel Saving: Best MAE (Research Paper Standard)\nMonitoring: MAE, RMSE, RÂ², Correlation, Laplace LL\n============================================================\n\nâš¡ Mixed Precision Training ENABLED for P100\nEpoch 1/30\nTrain - Loss: 3183998.843750 | MAE: 2742.3 mL\nVal   - Loss: 1634427.425000 | MAE: 2481.2 mL\nVal   - RÂ²: -9.9408 | Correlation: 0.0776\nVal   - Laplace LL: -54.723586\nVal   - RMSE: 2601.3 mL\nâœ… NEW BEST MAE! Model saved: 2481.2 mL (Laplace LL: -54.7236)\nğŸ’¾ Best Laplace LL model saved: -54.7236 (MAE: 2481.2 mL)\n------------------------------------------------------------\nEpoch 2/30\nTrain - Loss: 593483.791118 | MAE: 2739.9 mL\nVal   - Loss: 272487.904297 | MAE: 2477.0 mL\nVal   - RÂ²: -9.9064 | Correlation: 0.1708\nVal   - Laplace LL: -54.637395\nVal   - RMSE: 2597.2 mL\nâœ… NEW BEST MAE! Model saved: 2477.0 mL (Laplace LL: -54.6374)\nğŸ’¾ Best Laplace LL model saved: -54.6374 (MAE: 2477.0 mL)\n------------------------------------------------------------\nEpoch 3/30\nTrain - Loss: 79457.173597 | MAE: 2731.0 mL\nVal   - Loss: 36110.401953 | MAE: 2465.5 mL\nVal   - RÂ²: -9.8137 | Correlation: 0.3049\nVal   - Laplace LL: -54.404741\nVal   - RMSE: 2586.1 mL\nâœ… NEW BEST MAE! Model saved: 2465.5 mL (Laplace LL: -54.4047)\nğŸ’¾ Best Laplace LL model saved: -54.4047 (MAE: 2465.5 mL)\n------------------------------------------------------------\nEpoch 4/30\nTrain - Loss: 9373.033033 | MAE: 2711.1 mL\nVal   - Loss: 3724.724884 | MAE: 2441.4 mL\nVal   - RÂ²: -9.6228 | Correlation: 0.1691\nVal   - Laplace LL: -53.770244\nVal   - RMSE: 2563.2 mL\nâœ… NEW BEST MAE! Model saved: 2441.4 mL (Laplace LL: -53.7702)\nğŸ’¾ Best Laplace LL model saved: -53.7702 (MAE: 2441.4 mL)\n------------------------------------------------------------\nEpoch 5/30\nTrain - Loss: 963.786462 | MAE: 2674.7 mL\nVal   - Loss: 163.350028 | MAE: 2395.4 mL\nVal   - RÂ²: -9.2730 | Correlation: -0.7992\nVal   - Laplace LL: -29.561902\nVal   - RMSE: 2520.6 mL\nâœ… NEW BEST MAE! Model saved: 2395.4 mL (Laplace LL: -29.5619)\nğŸ’¾ Best Laplace LL model saved: -29.5619 (MAE: 2395.4 mL)\n------------------------------------------------------------\nEpoch 6/30\nTrain - Loss: 86.094361 | MAE: 2621.1 mL\nVal   - Loss: 20.045151 | MAE: 2336.3 mL\nVal   - RÂ²: -8.8092 | Correlation: 0.3508\nVal   - Laplace LL: -13.742822\nVal   - RMSE: 2463.1 mL\nâœ… NEW BEST MAE! Model saved: 2336.3 mL (Laplace LL: -13.7428)\nğŸ’¾ Best Laplace LL model saved: -13.7428 (MAE: 2336.3 mL)\n------------------------------------------------------------\nEpoch 7/30\nTrain - Loss: 14.119582 | MAE: 2538.7 mL\nVal   - Loss: 8.643306 | MAE: 2243.9 mL\nVal   - RÂ²: -8.1395 | Correlation: -0.2626\nVal   - Laplace LL: -9.789242\nVal   - RMSE: 2377.5 mL\nâœ… NEW BEST MAE! Model saved: 2243.9 mL (Laplace LL: -9.7892)\nğŸ’¾ Best Laplace LL model saved: -9.7892 (MAE: 2243.9 mL)\n------------------------------------------------------------\nEpoch 8/30\nTrain - Loss: 9.449272 | MAE: 2426.6 mL\nVal   - Loss: 8.230248 | MAE: 2128.4 mL\nVal   - RÂ²: -7.3071 | Correlation: 0.1151\nVal   - Laplace LL: -9.440137\nVal   - RMSE: 2266.7 mL\nâœ… NEW BEST MAE! Model saved: 2128.4 mL (Laplace LL: -9.4401)\nğŸ’¾ Best Laplace LL model saved: -9.4401 (MAE: 2128.4 mL)\n------------------------------------------------------------\nEpoch 9/30\nTrain - Loss: 9.391480 | MAE: 2280.3 mL\nVal   - Loss: 8.223007 | MAE: 1966.3 mL\nVal   - RÂ²: -6.2887 | Correlation: -0.5028\nVal   - Laplace LL: -9.341562\nVal   - RMSE: 2123.2 mL\nâœ… NEW BEST MAE! Model saved: 1966.3 mL (Laplace LL: -9.3416)\nğŸ’¾ Best Laplace LL model saved: -9.3416 (MAE: 1966.3 mL)\n------------------------------------------------------------\nEpoch 10/30\nTrain - Loss: 9.199685 | MAE: 2101.8 mL\nVal   - Loss: 8.093060 | MAE: 1798.5 mL\nVal   - RÂ²: -5.1860 | Correlation: 0.4057\nVal   - Laplace LL: -9.341042\nVal   - RMSE: 1956.0 mL\nâœ… NEW BEST MAE! Model saved: 1798.5 mL (Laplace LL: -9.3410)\nğŸ’¾ Best Laplace LL model saved: -9.3410 (MAE: 1798.5 mL)\n------------------------------------------------------------\nEpoch 11/30\nTrain - Loss: 9.121422 | MAE: 1853.1 mL\nVal   - Loss: 7.799899 | MAE: 1534.2 mL\nVal   - RÂ²: -3.7643 | Correlation: 0.4628\nVal   - Laplace LL: -9.026866\nVal   - RMSE: 1716.6 mL\nâœ… NEW BEST MAE! Model saved: 1534.2 mL (Laplace LL: -9.0269)\nğŸ’¾ Best Laplace LL model saved: -9.0269 (MAE: 1534.2 mL)\n------------------------------------------------------------\nEpoch 12/30\nTrain - Loss: 8.758810 | MAE: 1526.3 mL\nVal   - Loss: 7.632978 | MAE: 1191.5 mL\nVal   - RÂ²: -2.1888 | Correlation: 0.4509\nVal   - Laplace LL: -8.757050\nVal   - RMSE: 1404.4 mL\nâœ… NEW BEST MAE! Model saved: 1191.5 mL (Laplace LL: -8.7571)\nğŸ’¾ Best Laplace LL model saved: -8.7571 (MAE: 1191.5 mL)\n------------------------------------------------------------\nEpoch 13/30\nTrain - Loss: 8.317267 | MAE: 1126.7 mL\nVal   - Loss: 7.127071 | MAE: 807.7 mL\nVal   - RÂ²: -0.8890 | Correlation: 0.2517\nVal   - Laplace LL: -8.185665\nVal   - RMSE: 1080.9 mL\nâœ… NEW BEST MAE! Model saved: 807.7 mL (Laplace LL: -8.1857)\nğŸ’¾ Best Laplace LL model saved: -8.1857 (MAE: 807.7 mL)\n------------------------------------------------------------\nEpoch 14/30\nTrain - Loss: 8.095383 | MAE: 804.0 mL\nVal   - Loss: 6.884507 | MAE: 608.3 mL\nVal   - RÂ²: -0.0908 | Correlation: 0.7968\nVal   - Laplace LL: -7.949955\nVal   - RMSE: 821.4 mL\nâœ… NEW BEST MAE! Model saved: 608.3 mL (Laplace LL: -7.9500)\nğŸ’¾ Best Laplace LL model saved: -7.9500 (MAE: 608.3 mL)\n------------------------------------------------------------\nEpoch 15/30\nTrain - Loss: 7.969329 | MAE: 637.8 mL\nVal   - Loss: 6.761545 | MAE: 478.6 mL\nVal   - RÂ²: 0.2983 | Correlation: 0.8669\nVal   - Laplace LL: -7.790924\nVal   - RMSE: 658.8 mL\nâœ… NEW BEST MAE! Model saved: 478.6 mL (Laplace LL: -7.7909)\nğŸ’¾ Best Laplace LL model saved: -7.7909 (MAE: 478.6 mL)\n------------------------------------------------------------\nEpoch 16/30\nTrain - Loss: 7.899698 | MAE: 498.0 mL\nVal   - Loss: 6.714815 | MAE: 401.7 mL\nVal   - RÂ²: 0.4574 | Correlation: 0.8714\nVal   - Laplace LL: -7.639302\nVal   - RMSE: 579.3 mL\nâœ… NEW BEST MAE! Model saved: 401.7 mL (Laplace LL: -7.6393)\nğŸ’¾ Best Laplace LL model saved: -7.6393 (MAE: 401.7 mL)\n------------------------------------------------------------\nEpoch 17/30\nTrain - Loss: 7.750191 | MAE: 441.8 mL\nVal   - Loss: 6.886021 | MAE: 360.5 mL\nVal   - RÂ²: 0.6217 | Correlation: 0.9088\nVal   - Laplace LL: -7.720009\nVal   - RMSE: 483.7 mL\nâœ… NEW BEST MAE! Model saved: 360.5 mL (Laplace LL: -7.7200)\n------------------------------------------------------------\nEpoch 18/30\nTrain - Loss: 7.622718 | MAE: 392.9 mL\nVal   - Loss: 6.817686 | MAE: 359.9 mL\nVal   - RÂ²: 0.6898 | Correlation: 0.9446\nVal   - Laplace LL: -7.761696\nVal   - RMSE: 438.0 mL\nâœ… NEW BEST MAE! Model saved: 359.9 mL (Laplace LL: -7.7617)\n------------------------------------------------------------\nEpoch 19/30\nTrain - Loss: 7.559010 | MAE: 387.8 mL\nVal   - Loss: 6.779141 | MAE: 342.9 mL\nVal   - RÂ²: 0.6575 | Correlation: 0.8969\nVal   - Laplace LL: -7.657681\nVal   - RMSE: 460.3 mL\nâœ… NEW BEST MAE! Model saved: 342.9 mL (Laplace LL: -7.6577)\n------------------------------------------------------------\nEpoch 20/30\nTrain - Loss: 7.450092 | MAE: 381.8 mL\nVal   - Loss: 6.400335 | MAE: 267.3 mL\nVal   - RÂ²: 0.8155 | Correlation: 0.9415\nVal   - Laplace LL: -7.320960\nVal   - RMSE: 337.8 mL\nâœ… NEW BEST MAE! Model saved: 267.3 mL (Laplace LL: -7.3210)\nğŸ’¾ Best Laplace LL model saved: -7.3210 (MAE: 267.3 mL)\n------------------------------------------------------------\nEpoch 21/30\nTrain - Loss: 7.451907 | MAE: 369.6 mL\nVal   - Loss: 6.538450 | MAE: 290.4 mL\nVal   - RÂ²: 0.7959 | Correlation: 0.9463\nVal   - Laplace LL: -7.458961\nVal   - RMSE: 355.3 mL\n------------------------------------------------------------\nEpoch 22/30\nTrain - Loss: 7.732823 | MAE: 376.5 mL\nVal   - Loss: 6.538032 | MAE: 292.9 mL\nVal   - RÂ²: 0.7906 | Correlation: 0.9457\nVal   - Laplace LL: -7.465319\nVal   - RMSE: 359.9 mL\n------------------------------------------------------------\nEpoch 23/30\nTrain - Loss: 7.435363 | MAE: 369.5 mL\nVal   - Loss: 6.330201 | MAE: 242.2 mL\nVal   - RÂ²: 0.8420 | Correlation: 0.9427\nVal   - Laplace LL: -7.224372\nVal   - RMSE: 312.6 mL\nâœ… NEW BEST MAE! Model saved: 242.2 mL (Laplace LL: -7.2244)\nğŸ’¾ Best Laplace LL model saved: -7.2244 (MAE: 242.2 mL)\n------------------------------------------------------------\nEpoch 24/30\nTrain - Loss: 7.187570 | MAE: 378.3 mL\nVal   - Loss: 6.772543 | MAE: 288.5 mL\nVal   - RÂ²: 0.7851 | Correlation: 0.9277\nVal   - Laplace LL: -7.554183\nVal   - RMSE: 364.5 mL\n------------------------------------------------------------\nEpoch 25/30\nTrain - Loss: 7.295136 | MAE: 356.7 mL\nVal   - Loss: 6.386492 | MAE: 259.7 mL\nVal   - RÂ²: 0.8092 | Correlation: 0.9440\nVal   - Laplace LL: -7.267075\nVal   - RMSE: 343.5 mL\n------------------------------------------------------------\nEpoch 26/30\nTrain - Loss: 7.447204 | MAE: 354.1 mL\nVal   - Loss: 6.440166 | MAE: 254.9 mL\nVal   - RÂ²: 0.8200 | Correlation: 0.9306\nVal   - Laplace LL: -7.297576\nVal   - RMSE: 333.7 mL\n------------------------------------------------------------\nEpoch 27/30\nTrain - Loss: 7.390135 | MAE: 353.7 mL\nVal   - Loss: 6.319900 | MAE: 263.4 mL\nVal   - RÂ²: 0.8147 | Correlation: 0.9402\nVal   - Laplace LL: -7.268272\nVal   - RMSE: 338.6 mL\n------------------------------------------------------------\nEpoch 28/30\nTrain - Loss: 7.361809 | MAE: 358.5 mL\nVal   - Loss: 6.359122 | MAE: 261.8 mL\nVal   - RÂ²: 0.7992 | Correlation: 0.9399\nVal   - Laplace LL: -7.250670\nVal   - RMSE: 352.4 mL\n------------------------------------------------------------\nEpoch 29/30\nTrain - Loss: 7.267698 | MAE: 360.1 mL\nVal   - Loss: 6.194027 | MAE: 215.3 mL\nVal   - RÂ²: 0.8498 | Correlation: 0.9377\nVal   - Laplace LL: -7.039012\nVal   - RMSE: 304.8 mL\nâœ… NEW BEST MAE! Model saved: 215.3 mL (Laplace LL: -7.0390)\nğŸ’¾ Best Laplace LL model saved: -7.0390 (MAE: 215.3 mL)\n------------------------------------------------------------\nEpoch 30/30\nTrain - Loss: 7.465452 | MAE: 357.8 mL\nVal   - Loss: 6.364793 | MAE: 271.4 mL\nVal   - RÂ²: 0.7968 | Correlation: 0.9170\nVal   - Laplace LL: -7.300376\nVal   - RMSE: 354.5 mL\n------------------------------------------------------------\n\nğŸ† Training Summary:\n   Best Validation MAE: 215.329227\n   Best Laplace Log Likelihood: -7.039012\n\nğŸ¯ Training completed!\n   Best MAE: 215.329227\n   Best Laplace LL: -7.039012\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# osic_publication_ready_pipeline.py\n# Complete pipeline: patient-level slope prediction, fixed alpha uncertainty,\n# k-fold CV, baselines, per-horizon metrics, and robust batching.\n\nimport os\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport pydicom\nimport math\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\n\nfrom albumentations import Compose, Normalize, Resize\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# -------------------------\n# Reproducibility & Config\n# -------------------------\nSEED = 42\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(SEED)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_CSV = DATA_DIR / \"train.csv\"\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nBATCH_SIZE = 4 if torch.cuda.is_available() else 1  # use 4 on GPU if memory allows; increase as you can\nNUM_WORKERS = 2\nEPOCHS = 8   # increase for final runs (e.g., 30)\nLR = 1e-4\nALPHA_FIXED = 2.0  # mL per week uncertainty growth; tune via validation (grid 0.5-5.0)\nMIN_SIGMA = 70.0\n\n# -------------------------\n# Utilities\n# -------------------------\ndef ensure_1d_list(x):\n    \"\"\"Convert many possible input formats into a 1D python list of floats.\"\"\"\n    if isinstance(x, torch.Tensor):\n        arr = x.detach().cpu().numpy()\n        return list(np.array(arr).ravel())\n    if isinstance(x, np.ndarray):\n        return list(x.ravel())\n    if isinstance(x, (list, tuple)):\n        # flatten nested single-level lists commonly produced by DataLoader\n        flat = []\n        for el in x:\n            if isinstance(el, (list, tuple, np.ndarray)):\n                flat.extend(list(np.array(el).ravel()))\n            else:\n                flat.append(el)\n        return [float(v) for v in flat]\n    return [float(x)]\n\n# -------------------------\n# Data preparation: patient-level summary\n# -------------------------\ndef compute_patient_summary(df: pd.DataFrame) -> pd.DataFrame:\n    rows = []\n    for pid, sub in df.groupby('Patient'):\n        sub = sub.sort_values('Weeks')\n        weeks = sub['Weeks'].values.astype(np.float32)\n        fvc = sub['FVC'].values.astype(np.float32)\n        baseline_week = float(weeks[0])\n        baseline_fvc = float(fvc[0])\n        if len(weeks) >= 2:\n            # slope in mL/week\n            slope, _ = np.polyfit(weeks, fvc, 1)\n            slope = float(slope)\n        else:\n            slope = 0.0\n        rows.append({\n            'Patient': pid,\n            'baseline_week': baseline_week,\n            'baseline_fvc': baseline_fvc,\n            'slope': slope,\n            'Age': float(sub.iloc[0]['Age']),\n            'Sex': sub.iloc[0]['Sex'],\n            'SmokingStatus': sub.iloc[0]['SmokingStatus'],\n            'visits_weeks': weeks.tolist(),\n            'visits_fvc': fvc.tolist()\n        })\n    return pd.DataFrame(rows)\n\ndef encode_tabular(age, sex, smoking, baseline_fvc):\n    age_norm = (age - 30.0) / 30.0\n    sex_enc = 0.0 if sex == 'Male' else 1.0\n    if smoking == 'Never smoked':\n        smoke = [0.0, 0.0]\n    elif smoking == 'Ex-smoker':\n        smoke = [1.0, 1.0]\n    elif smoking == 'Currently smokes':\n        smoke = [0.0, 1.0]\n    else:\n        smoke = [1.0, 0.0]\n    baseline_fvc_norm = baseline_fvc / 1000.0\n    return np.array([age_norm, sex_enc, smoke[0], smoke[1], baseline_fvc_norm], dtype=np.float32)\n\n# -------------------------\n# Dataset (one sample per patient)\n# -------------------------\nclass OSICPatientDataset(Dataset):\n    def __init__(self, patient_df: pd.DataFrame, data_dir: Path, augment: bool = False):\n        self.df = patient_df.reset_index(drop=True)\n        self.data_dir = Path(data_dir)\n        self.augment = augment\n        self.transform = Compose([\n            Resize(512, 512),\n            Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n            ToTensorV2()\n        ])\n        self.patient_images = {}\n        for p in self.df['Patient'].unique():\n            pdir = self.data_dir / p\n            if pdir.exists():\n                files = sorted([f for f in pdir.iterdir() if f.suffix.lower() == '.dcm'])\n                if len(files) > 0:\n                    self.patient_images[p] = files\n        # filter out patients without images\n        before = len(self.df)\n        self.df = self.df[self.df['Patient'].isin(self.patient_images.keys())].reset_index(drop=True)\n        after = len(self.df)\n        print(f\"Dataset initialized: {after} patients with images (filtered {before-after})\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def load_dicom(self, path: Path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if img.ndim == 3:\n                img = img[img.shape[0] // 2]\n            img = cv2.resize(img, (512,512))\n            mn, mx = img.min(), img.max()\n            if mx > mn:\n                img = (img - mn) / (mx - mn) * 255.0\n            else:\n                img = img * 0.0\n            img = np.stack([img,img,img], axis=2).astype(np.uint8)\n            return img\n        except Exception:\n            return np.zeros((512,512,3), dtype=np.uint8)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        pid = row['Patient']\n        files = self.patient_images[pid]\n        img_path = files[len(files)//2]\n        img = self.load_dicom(img_path)\n        img = self.transform(image=img)['image']  # tensor C,H,W\n        tab = encode_tabular(row['Age'], row['Sex'], row['SmokingStatus'], row['baseline_fvc'])\n        tab = torch.tensor(tab, dtype=torch.float32)\n        weeks_abs = np.array(row['visits_weeks'], dtype=np.float32)\n        baseline_week = float(row['baseline_week'])\n        weeks_from_baseline = weeks_abs - baseline_week  # can include 0\n        fvc_targets = np.array(row['visits_fvc'], dtype=np.float32)\n        # return raw lists too (for linear extrap)\n        return img, tab, torch.tensor(weeks_from_baseline, dtype=torch.float32), torch.tensor(fvc_targets, dtype=torch.float32), float(row['baseline_fvc']), float(row['baseline_week']), float(row['slope']), row['Patient'], weeks_abs.tolist(), fvc_targets.tolist()\n\n# -------------------------\n# Collate (for batching variable-length visit arrays)\n# -------------------------\ndef patient_collate_fn(batch):\n    \"\"\"\n    Batch is list of samples: (img, tab, weeks_rel_tensor, fvc_targets_tensor, baseline_fvc, baseline_week, slope, patient, weeks_abs_list, fvc_list)\n    We'll stack images and tabs; keep lists of visit tensors raw for later evaluation/training per-sample.\n    \"\"\"\n    images = torch.stack([b[0] for b in batch], dim=0)\n    tabs = torch.stack([b[1] for b in batch], dim=0)\n    weeks_rel = [b[2] for b in batch]  # list of tensors\n    fvc_targets = [b[3] for b in batch]\n    baseline_fvcs = [b[4] for b in batch]\n    baseline_weeks = [b[5] for b in batch]\n    slopes = [b[6] for b in batch]\n    patients = [b[7] for b in batch]\n    weeks_abs_list = [b[8] for b in batch]\n    fvc_list = [b[9] for b in batch]\n    return images, tabs, weeks_rel, fvc_targets, baseline_fvcs, baseline_weeks, slopes, patients, weeks_abs_list, fvc_list\n\n# -------------------------\n# Models\n# -------------------------\nclass PatientLevelModel(nn.Module):\n    def __init__(self, tab_dim=5, dropout=0.3):\n        super().__init__()\n        backbone = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = backbone.features\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n        img_feat_dim = 1024\n        self.tab_mlp = nn.Sequential(\n            nn.Linear(tab_dim, 64),\n            nn.ReLU(),\n            nn.LayerNorm(64),\n            nn.Dropout(dropout)\n        )\n        self.fusion = nn.Sequential(\n            nn.Linear(img_feat_dim + 64, 512),\n            nn.ReLU(),\n            nn.LayerNorm(512),\n            nn.Dropout(dropout),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.LayerNorm(128)\n        )\n        self.slope_head = nn.Linear(128, 1)\n        # predict log_sigma0, but we'll use softplus + MIN_SIGMA; clipping applied outside\n        self.log_sigma0_head = nn.Linear(128, 1)\n\n    def forward(self, images, tabular):\n        x = self.features(images)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        t = self.tab_mlp(tabular)\n        fused = torch.cat([x, t], dim=1)\n        f = self.fusion(fused)\n        slope = self.slope_head(f).squeeze(1)\n        log_sigma0 = self.log_sigma0_head(f).squeeze(1)\n        return slope, log_sigma0\n\nclass TabularOnlyModel(nn.Module):\n    def __init__(self, tab_dim=5, hidden=128, dropout=0.3):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(tab_dim, hidden),\n            nn.ReLU(),\n            nn.LayerNorm(hidden),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, hidden//2),\n            nn.ReLU(),\n            nn.LayerNorm(hidden//2)\n        )\n        self.slope_head = nn.Linear(hidden//2, 1)\n        self.log_sigma0_head = nn.Linear(hidden//2, 1)\n\n    def forward(self, images_unused, tabular):\n        f = self.mlp(tabular)\n        slope = self.slope_head(f).squeeze(1)\n        log_sigma0 = self.log_sigma0_head(f).squeeze(1)\n        return slope, log_sigma0\n\n# -------------------------\n# Laplace loss / metrics\n# -------------------------\ndef softplus_tensor(x):\n    return F.softplus(x)\n\ndef laplace_ll_vectorized(preds, targets, sigmas):\n    \"\"\"Compute Laplace log-likelihood elementwise (numpy arrays).\"\"\"\n    sigma = np.maximum(sigmas, MIN_SIGMA)\n    delta = np.abs(preds - targets)\n    ll = -np.sqrt(2) * delta / sigma - np.log(np.sqrt(2) * sigma)\n    return ll\n\ndef patient_ll_and_mae(pred_slope_tensor, log_sigma0_tensor, baseline_fvc_tensor, weeks_rel_tensor, fvc_targets_tensor, alpha_fixed=ALPHA_FIXED):\n    \"\"\"\n    Compute mean Laplace ll and mae for one patient (PyTorch tensors).\n    weeks_rel_tensor shape (T,), fvc_targets_tensor shape (T,), baseline_fvc_tensor scalar\n    \"\"\"\n    device = pred_slope_tensor.device\n    pred_fvcs = baseline_fvc_tensor + pred_slope_tensor.unsqueeze(-1) * weeks_rel_tensor  # (T,)\n    sigma0 = softplus_tensor(log_sigma0_tensor)\n    alpha = torch.tensor(alpha_fixed, device=device, dtype=torch.float32)\n    sigma_t = sigma0 + alpha * weeks_rel_tensor.abs()\n    sigma_t = torch.clamp(sigma_t, min=MIN_SIGMA)\n    delta = torch.abs(pred_fvcs - fvc_targets_tensor)\n    sqrt2 = torch.sqrt(torch.tensor(2.0, device=device))\n    ll = -sqrt2 * delta / sigma_t - torch.log(sqrt2 * sigma_t)\n    mean_ll = ll.mean()\n    mae = delta.mean()\n    return mean_ll, mae, pred_fvcs.detach().cpu().numpy(), sigma_t.detach().cpu().numpy()\n\n# -------------------------\n# Evaluation (full diagnostics)\n# -------------------------\ndef linear_extrapolation_slope_from_first_two_visits(weeks_abs_list, fvc_list):\n    w = ensure_1d_list(weeks_abs_list)\n    f = ensure_1d_list(fvc_list)\n    if len(w) >= 2 and np.ptp(w[:2]) != 0:\n        w_arr = np.array(w[:2], dtype=np.float32)\n        f_arr = np.array(f[:2], dtype=np.float32)\n        slope, intercept = np.polyfit(w_arr, f_arr, 1)\n        return float(slope)\n    return 0.0\n\ndef evaluate_model_on_loader(model, loader, device=DEVICE, exclude_baseline_from_r2=True, compute_plots=False):\n    model.eval()\n    all_true, all_pred, all_sigma, all_baseline_pred, all_linex_pred, all_weeks_from_baseline, all_patient_ids = [], [], [], [], [], [], []\n\n    with torch.no_grad():\n        for batch in loader:\n            images, tabs, weeks_rel_list, fvc_targets_list, baseline_fvcs, baseline_weeks, slopes, patients, weeks_abs_list, fvc_list = batch\n            images = images.to(device)\n            tabs = tabs.to(device)\n            pred_slope_batch, log_sigma0_batch = model(images, tabs)\n            # ensure numpy\n            pred_slope_np = pred_slope_batch.detach().cpu().numpy()\n            log_sigma0_np = log_sigma0_batch.detach().cpu().numpy()\n\n            B = images.size(0)\n            for i in range(B):\n                patient = patients[i]\n                baseline_fvc = float(baseline_fvcs[i])\n                baseline_week = float(baseline_weeks[i])\n                weeks_abs = ensure_1d_list(weeks_abs_list[i])\n                fvc_obs = ensure_1d_list(fvc_list[i])\n                slope_val = float(pred_slope_np[i]) if np.ndim(pred_slope_np)>0 else float(pred_slope_np)\n                sigma0_val = float(softplus_tensor(torch.tensor(log_sigma0_np[i])).item())\n                alpha_val = float(ALPHA_FIXED)\n\n                # baselines\n                baseline_preds = np.array([baseline_fvc]*len(weeks_abs), dtype=np.float32)\n                lin_slope = linear_extrapolation_slope_from_first_two_visits(weeks_abs, fvc_obs)\n                linex_preds = np.array([baseline_fvc + lin_slope*(wk - baseline_week) for wk in weeks_abs], dtype=np.float32)\n\n                # per visit predicted fvc & sigma\n                for wk_abs, true_fvc in zip(weeks_abs, fvc_obs):\n                    time_delta = float(wk_abs) - baseline_week\n                    pred_fvc = baseline_fvc + slope_val * time_delta\n                    sigma_t = sigma0_val + abs(alpha_val * time_delta)\n                    sigma_t = max(sigma_t, MIN_SIGMA)\n\n                    all_true.append(float(true_fvc))\n                    all_pred.append(float(pred_fvc))\n                    all_sigma.append(float(sigma_t))\n                    all_baseline_pred.append(float(baseline_fvc))\n                    all_linex_pred.append(float(baseline_fvc + lin_slope*(wk_abs - baseline_week)))\n                    all_weeks_from_baseline.append(time_delta)\n                    all_patient_ids.append(patient)\n\n    all_true = np.array(all_true)\n    all_pred = np.array(all_pred)\n    all_sigma = np.array(all_sigma)\n    all_baseline_pred = np.array(all_baseline_pred)\n    all_linex_pred = np.array(all_linex_pred)\n    all_weeks_from_baseline = np.array(all_weeks_from_baseline)\n    all_patient_ids = np.array(all_patient_ids, dtype=object)\n\n    # Metrics: full\n    mae_model = mean_absolute_error(all_true, all_pred)\n    rmse_model = math.sqrt(mean_squared_error(all_true, all_pred))\n    r2_model_including = r2_score(all_true, all_pred) if len(np.unique(all_true))>1 else 0.0\n    ll_model = laplace_ll_vectorized(all_pred, all_true, all_sigma).mean()\n    # Pearson correlation\n    corr_model = np.corrcoef(all_true, all_pred)[0,1] if len(all_true)>1 else 0.0\n\n    # Baseline metrics\n    mae_base = mean_absolute_error(all_true, all_baseline_pred)\n    rmse_base = math.sqrt(mean_squared_error(all_true, all_baseline_pred))\n    r2_base = r2_score(all_true, all_baseline_pred) if len(np.unique(all_true))>1 else 0.0\n    ll_base = laplace_ll_vectorized(all_baseline_pred, all_true, np.full_like(all_baseline_pred, MIN_SIGMA)).mean()\n\n    mae_linex = mean_absolute_error(all_true, all_linex_pred)\n    rmse_linex = math.sqrt(mean_squared_error(all_true, all_linex_pred))\n    r2_linex = r2_score(all_true, all_linex_pred) if len(np.unique(all_true))>1 else 0.0\n    ll_linex = laplace_ll_vectorized(all_linex_pred, all_true, np.full_like(all_linex_pred, MIN_SIGMA)).mean()\n\n    # RÂ² excluding baseline visits (recommended)\n    mask_followup = all_weeks_from_baseline != 0\n    if mask_followup.sum() > 0:\n        r2_model_excluding = r2_score(all_true[mask_followup], all_pred[mask_followup]) if len(np.unique(all_true[mask_followup]))>1 else 0.0\n        mae_model_excluding = mean_absolute_error(all_true[mask_followup], all_pred[mask_followup])\n    else:\n        r2_model_excluding = r2_model_including\n        mae_model_excluding = mae_model\n\n    # Per-horizon metrics\n    horizons = [12, 24, 48]\n    per_horizon_results = {}\n    for h in horizons:\n        mask = (all_weeks_from_baseline >= (h-2)) & (all_weeks_from_baseline <= (h+2))\n        if mask.sum() >= 5:\n            per_horizon_results[h] = {\n                'mae': mean_absolute_error(all_true[mask], all_pred[mask]),\n                'r2': r2_score(all_true[mask], all_pred[mask]) if len(np.unique(all_true[mask]))>1 else 0.0,\n                'n': int(mask.sum())\n            }\n        else:\n            per_horizon_results[h] = {'mae': None, 'r2': None, 'n': int(mask.sum())}\n\n    # Per-patient R2 (patients with >1 obs)\n    pred_by_patient, true_by_patient, base_by_patient = defaultdict(list), defaultdict(list), defaultdict(list)\n    for pid, p, t, b in zip(all_patient_ids, all_pred, all_true, all_baseline_pred):\n        pred_by_patient[pid].append(p)\n        true_by_patient[pid].append(t)\n        base_by_patient[pid].append(b)\n    per_patient_r2_model, per_patient_r2_base = [], []\n    for pid in np.unique(all_patient_ids):\n        y_true = np.array(true_by_patient[pid])\n        if len(y_true) > 1:\n            try:\n                per_patient_r2_model.append(r2_score(y_true, np.array(pred_by_patient[pid])))\n            except:\n                per_patient_r2_model.append(0.0)\n            try:\n                per_patient_r2_base.append(r2_score(y_true, np.array(base_by_patient[pid])))\n            except:\n                per_patient_r2_base.append(0.0)\n    per_patient_r2_model = np.array(per_patient_r2_model) if per_patient_r2_model else np.array([0.0])\n    per_patient_r2_base = np.array(per_patient_r2_base) if per_patient_r2_base else np.array([0.0])\n\n    # Coverage\n    errors = np.abs(all_pred - all_true)\n    coverage_1sigma = np.mean(errors <= all_sigma) if len(all_sigma)>0 else 0.0\n    coverage_2sigma = np.mean(errors <= 2*all_sigma) if len(all_sigma)>0 else 0.0\n\n    # Avg sigma by bin\n    bins = [-999, 12, 24, 48, 9999]\n    bin_names = [\"<=12w\",\"13-24w\",\"25-48w\",\">48w\"]\n    avg_sigma_by_bin = {}\n    counts_by_bin = {}\n    for i in range(len(bins)-1):\n        m = (all_weeks_from_baseline > bins[i]) & (all_weeks_from_baseline <= bins[i+1])\n        if m.sum() > 0:\n            avg_sigma_by_bin[bin_names[i]] = all_sigma[m].mean()\n            counts_by_bin[bin_names[i]] = int(m.sum())\n        else:\n            avg_sigma_by_bin[bin_names[i]] = np.nan\n            counts_by_bin[bin_names[i]] = 0\n\n    # Print in the same style as original desired output\n    print(\"\\nSummary (over all observed visits in validation):\")\n    print(f\"Model    MAE: {mae_model:.1f} mL | RMSE: {rmse_model:.1f} mL | RÂ² (incl base): {r2_model_including:.4f} | Laplace LL: {ll_model:.6f}\")\n    print(f\"Model    MAE (follow-up only): {mae_model_excluding:.1f} mL | RÂ² (follow-up only): {r2_model_excluding:.4f}\")\n    print(f\"Baseline MAE: {mae_base:.1f} mL | RMSE: {rmse_base:.1f} mL | RÂ²: {r2_base:.4f} | Laplace LL: {ll_base:.6f}\")\n    print(f\"LinEx MAE: {mae_linex:.1f} mL | RMSE: {rmse_linex:.1f} mL | RÂ²: {r2_linex:.4f} | Laplace LL: {ll_linex:.6f}\")\n    print(f\"Pearson Correlation (model): {corr_model:.4f}\")\n    print(\"\")\n    print(\"Per-patient RÂ² (summary over patients with >1 observation):\")\n    print(f\" Model RÂ² meanÂ±std: {per_patient_r2_model.mean():.3f} Â± {per_patient_r2_model.std():.3f}\")\n    print(f\" Baseline RÂ² meanÂ±std: {per_patient_r2_base.mean():.3f} Â± {per_patient_r2_base.std():.3f}\")\n    print(\"\")\n    print(f\"Uncertainty coverage: Â±1Ïƒ: {coverage_1sigma*100:.1f}%  | Â±2Ïƒ: {coverage_2sigma*100:.1f}%\")\n    print(\"Average sigma by horizon bins:\")\n    for bn in bin_names:\n        print(f\" {bn}: {avg_sigma_by_bin[bn]:.1f} mL (n={counts_by_bin[bn]})\")\n    print(\"\")\n    print(\"Per-horizon metrics (Â±2w):\")\n    for h, info in per_horizon_results.items():\n        print(f\" {h}w: MAE={info['mae']}  RÂ²={info['r2']}  n={info['n']}\")\n\n    if compute_plots:\n        plt.figure(figsize=(6,6))\n        plt.scatter(all_true, all_pred, alpha=0.4, s=20)\n        mn = min(all_true.min(), all_pred.min())\n        mx = max(all_true.max(), all_pred.max())\n        plt.plot([mn,mx],[mn,mx], color='k', linestyle='--')\n        plt.xlabel(\"True FVC (mL)\")\n        plt.ylabel(\"Model predicted FVC (mL)\")\n        plt.title(\"True vs Model Predicted (validation)\")\n        plt.grid(True)\n        plt.show()\n\n    return {\n        'mae': mae_model,\n        'rmse': rmse_model,\n        'r2_including_baseline': r2_model_including,\n        'r2_excluding_baseline': r2_model_excluding,\n        'laplace_ll': ll_model,\n        'mae_baseline': mae_base,\n        'mae_linex': mae_linex,\n        'per_patient_r2_model': per_patient_r2_model,\n        'coverage_1sigma': coverage_1sigma,\n        'coverage_2sigma': coverage_2sigma,\n        'per_horizon': per_horizon_results\n    }\n\n# -------------------------\n# Trainer (handles batch training)\n# -------------------------\nclass PatientTrainer:\n    def __init__(self, model, device, lr=LR, use_laplace_loss=True, alpha_fixed=ALPHA_FIXED):\n        self.model = model.to(device)\n        self.device = device\n        self.alpha_fixed = alpha_fixed\n        self.opt = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n        self.use_laplace_loss = use_laplace_loss\n\n    def train_one_epoch(self, loader):\n        self.model.train()\n        losses = []\n        for batch in loader:\n            images, tabs, weeks_rel_list, fvc_targets_list, baseline_fvcs, baseline_weeks, slopes, patients, weeks_abs_list, fvc_list = batch\n            images = images.to(self.device)\n            tabs = tabs.to(self.device)\n            pred_slope_batch, log_sigma0_batch = self.model(images, tabs)  # (B,), (B,)\n            batch_loss = 0.0\n            B = images.size(0)\n            # compute per-sample mean_ll and sum\n            ll_sum = 0.0\n            for i in range(B):\n                weeks_rel = weeks_rel_list[i].to(self.device) if isinstance(weeks_rel_list[i], torch.Tensor) else torch.tensor(ensure_1d_list(weeks_rel_list[i]), device=self.device, dtype=torch.float32)\n                fvc_targets = fvc_targets_list[i].to(self.device) if isinstance(fvc_targets_list[i], torch.Tensor) else torch.tensor(ensure_1d_list(fvc_targets_list[i]), device=self.device, dtype=torch.float32)\n                baseline_fvc = torch.tensor(float(baseline_fvcs[i]), device=self.device, dtype=torch.float32)\n                pred_slope = pred_slope_batch[i].unsqueeze(0)\n                log_sigma0 = log_sigma0_batch[i].unsqueeze(0)\n                mean_ll, mae_val, _, _ = patient_ll_and_mae(pred_slope, log_sigma0, baseline_fvc, weeks_rel, fvc_targets, alpha_fixed=self.alpha_fixed)\n                ll_sum = ll_sum + mean_ll\n            mean_ll_batch = ll_sum / float(B)\n            loss = -mean_ll_batch if self.use_laplace_loss else torch.tensor(0.0, device=self.device)\n            self.opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.opt.step()\n            losses.append(float(loss.item()))\n        return float(np.mean(losses)) if losses else 0.0\n\n    def train(self, train_loader, val_loader, epochs=EPOCHS):\n        best_val_mae = float('inf')\n        best_state = None\n        for epoch in range(epochs):\n            tr_loss = self.train_one_epoch(train_loader)\n            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {tr_loss:.6f}\")\n            val_metrics = evaluate_model_on_loader(self.model, val_loader, device=self.device, exclude_baseline_from_r2=True, compute_plots=False)\n            # choose best by follow-up MAE (can also use laplace_ll)\n            val_mae = val_metrics['mae']\n            if val_mae < best_val_mae:\n                best_val_mae = val_mae\n                best_state = self.model.state_dict()\n                torch.save(best_state, 'best_patient_model_fold.pth')\n                print(f\"âœ… NEW BEST VAL MAE: {best_val_mae:.3f} mL - model saved\")\n        return best_val_mae\n\n# -------------------------\n# K-Fold CV orchestration\n# -------------------------\ndef run_kfold_cv(patient_summary_df, n_splits=5, alpha_fixed=ALPHA_FIXED):\n    # create stratification bucket based on slope\n    slopes = patient_summary_df['slope'].values\n    # bins: fast (< -15 mL/w), moderate (-15 to -5), slow (> -5)\n    bins = [-9999, -15.0, -5.0, 9999]\n    strata = np.digitize(slopes, bins)  # 1..3\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    fold_results = []\n    fold_idx = 0\n    for train_idx, val_idx in skf.split(patient_summary_df['Patient'].values, strata):\n        fold_idx += 1\n        print(\"\\n\" + \"=\"*60)\n        print(f\"Fold {fold_idx}/{n_splits}\")\n        print(\"=\"*60)\n        train_patients = patient_summary_df.iloc[train_idx]['Patient'].values\n        val_patients = patient_summary_df.iloc[val_idx]['Patient'].values\n        train_df = patient_summary_df[patient_summary_df['Patient'].isin(train_patients)].reset_index(drop=True)\n        val_df = patient_summary_df[patient_summary_df['Patient'].isin(val_patients)].reset_index(drop=True)\n        # datasets & loaders\n        train_ds = OSICPatientDataset(train_df, TRAIN_DIR, augment=False)\n        val_ds = OSICPatientDataset(val_df, TRAIN_DIR, augment=False)\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=patient_collate_fn)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=patient_collate_fn)\n        # model & trainer\n        model = PatientLevelModel(tab_dim=5)\n        trainer = PatientTrainer(model, DEVICE, lr=LR, use_laplace_loss=True, alpha_fixed=alpha_fixed)\n        best_mae = trainer.train(train_loader, val_loader, epochs=EPOCHS)\n        # load best model and evaluate\n        model.load_state_dict(torch.load('best_patient_model_fold.pth'))\n        model.to(DEVICE)\n        metrics = evaluate_model_on_loader(model, val_loader, device=DEVICE, exclude_baseline_from_r2=True, compute_plots=False)\n        fold_results.append(metrics)\n        print(f\"Fold {fold_idx} result MAE: {metrics['mae']:.1f} mL  RÂ²(excl baseline): {metrics['r2_excluding_baseline']:.3f}  Laplace LL: {metrics['laplace_ll']:.3f}\")\n    # aggregate\n    print(\"\\n\" + \"=\"*60)\n    print(f\"{n_splits}-fold CV summary (mean Â± std):\")\n    for key in ['mae', 'laplace_ll', 'r2_excluding_baseline']:\n        values = [f[key] for f in fold_results]\n        mean_v = np.mean(values)\n        std_v = np.std(values)\n        print(f\"{key.upper()}: {mean_v:.3f} Â± {std_v:.3f}\")\n    return fold_results\n\n# -------------------------\n# Main: execute CV + baselines + ablation\n# -------------------------\ndef main():\n    print(f\"Device: {DEVICE}\")\n    df = pd.read_csv(TRAIN_CSV)\n    print(f\"Loaded train.csv with shape: {df.shape}\")\n    patient_summary = compute_patient_summary(df)\n    print(f\"Computed patient summary: {len(patient_summary)} patients\")\n    # We'll run 5-fold CV and then final evaluation on hold-out fold if desired\n    fold_results = run_kfold_cv(patient_summary, n_splits=5, alpha_fixed=ALPHA_FIXED)\n\n    # After CV you may want to train on full training set and evaluate on held-out test you create.\n    # Example: create final train/val/test split (patient-level)\n    # Here we create a fixed 60/20/20 split for final hold-out evaluation\n    from sklearn.model_selection import train_test_split\n    pats = patient_summary['Patient'].values\n    t1, test_pats = train_test_split(pats, test_size=0.2, random_state=SEED)\n    train_pats, val_pats = train_test_split(t1, test_size=0.25, random_state=SEED)  # 0.25 * 0.8 = 0.2 => 60/20/20\n    train_df = patient_summary[patient_summary['Patient'].isin(train_pats)].reset_index(drop=True)\n    val_df = patient_summary[patient_summary['Patient'].isin(val_pats)].reset_index(drop=True)\n    test_df = patient_summary[patient_summary['Patient'].isin(test_pats)].reset_index(drop=True)\n    print(f\"\\nFinal split sizes: Train {len(train_df)} Val {len(val_df)} Test {len(test_df)}\")\n\n    # Train final model on Train+Val (or just train_df), evaluate on test_df\n    full_train_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n    train_ds = OSICPatientDataset(full_train_df, TRAIN_DIR, augment=False)\n    test_ds = OSICPatientDataset(test_df, TRAIN_DIR, augment=False)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=patient_collate_fn)\n    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=patient_collate_fn)\n\n    final_model = PatientLevelModel(tab_dim=5)\n    trainer = PatientTrainer(final_model, DEVICE, lr=LR, use_laplace_loss=True, alpha_fixed=ALPHA_FIXED)\n    print(\"\\nTraining final model on combined train+val for final test evaluation...\")\n    trainer.train(train_loader, test_loader, epochs=EPOCHS)\n    final_model.load_state_dict(torch.load('best_patient_model_fold.pth'))\n    final_model.to(DEVICE)\n    final_metrics = evaluate_model_on_loader(final_model, test_loader, device=DEVICE, exclude_baseline_from_r2=True, compute_plots=True)\n\n    # Tabular-only ablation: quick training to predict slope on full_train_df\n    print(\"\\nTraining tabular-only baseline (predict slope) on train+val...\")\n    tab_model = TabularOnlyModel(tab_dim=5)\n    tab_model.to(DEVICE)\n    opt_tab = torch.optim.AdamW(tab_model.parameters(), lr=LR, weight_decay=1e-4)\n\n    NUM_TAB_EPOCHS = 12\n    for epoch in range(NUM_TAB_EPOCHS):\n        tab_model.train()\n        losses = []\n        for batch in train_loader:\n            # batch structure: images, tabs, weeks_rel_list, fvc_targets_list, baseline_fvcs, baseline_weeks, slopes, patients, weeks_abs_list, fvc_list\n            _, tabs, _, _, _, _, slopes_list, _, _, _ = batch\n            # tabs is tensor (B, tab_dim), slopes_list is a python list of length B\n            tabs = tabs.to(DEVICE)\n            # create slopes tensor shape (B,)\n            slopes_tensor = torch.tensor([float(s) for s in slopes_list], dtype=torch.float32, device=DEVICE)\n            \n            # Sanity checks (optional)\n            assert tabs.dim() == 2 and tabs.size(0) == slopes_tensor.size(0), \"Batch size mismatch between tabs and slopes\"\n\n            opt_tab.zero_grad()\n            pred_slope_batch, log_sigma0_batch = tab_model(None, tabs)  # pred_slope_batch shape (B,)\n            \n            # ensure pred_slope_batch is same shape as slopes_tensor\n            if pred_slope_batch.dim() == 0:\n                pred_slope_batch = pred_slope_batch.unsqueeze(0)\n            \n            # Sanity check (optional)\n            assert pred_slope_batch.size() == slopes_tensor.size(), f\"pred {pred_slope_batch.size()} vs target {slopes_tensor.size()}\"\n            \n            loss = F.l1_loss(pred_slope_batch, slopes_tensor)\n            loss.backward()\n            opt_tab.step()\n            losses.append(float(loss.item()))\n\n        avg_loss = float(np.mean(losses)) if losses else 0.0\n        if (epoch+1) % 2 == 0 or epoch == NUM_TAB_EPOCHS-1:\n            print(f\" Tab-only epoch {epoch+1}/{NUM_TAB_EPOCHS} avg L1 loss: {avg_loss:.4f}\")\n\n    print(\"\\nEvaluating tabular-only model on test set...\")\n    metrics_tab = evaluate_model_on_loader(tab_model, test_loader, device=DEVICE, exclude_baseline_from_r2=True, compute_plots=False)\n\n    # Print final summary\n    print(\"\\nFinal test summary:\")\n    print(f\"Image+Tab MAE: {final_metrics['mae']:.1f} mL | RÂ² (follow-up): {final_metrics['r2_excluding_baseline']:.3f} | Laplace LL: {final_metrics['laplace_ll']:.3f}\")\n    print(f\"Tabular-only MAE: {metrics_tab['mae']:.1f} mL | Baseline-only MAE: {final_metrics['mae_baseline']:.1f} mL | LinEx MAE: {final_metrics['mae_linex']:.1f} mL\")\n    print(\"\\nDone.\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:11:52.124173Z","iopub.execute_input":"2025-10-12T08:11:52.124526Z","iopub.status.idle":"2025-10-12T08:17:24.385059Z","shell.execute_reply.started":"2025-10-12T08:11:52.124499Z","shell.execute_reply":"2025-10-12T08:17:24.384073Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoaded train.csv with shape: (1549, 7)\nComputed patient summary: 176 patients\n\n============================================================\nFold 1/5\n============================================================\nDataset initialized: 140 patients with images (filtered 0)\nDataset initialized: 36 patients with images (filtered 0)\nEpoch 1/8 - Train Loss: 7.244823\n\nSummary (over all observed visits in validation):\nModel    MAE: 184.4 mL | RMSE: 266.9 mL | RÂ² (incl base): 0.9034 | Laplace LL: -8.095689\nModel    MAE (follow-up only): 209.4 mL | RÂ² (follow-up only): 0.8908\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9530\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.914 Â± 2.662\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 34.6%  | Â±2Ïƒ: 54.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 74.6 mL (n=63)\n >48w: 110.8 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=170.5091106614401  RÂ²=0.9318125244881182  n=43\n 24w: MAE=201.9578568027133  RÂ²=0.8816692859503744  n=21\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 184.400 mL - model saved\nEpoch 2/8 - Train Loss: 7.188275\n\nSummary (over all observed visits in validation):\nModel    MAE: 185.9 mL | RMSE: 270.6 mL | RÂ² (incl base): 0.9007 | Laplace LL: -8.122991\nModel    MAE (follow-up only): 211.1 mL | RÂ² (follow-up only): 0.8878\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9516\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.893 Â± 2.550\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 33.7%  | Â±2Ïƒ: 53.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 74.8 mL (n=63)\n >48w: 111.1 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=172.8665452336156  RÂ²=0.9313732615320243  n=43\n 24w: MAE=194.58517744427635  RÂ²=0.8878403300758178  n=21\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 3/8 - Train Loss: 7.165286\n\nSummary (over all observed visits in validation):\nModel    MAE: 182.8 mL | RMSE: 266.8 mL | RÂ² (incl base): 0.9034 | Laplace LL: -8.063803\nModel    MAE (follow-up only): 207.6 mL | RÂ² (follow-up only): 0.8909\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9532\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.836 Â± 2.628\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 35.9%  | Â±2Ïƒ: 54.8%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 74.9 mL (n=63)\n >48w: 111.4 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=169.58714388969332  RÂ²=0.9314641373355408  n=43\n 24w: MAE=197.2927951812744  RÂ²=0.8889605072251757  n=21\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 182.775 mL - model saved\nEpoch 4/8 - Train Loss: 7.065406\n\nSummary (over all observed visits in validation):\nModel    MAE: 183.6 mL | RMSE: 266.6 mL | RÂ² (incl base): 0.9035 | Laplace LL: -8.083099\nModel    MAE (follow-up only): 208.5 mL | RÂ² (follow-up only): 0.8910\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9542\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.846 Â± 2.768\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 34.9%  | Â±2Ïƒ: 54.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.0 mL (n=63)\n >48w: 111.5 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=169.6594152984231  RÂ²=0.9298212924849301  n=43\n 24w: MAE=212.90959974697657  RÂ²=0.8737008999307933  n=21\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 5/8 - Train Loss: 7.084235\n\nSummary (over all observed visits in validation):\nModel    MAE: 183.2 mL | RMSE: 265.9 mL | RÂ² (incl base): 0.9041 | Laplace LL: -8.071149\nModel    MAE (follow-up only): 208.1 mL | RÂ² (follow-up only): 0.8917\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9534\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.867 Â± 2.515\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 34.3%  | Â±2Ïƒ: 54.2%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.2 mL (n=63)\n >48w: 111.8 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=169.4116477557393  RÂ²=0.9310748805672511  n=43\n 24w: MAE=205.293317428657  RÂ²=0.8753319160691885  n=21\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 6/8 - Train Loss: 7.031533\n\nSummary (over all observed visits in validation):\nModel    MAE: 178.9 mL | RMSE: 260.7 mL | RÂ² (incl base): 0.9078 | Laplace LL: -7.991163\nModel    MAE (follow-up only): 203.1 mL | RÂ² (follow-up only): 0.8958\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9555\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.887 Â± 3.317\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 36.2%  | Â±2Ïƒ: 56.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.2 mL (n=63)\n >48w: 112.0 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=165.96005028659522  RÂ²=0.934182699115793  n=43\n 24w: MAE=202.80720394849777  RÂ²=0.8770999828893032  n=21\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 178.880 mL - model saved\nEpoch 7/8 - Train Loss: 7.023088\n\nSummary (over all observed visits in validation):\nModel    MAE: 179.9 mL | RMSE: 264.1 mL | RÂ² (incl base): 0.9053 | Laplace LL: -8.009763\nModel    MAE (follow-up only): 204.3 mL | RÂ² (follow-up only): 0.8931\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9537\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.754 Â± 2.654\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 36.9%  | Â±2Ïƒ: 56.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.5 mL (n=63)\n >48w: 112.4 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=167.91957272485246  RÂ²=0.9318906730258668  n=43\n 24w: MAE=197.90601240737098  RÂ²=0.8837949549098544  n=21\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 8/8 - Train Loss: 6.919522\n\nSummary (over all observed visits in validation):\nModel    MAE: 185.9 mL | RMSE: 268.5 mL | RÂ² (incl base): 0.9022 | Laplace LL: -8.107028\nModel    MAE (follow-up only): 211.2 mL | RÂ² (follow-up only): 0.8895\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9512\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -2.631 Â± 4.182\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 34.9%  | Â±2Ïƒ: 53.8%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.5 mL (n=63)\n >48w: 112.5 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=169.4433721089086  RÂ²=0.9321307659620411  n=43\n 24w: MAE=209.24985089756194  RÂ²=0.8772047279937031  n=21\n 48w: MAE=None  RÂ²=None  n=1\n\nSummary (over all observed visits in validation):\nModel    MAE: 178.9 mL | RMSE: 260.7 mL | RÂ² (incl base): 0.9078 | Laplace LL: -7.991163\nModel    MAE (follow-up only): 203.1 mL | RÂ² (follow-up only): 0.8958\nBaseline MAE: 203.8 mL | RMSE: 295.2 mL | RÂ²: 0.8817 | Laplace LL: -8.711699\nLinEx MAE: 1782.3 mL | RMSE: 6401.7 mL | RÂ²: -54.6020 | Laplace LL: -40.603172\nPearson Correlation (model): 0.9555\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.887 Â± 3.317\n Baseline RÂ² meanÂ±std: -1.932 Â± 1.889\n\nUncertainty coverage: Â±1Ïƒ: 36.2%  | Â±2Ïƒ: 56.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=176)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.2 mL (n=63)\n >48w: 112.0 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=165.96005028659522  RÂ²=0.934182699115793  n=43\n 24w: MAE=202.80720394849777  RÂ²=0.8770999828893032  n=21\n 48w: MAE=None  RÂ²=None  n=1\nFold 1 result MAE: 178.9 mL  RÂ²(excl baseline): 0.896  Laplace LL: -7.991\n\n============================================================\nFold 2/5\n============================================================\nDataset initialized: 141 patients with images (filtered 0)\nDataset initialized: 35 patients with images (filtered 0)\nEpoch 1/8 - Train Loss: 7.595275\n\nSummary (over all observed visits in validation):\nModel    MAE: 121.8 mL | RMSE: 187.9 mL | RÂ² (incl base): 0.9340 | Laplace LL: -6.945419\nModel    MAE (follow-up only): 137.5 mL | RÂ² (follow-up only): 0.9256\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9679\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.733 Â± 1.529\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 50.6%  | Â±2Ïƒ: 73.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 74.5 mL (n=68)\n >48w: 112.1 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=151.3089494909559  RÂ²=0.8798788681013872  n=35\n 24w: MAE=166.0851832117353  RÂ²=0.9047965589715675  n=21\n 48w: MAE=None  RÂ²=None  n=0\nâœ… NEW BEST VAL MAE: 121.832 mL - model saved\nEpoch 2/8 - Train Loss: 7.478472\n\nSummary (over all observed visits in validation):\nModel    MAE: 122.6 mL | RMSE: 188.2 mL | RÂ² (incl base): 0.9338 | Laplace LL: -6.952174\nModel    MAE (follow-up only): 138.3 mL | RÂ² (follow-up only): 0.9254\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9678\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.744 Â± 1.527\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 52.6%  | Â±2Ïƒ: 73.4%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 74.9 mL (n=68)\n >48w: 113.0 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=150.6675897870745  RÂ²=0.8809903157790029  n=35\n 24w: MAE=166.80285742169335  RÂ²=0.9069330767872777  n=21\n 48w: MAE=None  RÂ²=None  n=0\nEpoch 3/8 - Train Loss: 7.459913\n\nSummary (over all observed visits in validation):\nModel    MAE: 123.5 mL | RMSE: 190.3 mL | RÂ² (incl base): 0.9323 | Laplace LL: -6.968119\nModel    MAE (follow-up only): 139.3 mL | RÂ² (follow-up only): 0.9237\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9671\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.884 Â± 1.700\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 50.6%  | Â±2Ïƒ: 73.7%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 74.9 mL (n=68)\n >48w: 112.9 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=151.73145506381988  RÂ²=0.8803416560547817  n=35\n 24w: MAE=164.79179222810836  RÂ²=0.9052457250254073  n=21\n 48w: MAE=None  RÂ²=None  n=0\nEpoch 4/8 - Train Loss: 7.356244\n\nSummary (over all observed visits in validation):\nModel    MAE: 126.7 mL | RMSE: 195.0 mL | RÂ² (incl base): 0.9289 | Laplace LL: -6.997317\nModel    MAE (follow-up only): 142.9 mL | RÂ² (follow-up only): 0.9199\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9658\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.991 Â± 1.788\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 52.3%  | Â±2Ïƒ: 73.4%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 74.8 mL (n=68)\n >48w: 112.7 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=147.7959325645651  RÂ²=0.8801087991292933  n=35\n 24w: MAE=161.2704160671149  RÂ²=0.911084247124623  n=21\n 48w: MAE=None  RÂ²=None  n=0\nEpoch 5/8 - Train Loss: 7.360915\n\nSummary (over all observed visits in validation):\nModel    MAE: 124.7 mL | RMSE: 189.5 mL | RÂ² (incl base): 0.9329 | Laplace LL: -6.987008\nModel    MAE (follow-up only): 140.7 mL | RÂ² (follow-up only): 0.9244\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9668\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.069 Â± 1.930\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 49.7%  | Â±2Ïƒ: 72.7%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 74.9 mL (n=68)\n >48w: 112.9 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=152.6092302935464  RÂ²=0.8844030710248382  n=35\n 24w: MAE=162.92674974032812  RÂ²=0.9137283351573108  n=21\n 48w: MAE=None  RÂ²=None  n=0\nEpoch 6/8 - Train Loss: 7.223941\n\nSummary (over all observed visits in validation):\nModel    MAE: 131.4 mL | RMSE: 200.4 mL | RÂ² (incl base): 0.9249 | Laplace LL: -7.090403\nModel    MAE (follow-up only): 148.3 mL | RÂ² (follow-up only): 0.9154\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9641\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.276 Â± 2.337\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 48.4%  | Â±2Ïƒ: 71.4%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 74.8 mL (n=68)\n >48w: 112.9 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=154.30131734439306  RÂ²=0.8819238557211899  n=35\n 24w: MAE=175.1120400386197  RÂ²=0.9026722101475819  n=21\n 48w: MAE=None  RÂ²=None  n=0\nEpoch 7/8 - Train Loss: 7.155564\n\nSummary (over all observed visits in validation):\nModel    MAE: 129.5 mL | RMSE: 199.4 mL | RÂ² (incl base): 0.9256 | Laplace LL: -7.040461\nModel    MAE (follow-up only): 146.1 mL | RÂ² (follow-up only): 0.9162\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9638\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.393 Â± 2.566\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 51.6%  | Â±2Ïƒ: 72.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 75.0 mL (n=68)\n >48w: 113.1 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=146.9492550441197  RÂ²=0.883574615383365  n=35\n 24w: MAE=163.7143429517746  RÂ²=0.9083898386511544  n=21\n 48w: MAE=None  RÂ²=None  n=0\nEpoch 8/8 - Train Loss: 7.278442\n\nSummary (over all observed visits in validation):\nModel    MAE: 134.1 mL | RMSE: 203.2 mL | RÂ² (incl base): 0.9228 | Laplace LL: -7.133852\nModel    MAE (follow-up only): 151.3 mL | RÂ² (follow-up only): 0.9130\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9624\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.352 Â± 2.133\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 49.4%  | Â±2Ïƒ: 71.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 75.2 mL (n=68)\n >48w: 113.6 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=154.98919526849474  RÂ²=0.8769876442649127  n=35\n 24w: MAE=174.7600382566452  RÂ²=0.8972923788080575  n=21\n 48w: MAE=None  RÂ²=None  n=0\n\nSummary (over all observed visits in validation):\nModel    MAE: 121.8 mL | RMSE: 187.9 mL | RÂ² (incl base): 0.9340 | Laplace LL: -6.945419\nModel    MAE (follow-up only): 137.5 mL | RÂ² (follow-up only): 0.9256\nBaseline MAE: 144.6 mL | RMSE: 213.0 mL | RÂ²: 0.9152 | Laplace LL: -7.516378\nLinEx MAE: 795.6 mL | RMSE: 1916.0 mL | RÂ²: -5.8654 | Laplace LL: -20.667887\nPearson Correlation (model): 0.9679\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.733 Â± 1.529\n Baseline RÂ² meanÂ±std: -1.159 Â± 1.289\n\nUncertainty coverage: Â±1Ïƒ: 50.6%  | Â±2Ïƒ: 73.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=37)\n 25-48w: 74.5 mL (n=68)\n >48w: 112.1 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=151.3089494909559  RÂ²=0.8798788681013872  n=35\n 24w: MAE=166.0851832117353  RÂ²=0.9047965589715675  n=21\n 48w: MAE=None  RÂ²=None  n=0\nFold 2 result MAE: 121.8 mL  RÂ²(excl baseline): 0.926  Laplace LL: -6.945\n\n============================================================\nFold 3/5\n============================================================\nDataset initialized: 141 patients with images (filtered 0)\nDataset initialized: 35 patients with images (filtered 0)\nEpoch 1/8 - Train Loss: 7.401967\n\nSummary (over all observed visits in validation):\nModel    MAE: 147.2 mL | RMSE: 218.3 mL | RÂ² (incl base): 0.9256 | Laplace LL: -7.380493\nModel    MAE (follow-up only): 166.5 mL | RÂ² (follow-up only): 0.9152\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9636\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.535 Â± 2.954\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 39.9%  | Â±2Ïƒ: 64.3%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 74.9 mL (n=63)\n >48w: 111.0 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=167.75977178542846  RÂ²=0.9282959317805255  n=31\n 24w: MAE=168.14803142547606  RÂ²=0.8828646966710036  n=20\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 147.194 mL - model saved\nEpoch 2/8 - Train Loss: 7.350428\n\nSummary (over all observed visits in validation):\nModel    MAE: 147.5 mL | RMSE: 217.0 mL | RÂ² (incl base): 0.9265 | Laplace LL: -7.378876\nModel    MAE (follow-up only): 166.8 mL | RÂ² (follow-up only): 0.9163\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9642\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.649 Â± 3.188\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 37.9%  | Â±2Ïƒ: 63.3%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.3 mL (n=63)\n >48w: 111.8 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=167.70854277764596  RÂ²=0.9292708937998587  n=31\n 24w: MAE=162.34978642463685  RÂ²=0.8891125269890446  n=20\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 3/8 - Train Loss: 7.311423\n\nSummary (over all observed visits in validation):\nModel    MAE: 150.1 mL | RMSE: 217.3 mL | RÂ² (incl base): 0.9263 | Laplace LL: -7.424032\nModel    MAE (follow-up only): 169.8 mL | RÂ² (follow-up only): 0.9160\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9643\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.939 Â± 3.773\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 35.7%  | Â±2Ïƒ: 62.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.4 mL (n=63)\n >48w: 111.9 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=170.62866703925593  RÂ²=0.9285018442627665  n=31\n 24w: MAE=164.8455003976822  RÂ²=0.8902270220827121  n=20\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 4/8 - Train Loss: 7.280871\n\nSummary (over all observed visits in validation):\nModel    MAE: 145.4 mL | RMSE: 213.1 mL | RÂ² (incl base): 0.9291 | Laplace LL: -7.348640\nModel    MAE (follow-up only): 164.5 mL | RÂ² (follow-up only): 0.9193\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9656\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.490 Â± 2.751\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 38.3%  | Â±2Ïƒ: 65.0%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.5 mL (n=63)\n >48w: 112.2 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=167.73783072781177  RÂ²=0.930433619774885  n=31\n 24w: MAE=158.84031715393067  RÂ²=0.8951955315744156  n=20\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 145.429 mL - model saved\nEpoch 5/8 - Train Loss: 7.246240\n\nSummary (over all observed visits in validation):\nModel    MAE: 147.5 mL | RMSE: 219.3 mL | RÂ² (incl base): 0.9249 | Laplace LL: -7.371043\nModel    MAE (follow-up only): 166.8 mL | RÂ² (follow-up only): 0.9145\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9636\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.593 Â± 3.781\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 39.2%  | Â±2Ïƒ: 64.6%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.7 mL (n=63)\n >48w: 112.4 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=167.71371455346383  RÂ²=0.9297908773074935  n=31\n 24w: MAE=157.73399796485901  RÂ²=0.8979547216045887  n=20\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 6/8 - Train Loss: 7.287892\n\nSummary (over all observed visits in validation):\nModel    MAE: 148.2 mL | RMSE: 213.7 mL | RÂ² (incl base): 0.9287 | Laplace LL: -7.390979\nModel    MAE (follow-up only): 167.6 mL | RÂ² (follow-up only): 0.9188\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9655\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.983 Â± 4.241\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 38.3%  | Â±2Ïƒ: 62.7%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.7 mL (n=63)\n >48w: 112.5 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=169.53416923169166  RÂ²=0.9301100511324165  n=31\n 24w: MAE=159.6923009991646  RÂ²=0.8980215079539783  n=20\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 7/8 - Train Loss: 7.210503\n\nSummary (over all observed visits in validation):\nModel    MAE: 142.6 mL | RMSE: 220.4 mL | RÂ² (incl base): 0.9241 | Laplace LL: -7.293087\nModel    MAE (follow-up only): 161.2 mL | RÂ² (follow-up only): 0.9136\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9634\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.817 Â± 1.780\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 44.1%  | Â±2Ïƒ: 66.9%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.9 mL (n=63)\n >48w: 112.7 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=160.42734206876446  RÂ²=0.93212420127829  n=31\n 24w: MAE=164.15256335735322  RÂ²=0.8896670972923384  n=20\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 142.581 mL - model saved\nEpoch 8/8 - Train Loss: 7.188087\n\nSummary (over all observed visits in validation):\nModel    MAE: 141.9 mL | RMSE: 209.7 mL | RÂ² (incl base): 0.9314 | Laplace LL: -7.276116\nModel    MAE (follow-up only): 160.4 mL | RÂ² (follow-up only): 0.9218\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9673\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.544 Â± 3.679\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 42.1%  | Â±2Ïƒ: 63.3%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.9 mL (n=63)\n >48w: 112.9 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=160.20395452360953  RÂ²=0.9355477908006985  n=31\n 24w: MAE=154.99380759000778  RÂ²=0.9031954653967346  n=20\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 141.882 mL - model saved\n\nSummary (over all observed visits in validation):\nModel    MAE: 141.9 mL | RMSE: 209.7 mL | RÂ² (incl base): 0.9314 | Laplace LL: -7.276116\nModel    MAE (follow-up only): 160.4 mL | RÂ² (follow-up only): 0.9218\nBaseline MAE: 151.2 mL | RMSE: 241.9 mL | RÂ²: 0.9087 | Laplace LL: -7.649952\nLinEx MAE: 1263.9 mL | RMSE: 3233.1 mL | RÂ²: -15.3183 | Laplace LL: -30.129380\nPearson Correlation (model): 0.9673\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.544 Â± 3.679\n Baseline RÂ² meanÂ±std: -0.858 Â± 1.166\n\nUncertainty coverage: Â±1Ïƒ: 42.1%  | Â±2Ïƒ: 63.3%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=172)\n 13-24w: 70.0 mL (n=42)\n 25-48w: 75.9 mL (n=63)\n >48w: 112.9 mL (n=34)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=160.20395452360953  RÂ²=0.9355477908006985  n=31\n 24w: MAE=154.99380759000778  RÂ²=0.9031954653967346  n=20\n 48w: MAE=None  RÂ²=None  n=1\nFold 3 result MAE: 141.9 mL  RÂ²(excl baseline): 0.922  Laplace LL: -7.276\n\n============================================================\nFold 4/5\n============================================================\nDataset initialized: 141 patients with images (filtered 0)\nDataset initialized: 35 patients with images (filtered 0)\nEpoch 1/8 - Train Loss: 7.595282\n\nSummary (over all observed visits in validation):\nModel    MAE: 136.1 mL | RMSE: 209.1 mL | RÂ² (incl base): 0.9366 | Laplace LL: -7.185241\nModel    MAE (follow-up only): 153.3 mL | RÂ² (follow-up only): 0.9290\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9678\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.396 Â± 2.854\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 45.3%  | Â±2Ïƒ: 69.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 75.3 mL (n=67)\n >48w: 112.4 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=153.5895775137721  RÂ²=0.9474252992266434  n=37\n 24w: MAE=142.77303904294968  RÂ²=0.9364210481999886  n=16\n 48w: MAE=None  RÂ²=None  n=2\nâœ… NEW BEST VAL MAE: 136.064 mL - model saved\nEpoch 2/8 - Train Loss: 7.434826\n\nSummary (over all observed visits in validation):\nModel    MAE: 135.8 mL | RMSE: 209.7 mL | RÂ² (incl base): 0.9362 | Laplace LL: -7.177390\nModel    MAE (follow-up only): 153.0 mL | RÂ² (follow-up only): 0.9286\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9677\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.376 Â± 2.956\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 44.1%  | Â±2Ïƒ: 68.8%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 75.5 mL (n=67)\n >48w: 112.6 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=153.82887893754082  RÂ²=0.947491722432909  n=37\n 24w: MAE=136.55449478328228  RÂ²=0.9388230775864154  n=16\n 48w: MAE=None  RÂ²=None  n=2\nâœ… NEW BEST VAL MAE: 135.781 mL - model saved\nEpoch 3/8 - Train Loss: 7.388652\n\nSummary (over all observed visits in validation):\nModel    MAE: 139.5 mL | RMSE: 215.5 mL | RÂ² (incl base): 0.9326 | Laplace LL: -7.235018\nModel    MAE (follow-up only): 157.2 mL | RÂ² (follow-up only): 0.9246\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9659\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.198 Â± 1.949\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 45.3%  | Â±2Ïƒ: 68.8%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 75.8 mL (n=67)\n >48w: 113.3 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=152.3512853609549  RÂ²=0.9467697715358796  n=37\n 24w: MAE=140.34078434109688  RÂ²=0.9378618851382353  n=16\n 48w: MAE=None  RÂ²=None  n=2\nEpoch 4/8 - Train Loss: 7.357490\n\nSummary (over all observed visits in validation):\nModel    MAE: 139.3 mL | RMSE: 214.3 mL | RÂ² (incl base): 0.9334 | Laplace LL: -7.229900\nModel    MAE (follow-up only): 157.0 mL | RÂ² (follow-up only): 0.9254\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9661\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.423 Â± 2.601\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 45.0%  | Â±2Ïƒ: 66.6%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 75.8 mL (n=67)\n >48w: 113.3 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=152.88972662590646  RÂ²=0.9472645053134121  n=37\n 24w: MAE=144.26350651681423  RÂ²=0.9342318657375736  n=16\n 48w: MAE=None  RÂ²=None  n=2\nEpoch 5/8 - Train Loss: 7.399187\n\nSummary (over all observed visits in validation):\nModel    MAE: 144.0 mL | RMSE: 220.4 mL | RÂ² (incl base): 0.9295 | Laplace LL: -7.310730\nModel    MAE (follow-up only): 162.3 mL | RÂ² (follow-up only): 0.9211\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9652\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.425 Â± 2.704\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 43.4%  | Â±2Ïƒ: 67.2%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 75.9 mL (n=67)\n >48w: 113.5 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=154.11882892391972  RÂ²=0.947314128116025  n=37\n 24w: MAE=150.2539150142111  RÂ²=0.9322400458760172  n=16\n 48w: MAE=None  RÂ²=None  n=2\nEpoch 6/8 - Train Loss: 7.371366\n\nSummary (over all observed visits in validation):\nModel    MAE: 148.1 mL | RMSE: 227.1 mL | RÂ² (incl base): 0.9252 | Laplace LL: -7.377859\nModel    MAE (follow-up only): 166.9 mL | RÂ² (follow-up only): 0.9163\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9622\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.527 Â± 2.828\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 42.4%  | Â±2Ïƒ: 64.3%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 76.0 mL (n=67)\n >48w: 113.7 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=160.37096305070696  RÂ²=0.9426483948956667  n=37\n 24w: MAE=132.75841946713626  RÂ²=0.934279175899629  n=16\n 48w: MAE=None  RÂ²=None  n=2\nEpoch 7/8 - Train Loss: 7.155010\n\nSummary (over all observed visits in validation):\nModel    MAE: 145.8 mL | RMSE: 227.3 mL | RÂ² (incl base): 0.9251 | Laplace LL: -7.335913\nModel    MAE (follow-up only): 164.3 mL | RÂ² (follow-up only): 0.9161\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9636\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.620 Â± 4.614\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 44.7%  | Â±2Ïƒ: 65.3%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 76.2 mL (n=67)\n >48w: 114.0 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=152.62941689265742  RÂ²=0.9461191324448093  n=37\n 24w: MAE=154.67303251847625  RÂ²=0.9319915031272775  n=16\n 48w: MAE=None  RÂ²=None  n=2\nEpoch 8/8 - Train Loss: 7.201330\n\nSummary (over all observed visits in validation):\nModel    MAE: 146.9 mL | RMSE: 222.8 mL | RÂ² (incl base): 0.9280 | Laplace LL: -7.338590\nModel    MAE (follow-up only): 165.5 mL | RÂ² (follow-up only): 0.9194\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9634\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -2.013 Â± 4.567\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 42.1%  | Â±2Ïƒ: 65.6%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 76.4 mL (n=67)\n >48w: 114.4 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=149.73392063439698  RÂ²=0.9490066181598609  n=37\n 24w: MAE=161.2083923406899  RÂ²=0.9250886621388156  n=16\n 48w: MAE=None  RÂ²=None  n=2\n\nSummary (over all observed visits in validation):\nModel    MAE: 135.8 mL | RMSE: 209.7 mL | RÂ² (incl base): 0.9362 | Laplace LL: -7.177390\nModel    MAE (follow-up only): 153.0 mL | RÂ² (follow-up only): 0.9286\nBaseline MAE: 149.2 mL | RMSE: 234.7 mL | RÂ²: 0.9201 | Laplace LL: -7.609676\nLinEx MAE: 605.6 mL | RMSE: 1228.8 mL | RÂ²: -1.1901 | Laplace LL: -16.830274\nPearson Correlation (model): 0.9677\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.376 Â± 2.956\n Baseline RÂ² meanÂ±std: -0.956 Â± 1.176\n\nUncertainty coverage: Â±1Ïƒ: 44.1%  | Â±2Ïƒ: 68.8%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=167)\n 13-24w: 70.0 mL (n=46)\n 25-48w: 75.5 mL (n=67)\n >48w: 112.6 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=153.82887893754082  RÂ²=0.947491722432909  n=37\n 24w: MAE=136.55449478328228  RÂ²=0.9388230775864154  n=16\n 48w: MAE=None  RÂ²=None  n=2\nFold 4 result MAE: 135.8 mL  RÂ²(excl baseline): 0.929  Laplace LL: -7.177\n\n============================================================\nFold 5/5\n============================================================\nDataset initialized: 141 patients with images (filtered 0)\nDataset initialized: 35 patients with images (filtered 0)\nEpoch 1/8 - Train Loss: 7.501650\n\nSummary (over all observed visits in validation):\nModel    MAE: 141.4 mL | RMSE: 207.9 mL | RÂ² (incl base): 0.9449 | Laplace LL: -7.306939\nModel    MAE (follow-up only): 159.8 mL | RÂ² (follow-up only): 0.9374\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9752\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.823 Â± 1.363\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 41.0%  | Â±2Ïƒ: 62.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 74.9 mL (n=66)\n >48w: 112.1 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=133.39824351999494  RÂ²=0.9576241720008651  n=36\n 24w: MAE=177.06788392861685  RÂ²=0.8863409814017004  n=18\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 141.402 mL - model saved\nEpoch 2/8 - Train Loss: 7.354630\n\nSummary (over all observed visits in validation):\nModel    MAE: 147.5 mL | RMSE: 217.0 mL | RÂ² (incl base): 0.9400 | Laplace LL: -7.400558\nModel    MAE (follow-up only): 166.7 mL | RÂ² (follow-up only): 0.9318\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9725\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.030 Â± 1.594\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 40.1%  | Â±2Ïƒ: 60.6%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.2 mL (n=66)\n >48w: 112.7 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=136.2721203068892  RÂ²=0.956037651274606  n=36\n 24w: MAE=187.36808902687497  RÂ²=0.8706577208097526  n=18\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 3/8 - Train Loss: 7.329773\n\nSummary (over all observed visits in validation):\nModel    MAE: 142.6 mL | RMSE: 206.8 mL | RÂ² (incl base): 0.9455 | Laplace LL: -7.312062\nModel    MAE (follow-up only): 161.1 mL | RÂ² (follow-up only): 0.9381\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9748\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.990 Â± 1.645\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 41.4%  | Â±2Ïƒ: 61.6%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.3 mL (n=66)\n >48w: 113.0 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=132.56088412801424  RÂ²=0.957948604665885  n=36\n 24w: MAE=177.3550098207262  RÂ²=0.8876253146269029  n=18\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 4/8 - Train Loss: 7.264105\n\nSummary (over all observed visits in validation):\nModel    MAE: 135.4 mL | RMSE: 199.3 mL | RÂ² (incl base): 0.9494 | Laplace LL: -7.196666\nModel    MAE (follow-up only): 153.0 mL | RÂ² (follow-up only): 0.9425\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9775\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.678 Â± 1.114\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 42.3%  | Â±2Ïƒ: 67.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.2 mL (n=66)\n >48w: 112.8 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=130.86007707276278  RÂ²=0.9587723378103779  n=36\n 24w: MAE=163.8559184115794  RÂ²=0.9018712132037499  n=18\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 135.392 mL - model saved\nEpoch 5/8 - Train Loss: 7.252313\n\nSummary (over all observed visits in validation):\nModel    MAE: 138.7 mL | RMSE: 203.2 mL | RÂ² (incl base): 0.9474 | Laplace LL: -7.248898\nModel    MAE (follow-up only): 156.7 mL | RÂ² (follow-up only): 0.9402\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9761\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.841 Â± 1.382\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 41.4%  | Â±2Ïƒ: 65.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.4 mL (n=66)\n >48w: 113.2 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=132.24488253229194  RÂ²=0.9573108629883371  n=36\n 24w: MAE=173.22054383158684  RÂ²=0.8792373613302673  n=18\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 6/8 - Train Loss: 7.214478\n\nSummary (over all observed visits in validation):\nModel    MAE: 148.1 mL | RMSE: 217.5 mL | RÂ² (incl base): 0.9397 | Laplace LL: -7.403050\nModel    MAE (follow-up only): 167.4 mL | RÂ² (follow-up only): 0.9315\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9768\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.099 Â± 1.721\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 44.3%  | Â±2Ïƒ: 63.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.0 mL (n=66)\n >48w: 112.3 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=134.7708139369885  RÂ²=0.9554525274905288  n=36\n 24w: MAE=162.27620142698288  RÂ²=0.9032187912107821  n=18\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 7/8 - Train Loss: 7.276502\n\nSummary (over all observed visits in validation):\nModel    MAE: 145.4 mL | RMSE: 212.8 mL | RÂ² (incl base): 0.9423 | Laplace LL: -7.360066\nModel    MAE (follow-up only): 164.4 mL | RÂ² (follow-up only): 0.9344\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9736\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.104 Â± 1.672\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 38.1%  | Â±2Ïƒ: 63.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.8 mL (n=66)\n >48w: 114.0 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=135.5020448507534  RÂ²=0.9556172592018769  n=36\n 24w: MAE=183.4208112789525  RÂ²=0.872297805196618  n=18\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 8/8 - Train Loss: 7.115156\n\nSummary (over all observed visits in validation):\nModel    MAE: 139.4 mL | RMSE: 204.4 mL | RÂ² (incl base): 0.9468 | Laplace LL: -7.272061\nModel    MAE (follow-up only): 157.5 mL | RÂ² (follow-up only): 0.9396\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9762\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.979 Â± 1.580\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 41.7%  | Â±2Ïƒ: 62.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.7 mL (n=66)\n >48w: 113.8 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=132.26417773196266  RÂ²=0.9566132344326689  n=36\n 24w: MAE=182.6549152727756  RÂ²=0.8792992149096075  n=18\n 48w: MAE=None  RÂ²=None  n=1\n\nSummary (over all observed visits in validation):\nModel    MAE: 135.4 mL | RMSE: 199.3 mL | RÂ² (incl base): 0.9494 | Laplace LL: -7.196666\nModel    MAE (follow-up only): 153.0 mL | RÂ² (follow-up only): 0.9425\nBaseline MAE: 160.1 mL | RMSE: 239.8 mL | RÂ²: 0.9268 | Laplace LL: -7.829992\nLinEx MAE: 796.3 mL | RMSE: 1738.2 mL | RÂ²: -2.8493 | Laplace LL: -20.682854\nPearson Correlation (model): 0.9775\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -0.678 Â± 1.114\n Baseline RÂ² meanÂ±std: -1.172 Â± 1.381\n\nUncertainty coverage: Â±1Ïƒ: 42.3%  | Â±2Ïƒ: 67.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=170)\n 13-24w: 70.0 mL (n=38)\n 25-48w: 75.2 mL (n=66)\n >48w: 112.8 mL (n=33)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=130.86007707276278  RÂ²=0.9587723378103779  n=36\n 24w: MAE=163.8559184115794  RÂ²=0.9018712132037499  n=18\n 48w: MAE=None  RÂ²=None  n=1\nFold 5 result MAE: 135.4 mL  RÂ²(excl baseline): 0.943  Laplace LL: -7.197\n\n============================================================\n5-fold CV summary (mean Â± std):\nMAE: 142.753 Â± 19.217\nLAPLACE_LL: -7.317 Â± 0.354\nR2_EXCLUDING_BASELINE: 0.923 Â± 0.015\n\nFinal split sizes: Train 105 Val 35 Test 36\nDataset initialized: 140 patients with images (filtered 0)\nDataset initialized: 36 patients with images (filtered 0)\n\nTraining final model on combined train+val for final test evaluation...\nEpoch 1/8 - Train Loss: 7.520799\n\nSummary (over all observed visits in validation):\nModel    MAE: 131.6 mL | RMSE: 197.9 mL | RÂ² (incl base): 0.9367 | Laplace LL: -7.097783\nModel    MAE (follow-up only): 148.7 mL | RÂ² (follow-up only): 0.9288\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9679\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.383 Â± 2.734\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 43.4%  | Â±2Ïƒ: 68.0%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 75.5 mL (n=66)\n >48w: 112.8 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=132.87863406320898  RÂ²=0.9458321561679471  n=41\n 24w: MAE=137.18603450373598  RÂ²=0.9231742404439789  n=19\n 48w: MAE=None  RÂ²=None  n=1\nâœ… NEW BEST VAL MAE: 131.577 mL - model saved\nEpoch 2/8 - Train Loss: 7.450760\n\nSummary (over all observed visits in validation):\nModel    MAE: 133.1 mL | RMSE: 199.6 mL | RÂ² (incl base): 0.9356 | Laplace LL: -7.122645\nModel    MAE (follow-up only): 150.4 mL | RÂ² (follow-up only): 0.9276\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9673\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.374 Â± 2.604\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 41.8%  | Â±2Ïƒ: 66.1%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 75.6 mL (n=66)\n >48w: 113.2 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=133.59274889783163  RÂ²=0.9456830659150095  n=41\n 24w: MAE=136.03122333476418  RÂ²=0.9228922148931744  n=19\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 3/8 - Train Loss: 7.419599\n\nSummary (over all observed visits in validation):\nModel    MAE: 136.0 mL | RMSE: 201.7 mL | RÂ² (incl base): 0.9342 | Laplace LL: -7.172897\nModel    MAE (follow-up only): 153.7 mL | RÂ² (follow-up only): 0.9260\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9666\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.613 Â± 2.922\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 41.5%  | Â±2Ïƒ: 65.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 75.8 mL (n=66)\n >48w: 113.4 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=135.6650862577485  RÂ²=0.945193497332047  n=41\n 24w: MAE=134.27457948734886  RÂ²=0.9253205690170141  n=19\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 4/8 - Train Loss: 7.325407\n\nSummary (over all observed visits in validation):\nModel    MAE: 142.0 mL | RMSE: 207.1 mL | RÂ² (incl base): 0.9306 | Laplace LL: -7.261612\nModel    MAE (follow-up only): 160.4 mL | RÂ² (follow-up only): 0.9220\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9648\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -2.153 Â± 3.868\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 38.9%  | Â±2Ïƒ: 63.6%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 75.7 mL (n=66)\n >48w: 113.4 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=137.17503882035976  RÂ²=0.9453011473136332  n=41\n 24w: MAE=137.0077650594084  RÂ²=0.9208521179052908  n=19\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 5/8 - Train Loss: 7.342239\n\nSummary (over all observed visits in validation):\nModel    MAE: 140.1 mL | RMSE: 205.3 mL | RÂ² (incl base): 0.9319 | Laplace LL: -7.250091\nModel    MAE (follow-up only): 158.3 mL | RÂ² (follow-up only): 0.9233\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9662\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.635 Â± 3.213\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 39.6%  | Â±2Ïƒ: 65.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 75.8 mL (n=66)\n >48w: 113.5 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=131.27506249670574  RÂ²=0.9464985363444456  n=41\n 24w: MAE=159.99160212121512  RÂ²=0.9063337461000608  n=19\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 6/8 - Train Loss: 7.216082\n\nSummary (over all observed visits in validation):\nModel    MAE: 137.0 mL | RMSE: 201.6 mL | RÂ² (incl base): 0.9343 | Laplace LL: -7.174791\nModel    MAE (follow-up only): 154.8 mL | RÂ² (follow-up only): 0.9261\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9671\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -2.084 Â± 4.180\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 40.8%  | Â±2Ïƒ: 66.5%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 76.2 mL (n=66)\n >48w: 114.2 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=138.19677298679585  RÂ²=0.9441501731308065  n=41\n 24w: MAE=141.82090008886237  RÂ²=0.9226823594795427  n=19\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 7/8 - Train Loss: 7.244015\n\nSummary (over all observed visits in validation):\nModel    MAE: 145.9 mL | RMSE: 211.2 mL | RÂ² (incl base): 0.9279 | Laplace LL: -7.320015\nModel    MAE (follow-up only): 164.9 mL | RÂ² (follow-up only): 0.9189\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9635\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -2.533 Â± 4.409\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 37.3%  | Â±2Ïƒ: 62.7%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 76.2 mL (n=66)\n >48w: 114.1 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=140.94471626047317  RÂ²=0.9439954492981656  n=41\n 24w: MAE=141.716757736708  RÂ²=0.9171170956672211  n=19\n 48w: MAE=None  RÂ²=None  n=1\nEpoch 8/8 - Train Loss: 7.202430\n\nSummary (over all observed visits in validation):\nModel    MAE: 144.1 mL | RMSE: 211.8 mL | RÂ² (incl base): 0.9275 | Laplace LL: -7.305686\nModel    MAE (follow-up only): 162.8 mL | RÂ² (follow-up only): 0.9184\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9643\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -2.160 Â± 4.226\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 39.9%  | Â±2Ïƒ: 62.7%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 76.1 mL (n=66)\n >48w: 114.0 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=137.7745571732521  RÂ²=0.9435492292103395  n=41\n 24w: MAE=157.26991012849305  RÂ²=0.9182935550703961  n=19\n 48w: MAE=None  RÂ²=None  n=1\n\nSummary (over all observed visits in validation):\nModel    MAE: 131.6 mL | RMSE: 197.9 mL | RÂ² (incl base): 0.9367 | Laplace LL: -7.097783\nModel    MAE (follow-up only): 148.7 mL | RÂ² (follow-up only): 0.9288\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9679\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.383 Â± 2.734\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 43.4%  | Â±2Ïƒ: 68.0%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 75.5 mL (n=66)\n >48w: 112.8 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=132.87863406320898  RÂ²=0.9458321561679471  n=41\n 24w: MAE=137.18603450373598  RÂ²=0.9231742404439789  n=19\n 48w: MAE=None  RÂ²=None  n=1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nTraining tabular-only baseline (predict slope) on train+val...\n Tab-only epoch 2/12 avg L1 loss: 4.6482\n Tab-only epoch 4/12 avg L1 loss: 4.3691\n Tab-only epoch 6/12 avg L1 loss: 4.2509\n Tab-only epoch 8/12 avg L1 loss: 4.2001\n Tab-only epoch 10/12 avg L1 loss: 4.1552\n Tab-only epoch 12/12 avg L1 loss: 4.1484\n\nEvaluating tabular-only model on test set...\n\nSummary (over all observed visits in validation):\nModel    MAE: 130.5 mL | RMSE: 197.5 mL | RÂ² (incl base): 0.9369 | Laplace LL: -7.081927\nModel    MAE (follow-up only): 147.5 mL | RÂ² (follow-up only): 0.9290\nBaseline MAE: 141.5 mL | RMSE: 218.3 mL | RÂ²: 0.9230 | Laplace LL: -7.453545\nLinEx MAE: 705.9 mL | RMSE: 1478.8 mL | RÂ²: -2.5360 | Laplace LL: -18.856145\nPearson Correlation (model): 0.9680\n\nPer-patient RÂ² (summary over patients with >1 observation):\n Model RÂ² meanÂ±std: -1.077 Â± 1.907\n Baseline RÂ² meanÂ±std: -0.898 Â± 0.883\n\nUncertainty coverage: Â±1Ïƒ: 43.4%  | Â±2Ïƒ: 67.7%\nAverage sigma by horizon bins:\n <=12w: 70.0 mL (n=174)\n 13-24w: 70.0 mL (n=45)\n 25-48w: 75.4 mL (n=66)\n >48w: 112.7 mL (n=31)\n\nPer-horizon metrics (Â±2w):\n 12w: MAE=130.97492655893652  RÂ²=0.9461100482907462  n=41\n 24w: MAE=134.33172262342353  RÂ²=0.9251553728949569  n=19\n 48w: MAE=None  RÂ²=None  n=1\n\nFinal test summary:\nImage+Tab MAE: 131.6 mL | RÂ² (follow-up): 0.929 | Laplace LL: -7.098\nTabular-only MAE: 130.5 mL | Baseline-only MAE: 141.5 mL | LinEx MAE: 705.9 mL\n\nDone.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom PIL import Image\nimport json\nfrom pathlib import Path\nimport joblib\nimport warnings\nimport pickle\nfrom typing import Dict, List, Tuple, Optional\nfrom scipy import ndimage\nfrom scipy.ndimage import binary_fill_holes, generate_binary_structure\nfrom skimage import measure, morphology\nfrom skimage.transform import resize\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport scipy.stats as stats\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    \"\"\"Ensure reproducibility across all random operations\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"ğŸš€ Fixed OSIC Model - Complete Working Version\")\nprint(\"=\" * 60)\nprint(f\"ğŸ“± Device: {DEVICE}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name()}\")\n    print(f\"ğŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(\"=\" * 60)\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_tab_features(df_row):\n    \"\"\"Extract tabular features (returns 4 features)\"\"\"\n    vector = [(df_row['Age'] - 30) / 30] \n    \n    # Sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(0)\n    else:\n        vector.append(1)\n    \n    # Smoking status encoding\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([1, 1])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 1])\n    else:\n        vector.extend([1, 0])\n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"\n    Calculate Log Laplace Likelihood (LLL) metric\n    LLL = -sqrt(2)*|actual - predicted|/sigma - log(sigma*sqrt(2))\n    \"\"\"\n    sigma_clipped = np.maximum(sigma, 70)  # Clip sigma to avoid division by very small values\n    delta = np.abs(actual - predicted)\n    term1 = -np.sqrt(2) * delta / sigma_clipped\n    term2 = -np.log(sigma_clipped * np.sqrt(2))\n    return term1 + term2\n\n# Calculate linear decay coefficients for each patient\nA = {} \nTAB = {} \nP = [] \n\nprint(\"Calculating linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy()\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, b = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n            TAB[patient] = get_tab_features(sub.iloc[0])\n            P.append(patient)\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0]) if len(weeks) > 1 else 0.0\n            TAB[patient] = get_tab_features(sub.iloc[0])\n            P.append(patient)\n    else:\n        A[patient] = 0.0\n        TAB[patient] = get_tab_features(sub.iloc[0])\n        P.append(patient)\n\nprint(f\"Processed {len(P)} patients with decay coefficients\")\n\nclass MedicalAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=15, p=0.7),\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                albu.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        x_cat = self.conv1(x_cat)\n        return x * self.sigmoid(x_cat)\n\nclass WorkingDenseNetModel(nn.Module):\n    \"\"\"\n    Working model with proper dimension matching\n    \"\"\"\n    \n    def __init__(self, tabular_dim=4, dropout_rate=0.4):\n        super(WorkingDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Spatial attention\n        self.spatial_attention = SpatialAttention()\n        \n        # Enhanced tabular processing\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU()\n        )\n        \n        # Cross-modal attention\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=1024, num_heads=8, dropout=0.2, batch_first=True\n        )\n        \n        # Initialize projection weight properly\n        self.tab_projection = nn.Linear(512, 1024)\n        \n        # Multi-modal fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 512, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate/2)\n        )\n        \n        # Uncertainty quantification heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 1)\n        )\n        \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        \n        # Apply spatial attention\n        img_features = self.spatial_attention(img_features)\n        \n        # Global average pooling\n        img_features = F.adaptive_avg_pool2d(img_features, (1, 1))\n        img_features = img_features.view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Cross-modal attention\n        img_expanded = img_features.unsqueeze(1)\n        tab_expanded = tab_features.unsqueeze(1)\n        \n        # Project tabular to same dimension for attention\n        tab_proj = self.tab_projection(tab_expanded)\n        \n        attended_img, _ = self.cross_attention(\n            img_expanded, tab_proj, tab_proj\n        )\n        attended_img = attended_img.squeeze(1)\n        \n        # Fusion\n        combined_features = torch.cat([attended_img, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OSICDenseNetDataset(Dataset):\n    \"\"\"Enhanced dataset with medical augmentations and robust loading\"\"\"\n    \n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True):\n        # Filter out problematic patients\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augment = augment\n        self.augmentor = MedicalAugmentation(augment=augment)\n        \n        # Prepare image paths for each patient\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        # Filter patients with available images\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 6\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image for this patient\n        available_images = self.patient_images[patient]\n        if len(available_images) > 1:\n            selected_image = np.random.choice(available_images)\n        else:\n            selected_image = available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_and_preprocess_dicom(selected_image)\n        \n        # Apply augmentations\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (decay coefficient)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_and_preprocess_dicom(self, path):\n        \"\"\"Enhanced DICOM loading with better preprocessing\"\"\"\n        try:\n            # Load DICOM\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            # Handle different DICOM formats\n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            # Resize to target size\n            img = cv2.resize(img, (512, 512))\n            \n            # Normalize to 0-255 range\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\nclass SimpleTrainer:\n    \"\"\"\n    Simple trainer that works with any model structure\n    \"\"\"\n    \n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_mae = float('inf')\n        self.best_val_r2 = -float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets, reduction='mean'):\n        \"\"\"Uncertainty-aware loss function\"\"\"\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        loss = 0.5 * (mse_loss / var + log_var)\n        \n        if reduction == 'mean':\n            return loss.mean()\n        return loss.sum()\n        \n    def train(self, train_loader, val_loader, epochs=30, patience=8):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='min', factor=0.5, patience=4, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training phase\n            self.model.train()\n            train_loss = 0.0\n            train_mae = 0.0\n            train_batches = 0\n            \n            for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n                try:\n                    images = images.to(self.device)\n                    tabular = tabular.to(self.device) \n                    targets = targets.to(self.device)\n                    \n                    optimizer.zero_grad()\n                    \n                    # Forward pass\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Calculate loss\n                    loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                    mae = F.l1_loss(mean_pred, targets)\n                    \n                    # Backward pass\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                    optimizer.step()\n                    \n                    train_loss += loss.item()\n                    train_mae += mae.item()\n                    train_batches += 1\n                    \n                except Exception as e:\n                    print(f\"Error in training batch {batch_idx}: {e}\")\n                    continue\n            \n            # Validation phase\n            self.model.eval()\n            val_loss = 0.0\n            val_mae = 0.0\n            val_predictions = []\n            val_targets = []\n            val_log_vars = []\n            \n            with torch.no_grad():\n                for batch_idx, (images, tabular, targets, _) in enumerate(val_loader):\n                    try:\n                        images = images.to(self.device)\n                        tabular = tabular.to(self.device)\n                        targets = targets.to(self.device)\n                        \n                        mean_pred, log_var = self.model(images, tabular)\n                        \n                        loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                        mae = F.l1_loss(mean_pred, targets)\n                        \n                        val_loss += loss.item()\n                        val_mae += mae.item()\n                        \n                        val_predictions.extend(mean_pred.cpu().numpy())\n                        val_targets.extend(targets.cpu().numpy())\n                        val_log_vars.extend(log_var.cpu().numpy())\n                        \n                    except Exception as e:\n                        print(f\"Error in validation batch {batch_idx}: {e}\")\n                        continue\n            \n            # Calculate metrics\n            if train_batches > 0 and len(val_predictions) > 0:\n                avg_train_loss = train_loss / train_batches\n                avg_train_mae = train_mae / train_batches\n                avg_val_loss = val_loss / len(val_loader)\n                avg_val_mae = val_mae / len(val_loader)\n                \n                # Convert to numpy arrays for metric calculation\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)  # Convert log variance to standard deviation\n                \n                # Calculate RÂ² score\n                r2 = r2_score(val_target_np, val_pred_np)\n                \n                # Calculate LLL (Log Laplace Likelihood)\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                print(f\"Epoch {epoch+1}/{epochs}\")\n                print(f\"Train Loss: {avg_train_loss:.6f}, MAE: {avg_train_mae:.6f}\")\n                print(f\"Val Loss: {avg_val_loss:.6f}, MAE: {avg_val_mae:.6f}\")\n                print(f\"Val RÂ²: {r2:.6f}, Val LLL: {avg_lll:.6f}\")\n                \n                # Learning rate scheduling\n                scheduler.step(avg_val_mae)\n                \n                # Early stopping and model saving\n                if avg_val_mae < self.best_val_mae:\n                    self.best_val_mae = avg_val_mae\n                    self.best_val_r2 = r2\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'best_working_model.pth')\n                    print(\"âœ… New best model saved!\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    \n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                    \n                print(\"-\" * 60)\n        \n        print(f\"ğŸ¯ Training completed!\")\n        print(f\"Best validation MAE: {self.best_val_mae:.6f}\")\n        print(f\"Best validation RÂ²: {self.best_val_r2:.6f}\")\n        print(f\"Best validation LLL: {self.best_val_lll:.6f}\")\n        \n        return self.best_val_mae, self.best_val_r2, self.best_val_lll\n\n# Main execution\ndef main():\n    print(\"ğŸ”„ Creating data loaders...\")\n    \n    # Split patients into train and validation (80-20 split)\n    patients_list = list(P)\n    train_patients, val_patients = train_test_split(\n        patients_list, \n        test_size=0.2, \n        random_state=42,\n        shuffle=True\n    )\n    \n    print(f\"Total patients: {len(patients_list)}\")\n    print(f\"Train patients: {len(train_patients)}\")\n    print(f\"Validation patients: {len(val_patients)}\")\n    \n    # Create datasets using only TRAIN_DIR\n    train_dataset = OSICDenseNetDataset(\n        patients=train_patients,\n        A_dict=A,\n        TAB_dict=TAB,\n        data_dir=TRAIN_DIR,\n        split='train',\n        augment=True\n    )\n    \n    val_dataset = OSICDenseNetDataset(\n        patients=val_patients,\n        A_dict=A,\n        TAB_dict=TAB,\n        data_dir=TRAIN_DIR,\n        split='val',\n        augment=False\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=8,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=8,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n        drop_last=False\n    )\n    \n    print(f\"âœ… Data loaders created!\")\n    print(f\"   Train batches: {len(train_loader)}\")\n    print(f\"   Val batches: {len(val_loader)}\")\n    \n    # Initialize model\n    print(\"ğŸ”„ Initializing model...\")\n    model = WorkingDenseNetModel(tabular_dim=4).to(DEVICE)\n    print(f\"âœ… Model initialized!\")\n    print(f\"ğŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test model with actual batch\n    try:\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images = images.to(DEVICE)\n        tabular = tabular.to(DEVICE)\n        \n        print(f\"ğŸ” Testing model...\")\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n            print(f\"âœ… Model forward pass successful!\")\n            \n    except Exception as e:\n        print(f\"âŒ Model test failed: {e}\")\n        return\n    \n    # Create trainer and start training\n    print(\"ğŸš€ Starting training...\")\n    trainer = SimpleTrainer(model, DEVICE, lr=1e-4)\n    \n    best_val_mae, best_val_r2, best_val_lll = trainer.train(\n        train_loader, \n        val_loader, \n        epochs=30,\n        patience=8\n    )\n    \n    print(f\"\\nğŸ¯ Final Results:\")\n    print(f\"Best validation MAE: {best_val_mae:.6f}\")\n    print(f\"Best validation RÂ²: {best_val_r2:.6f}\")\n    print(f\"Best validation LLL: {best_val_lll:.6f}\")\n    \n    return model, train_loader, val_loader, best_val_mae, best_val_r2, best_val_lll\n\nif __name__ == \"__main__\":\n    model, train_loader, val_loader, best_val_mae, best_val_r2, best_val_lll = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T08:38:11.754753Z","iopub.execute_input":"2025-10-12T08:38:11.755499Z","iopub.status.idle":"2025-10-12T08:45:47.230160Z","shell.execute_reply.started":"2025-10-12T08:38:11.755468Z","shell.execute_reply":"2025-10-12T08:45:47.229388Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Fixed OSIC Model - Complete Working Version\n============================================================\nğŸ“± Device: cuda\nğŸ”¥ GPU: Tesla P100-PCIE-16GB\nğŸ’¾ Memory: 17.1 GB\n============================================================\nLoaded dataset with shape: (1549, 7)\nCalculating linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 1461.61it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with decay coefficients\nğŸ”„ Creating data loaders...\nTotal patients: 176\nTrain patients: 140\nValidation patients: 36\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 138 patients with images\nDataset val: 36 patients with images\nâœ… Data loaders created!\n   Train batches: 103\n   Val batches: 5\nğŸ”„ Initializing model...\nâœ… Model initialized!\nğŸ“Š Total parameters: 13,281,764\nğŸ” Testing model...\nâœ… Model forward pass successful!\nğŸš€ Starting training...\nEpoch 1/30\nTrain Loss: 12.334829, MAE: 5.045496\nVal Loss: 9.274148, MAE: 5.783213\nVal RÂ²: -0.294617, Val LLL: -4.712226\nâœ… New best model saved!\n------------------------------------------------------------\nEpoch 2/30\nTrain Loss: 3.068125, MAE: 4.328621\nVal Loss: 3.633681, MAE: 5.190477\nVal RÂ²: -0.079952, Val LLL: -4.701132\nâœ… New best model saved!\n------------------------------------------------------------\nEpoch 3/30\nTrain Loss: 2.360442, MAE: 4.077540\nVal Loss: 3.795365, MAE: 5.100979\nVal RÂ²: -0.071357, Val LLL: -4.699964\nâœ… New best model saved!\n------------------------------------------------------------\nEpoch 4/30\nTrain Loss: 2.315327, MAE: 4.053044\nVal Loss: 3.414498, MAE: 5.049111\nVal RÂ²: -0.036320, Val LLL: -4.699051\nâœ… New best model saved!\n------------------------------------------------------------\nEpoch 5/30\nTrain Loss: 2.430036, MAE: 4.069858\nVal Loss: 4.383420, MAE: 5.115722\nVal RÂ²: -0.065987, Val LLL: -4.700410\n------------------------------------------------------------\nEpoch 6/30\nTrain Loss: 2.305119, MAE: 4.054875\nVal Loss: 3.339587, MAE: 5.000249\nVal RÂ²: -0.041449, Val LLL: -4.698298\nâœ… New best model saved!\n------------------------------------------------------------\nEpoch 7/30\nTrain Loss: 2.272217, MAE: 4.029190\nVal Loss: 3.498730, MAE: 5.042814\nVal RÂ²: -0.052748, Val LLL: -4.698875\n------------------------------------------------------------\nEpoch 8/30\nTrain Loss: 2.335133, MAE: 4.009451\nVal Loss: 3.454934, MAE: 5.083260\nVal RÂ²: -0.076839, Val LLL: -4.699958\n------------------------------------------------------------\nEpoch 9/30\nTrain Loss: 2.315600, MAE: 4.021015\nVal Loss: 3.226096, MAE: 4.919384\nVal RÂ²: -0.017679, Val LLL: -4.696798\nâœ… New best model saved!\n------------------------------------------------------------\nEpoch 10/30\nTrain Loss: 2.378862, MAE: 4.065500\nVal Loss: 3.259205, MAE: 4.949033\nVal RÂ²: -0.034594, Val LLL: -4.697855\n------------------------------------------------------------\nEpoch 11/30\nTrain Loss: 2.272877, MAE: 3.958443\nVal Loss: 4.174737, MAE: 5.188224\nVal RÂ²: -0.097966, Val LLL: -4.702150\n------------------------------------------------------------\nEpoch 12/30\nTrain Loss: 2.293531, MAE: 4.025678\nVal Loss: 3.426500, MAE: 5.089371\nVal RÂ²: -0.074713, Val LLL: -4.700689\n------------------------------------------------------------\nEpoch 13/30\nTrain Loss: 2.285124, MAE: 3.984279\nVal Loss: 3.411626, MAE: 5.097248\nVal RÂ²: -0.070070, Val LLL: -4.700353\n------------------------------------------------------------\nEpoch 14/30\nTrain Loss: 2.294331, MAE: 4.023368\nVal Loss: 3.297719, MAE: 5.031914\nVal RÂ²: -0.042691, Val LLL: -4.699765\n------------------------------------------------------------\nEpoch 15/30\nTrain Loss: 2.240074, MAE: 3.969477\nVal Loss: 3.042585, MAE: 4.993730\nVal RÂ²: -0.035272, Val LLL: -4.698991\n------------------------------------------------------------\nEpoch 16/30\nTrain Loss: 2.240943, MAE: 3.966127\nVal Loss: 3.362078, MAE: 5.021798\nVal RÂ²: -0.050934, Val LLL: -4.699355\n------------------------------------------------------------\nEpoch 17/30\nTrain Loss: 2.268350, MAE: 3.967457\nVal Loss: 3.178765, MAE: 4.977905\nVal RÂ²: -0.030040, Val LLL: -4.698555\nEarly stopping at epoch 17\nğŸ¯ Training completed!\nBest validation MAE: 4.919384\nBest validation RÂ²: -0.017679\nBest validation LLL: -4.696798\n\nğŸ¯ Final Results:\nBest validation MAE: 4.919384\nBest validation RÂ²: -0.017679\nBest validation LLL: -4.696798\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**MAE and LLL - OKAY Let's work on LLL**","metadata":{}},{"cell_type":"code","source":"# Lets do R^2 for FCV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T09:07:06.518152Z","iopub.execute_input":"2025-10-12T09:07:06.518855Z","iopub.status.idle":"2025-10-12T09:07:06.522329Z","shell.execute_reply.started":"2025-10-12T09:07:06.518826Z","shell.execute_reply":"2025-10-12T09:07:06.521740Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Fixed a major leak","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T09:42:02.266500Z","iopub.execute_input":"2025-10-12T09:42:02.266753Z","iopub.status.idle":"2025-10-12T09:42:02.270491Z","shell.execute_reply.started":"2025-10-12T09:42:02.266727Z","shell.execute_reply":"2025-10-12T09:42:02.269899Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"**Test 2 SEEK**","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom PIL import Image\nimport json\nfrom pathlib import Path\nimport joblib\nimport warnings\nimport pickle\nfrom typing import Dict, List, Tuple, Optional\nfrom scipy import ndimage\nfrom scipy.ndimage import binary_fill_holes, generate_binary_structure\nfrom skimage import measure, morphology\nfrom skimage.transform import resize\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport scipy.stats as stats\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    \"\"\"Ensure reproducibility across all random operations\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"ğŸš€ Enhanced OSIC Model - Progressive Training for Better RÂ²\")\nprint(\"=\" * 60)\nprint(f\"ğŸ“± Device: {DEVICE}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name()}\")\n    print(f\"ğŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(\"=\" * 60)\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_tab_features(df_row):\n    \"\"\"Extract tabular features (returns 4 features)\"\"\"\n    vector = [(df_row['Age'] - 30) / 30] \n    \n    # Sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(0)\n    else:\n        vector.append(1)\n    \n    # Smoking status encoding\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([1, 1])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 1])\n    else:\n        vector.extend([1, 0])\n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"\n    Calculate Log Laplace Likelihood (LLL) metric\n    LLL = -sqrt(2)*|actual - predicted|/sigma - log(sigma*sqrt(2))\n    \"\"\"\n    sigma_clipped = np.maximum(sigma, 70)  # Clip sigma to avoid division by very small values\n    delta = np.abs(actual - predicted)\n    term1 = -np.sqrt(2) * delta / sigma_clipped\n    term2 = -np.log(sigma_clipped * np.sqrt(2))\n    return term1 + term2\n\n# Calculate linear decay coefficients for each patient\nA = {} \nTAB = {} \nP = [] \n\nprint(\"Calculating linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy()\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, b = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n            TAB[patient] = get_tab_features(sub.iloc[0])\n            P.append(patient)\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0]) if len(weeks) > 1 else 0.0\n            TAB[patient] = get_tab_features(sub.iloc[0])\n            P.append(patient)\n    else:\n        A[patient] = 0.0\n        TAB[patient] = get_tab_features(sub.iloc[0])\n        P.append(patient)\n\nprint(f\"Processed {len(P)} patients with decay coefficients\")\n\nclass EnhancedMedicalAugmentation:\n    def __init__(self, augment=True, strength=0.7):\n        if augment:\n            self.transform = albu.Compose([\n                # Geometric transformations\n                albu.Rotate(limit=20, p=0.7 * strength),\n                albu.HorizontalFlip(p=0.5 * strength),\n                albu.VerticalFlip(p=0.3 * strength),\n                albu.ShiftScaleRotate(\n                    shift_limit=0.1 * strength, \n                    scale_limit=0.2 * strength, \n                    rotate_limit=20 * strength, \n                    p=0.8 * strength\n                ),\n                albu.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3 * strength),\n                \n                # Intensity transformations\n                albu.RandomBrightnessContrast(\n                    brightness_limit=0.3 * strength, \n                    contrast_limit=0.3 * strength, \n                    p=0.7 * strength\n                ),\n                albu.GaussNoise(var_limit=(10.0, 50.0), p=0.5 * strength),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5 * strength),\n                albu.CLAHE(clip_limit=4.0, p=0.3 * strength),\n                \n                # Advanced augmentations\n                albu.GridDistortion(num_steps=5, distort_limit=0.3 * strength, p=0.3 * strength),\n                albu.MotionBlur(blur_limit=7, p=0.2 * strength),\n                albu.MedianBlur(blur_limit=5, p=0.2 * strength),\n                albu.CoarseDropout(\n                    max_holes=8, \n                    max_height=32, \n                    max_width=32, \n                    min_holes=1,\n                    fill_value=0,\n                    p=0.3 * strength\n                ),\n                \n                # Normalization\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        x_cat = self.conv1(x_cat)\n        return x * self.sigmoid(x_cat)\n\nclass EnhancedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=4, dropout_rate=0.3):\n        super(EnhancedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone with unfrozen later layers\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Unfreeze last 2 blocks for fine-tuning\n        for param in list(self.features.parameters())[-100:]:\n            param.requires_grad = True\n        \n        # Enhanced spatial attention\n        self.spatial_attention = SpatialAttention(kernel_size=9)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(1024, 1024 // 16, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(1024 // 16, 1024, 1, bias=False),\n            nn.Sigmoid()\n        )\n        \n        # More sophisticated tabular processing\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU()\n        )\n        \n        # Multi-scale feature fusion\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        # Cross-modal fusion with residual connections\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 1024, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate/2),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU()\n        )\n        \n        # Output heads with better initialization\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain log variance\n        )\n        \n        # Initialize weights\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        \n        # Apply attention mechanisms\n        img_features = self.spatial_attention(img_features)\n        channel_weights = self.channel_attention(img_features)\n        img_features = img_features * channel_weights\n        \n        # Multi-scale pooling\n        avg_features = self.global_pool(img_features).view(batch_size, -1)\n        max_features = self.max_pool(img_features).view(batch_size, -1)\n        img_features = 0.6 * avg_features + 0.4 * max_features\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OSICDenseNetDataset(Dataset):\n    \"\"\"Enhanced dataset with medical augmentations and robust loading\"\"\"\n    \n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True, aug_strength=0.7):\n        # Filter out problematic patients\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augment = augment\n        self.augmentor = EnhancedMedicalAugmentation(augment=augment, strength=aug_strength)\n        \n        # Prepare image paths for each patient\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        # Filter patients with available images\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8  # More samples per patient\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image for this patient\n        available_images = self.patient_images[patient]\n        if len(available_images) > 1:\n            selected_image = np.random.choice(available_images)\n        else:\n            selected_image = available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_and_preprocess_dicom(selected_image)\n        \n        # Apply augmentations\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (decay coefficient)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_and_preprocess_dicom(self, path):\n        \"\"\"Enhanced DICOM loading with better preprocessing\"\"\"\n        try:\n            # Load DICOM\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            # Handle different DICOM formats\n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            # Resize to target size\n            img = cv2.resize(img, (512, 512))\n            \n            # Apply CLAHE for better contrast\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            if len(img.shape) == 2:\n                img = clahe.apply(img.astype(np.uint8))\n            \n            # Normalize to 0-255 range\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\nclass ProgressiveTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets, reduction='mean'):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        loss = 0.5 * (mse_loss / var + log_var)\n        return loss.mean() if reduction == 'mean' else loss.sum()\n    \n    def progressive_learning_schedule(self, epoch):\n        \"\"\"Progressive learning rate and augmentation schedule\"\"\"\n        if epoch < 10:\n            # Phase 1: Basic features\n            lr = self.lr\n            aug_strength = 0.3\n        elif epoch < 20:\n            # Phase 2: Intermediate\n            lr = self.lr * 0.5\n            aug_strength = 0.5\n        else:\n            # Phase 3: Fine-tuning\n            lr = self.lr * 0.1\n            aug_strength = 0.7\n        return lr, aug_strength\n    \n    def train(self, train_loader, val_loader, epochs=50, patience=15):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Progressive learning schedule\n            current_lr, aug_strength = self.progressive_learning_schedule(epoch)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = current_lr\n            \n            # Training phase\n            self.model.train()\n            train_loss = 0.0\n            train_mae = 0.0\n            train_batches = 0\n            \n            for batch_idx, (images, tabular, targets, _) in enumerate(train_loader):\n                try:\n                    images = images.to(self.device)\n                    tabular = tabular.to(self.device) \n                    targets = targets.to(self.device)\n                    \n                    optimizer.zero_grad()\n                    \n                    # Forward pass\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Combined loss with progressive weighting\n                    mse_loss = F.mse_loss(mean_pred, targets)\n                    uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                    mae_loss = F.l1_loss(mean_pred, targets)\n                    \n                    # Progressive loss weighting\n                    if epoch < 15:\n                        # Focus on MSE first\n                        loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                    else:\n                        # Then focus on uncertainty\n                        loss = 0.3 * mse_loss + 0.7 * uncertainty_loss + 0.1 * mae_loss\n                    \n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                    optimizer.step()\n                    \n                    train_loss += loss.item()\n                    train_mae += mae_loss.item()\n                    train_batches += 1\n                    \n                except Exception as e:\n                    continue\n            \n            # Validation phase\n            self.model.eval()\n            val_loss = 0.0\n            val_mae = 0.0\n            val_predictions = []\n            val_targets = []\n            val_log_vars = []\n            \n            with torch.no_grad():\n                for batch_idx, (images, tabular, targets, _) in enumerate(val_loader):\n                    try:\n                        images = images.to(self.device)\n                        tabular = tabular.to(self.device)\n                        targets = targets.to(self.device)\n                        \n                        mean_pred, log_var = self.model(images, tabular)\n                        \n                        loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                        mae = F.l1_loss(mean_pred, targets)\n                        \n                        val_loss += loss.item()\n                        val_mae += mae.item()\n                        \n                        val_predictions.extend(mean_pred.cpu().numpy())\n                        val_targets.extend(targets.cpu().numpy())\n                        val_log_vars.extend(log_var.cpu().numpy())\n                        \n                    except Exception as e:\n                        continue\n            \n            # Calculate metrics\n            if train_batches > 0 and len(val_predictions) > 0:\n                avg_train_loss = train_loss / train_batches\n                avg_train_mae = train_mae / train_batches\n                avg_val_loss = val_loss / len(val_loader)\n                avg_val_mae = val_mae / len(val_loader)\n                \n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate RÂ² score\n                r2 = r2_score(val_target_np, val_pred_np)\n                \n                # Calculate LLL\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                print(f\"Epoch {epoch+1}/{epochs} (LR: {current_lr:.2e})\")\n                print(f\"Train Loss: {avg_train_loss:.6f}, MAE: {avg_train_mae:.6f}\")\n                print(f\"Val Loss: {avg_val_loss:.6f}, MAE: {avg_val_mae:.6f}\")\n                print(f\"Val RÂ²: {r2:.6f}, Val LLL: {avg_lll:.6f}\")\n                \n                # Early stopping based on RÂ²\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = avg_val_mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'best_progressive_model.pth')\n                    print(f\"âœ… New best model! RÂ²: {r2:.6f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    \n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                    \n                print(\"-\" * 60)\n        \n        print(f\"ğŸ¯ Training completed!\")\n        print(f\"Best validation RÂ²: {self.best_val_r2:.6f}\")\n        print(f\"Best validation MAE: {self.best_val_mae:.6f}\")\n        print(f\"Best validation LLL: {self.best_val_lll:.6f}\")\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef enhanced_main():\n    print(\"ğŸ”„ Creating enhanced data loaders...\")\n    \n    # Use stratified split based on decay coefficient bins\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=5, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, \n        test_size=0.15,  # Smaller validation for more training data\n        random_state=42,\n        stratify=decay_bins,\n        shuffle=True\n    )\n    \n    print(f\"Total patients: {len(patients_list)}\")\n    print(f\"Train patients: {len(train_patients)}\")\n    print(f\"Validation patients: {len(val_patients)}\")\n    \n    # Create enhanced datasets\n    train_dataset = OSICDenseNetDataset(\n        patients=train_patients,\n        A_dict=A,\n        TAB_dict=TAB,\n        data_dir=TRAIN_DIR,\n        split='train',\n        augment=True,\n        aug_strength=0.7\n    )\n    \n    val_dataset = OSICDenseNetDataset(\n        patients=val_patients,\n        A_dict=A,\n        TAB_dict=TAB,\n        data_dir=TRAIN_DIR,\n        split='val',\n        augment=False\n    )\n    \n    # Larger batch size for better gradient estimation\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=16,  # Increased batch size\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=16,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=False\n    )\n    \n    print(f\"âœ… Enhanced data loaders created!\")\n    print(f\"   Train batches: {len(train_loader)}\")\n    print(f\"   Val batches: {len(val_loader)}\")\n    \n    # Initialize enhanced model\n    print(\"ğŸ”„ Initializing enhanced model...\")\n    model = EnhancedDenseNetModel(tabular_dim=4).to(DEVICE)\n    \n    # Count trainable parameters\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"âœ… Enhanced model initialized!\")\n    print(f\"ğŸ“Š Trainable parameters: {trainable_params:,}\")\n    \n    # Test model\n    try:\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images = images.to(DEVICE)\n        tabular = tabular.to(DEVICE)\n        \n        print(f\"ğŸ” Testing enhanced model...\")\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n            print(f\"âœ… Enhanced model forward pass successful!\")\n            \n    except Exception as e:\n        print(f\"âŒ Model test failed: {e}\")\n        return\n    \n    # Create progressive trainer\n    print(\"ğŸš€ Starting progressive training...\")\n    trainer = ProgressiveTrainer(model, DEVICE, lr=2e-4)  # Higher initial LR\n    \n    best_val_r2, best_val_mae, best_val_lll = trainer.train(\n        train_loader, \n        val_loader, \n        epochs=50,  # More epochs\n        patience=15  # More patience\n    )\n    \n    print(f\"\\nğŸ¯ Final Enhanced Results:\")\n    print(f\"Best validation RÂ²: {best_val_r2:.6f}\")\n    print(f\"Best validation MAE: {best_val_mae:.6f}\")\n    print(f\"Best validation LLL: {best_val_lll:.6f}\")\n    \n    return model, train_loader, val_loader, best_val_r2, best_val_mae, best_val_lll\n\n# Run the enhanced training\nif __name__ == \"__main__\":\n    model, train_loader, val_loader, best_val_r2, best_val_mae, best_val_lll = enhanced_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T09:15:22.560294Z","iopub.execute_input":"2025-10-12T09:15:22.560525Z","iopub.status.idle":"2025-10-12T09:42:02.263980Z","shell.execute_reply.started":"2025-10-12T09:15:22.560502Z","shell.execute_reply":"2025-10-12T09:42:02.263075Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Enhanced OSIC Model - Progressive Training for Better RÂ²\n============================================================\nğŸ“± Device: cuda\nğŸ”¥ GPU: Tesla P100-PCIE-16GB\nğŸ’¾ Memory: 17.1 GB\n============================================================\nLoaded dataset with shape: (1549, 7)\nCalculating linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 1496.41it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with decay coefficients\nğŸ”„ Creating enhanced data loaders...\nTotal patients: 176\nTrain patients: 149\nValidation patients: 27\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 147 patients with images\nDataset val: 27 patients with images\nâœ… Enhanced data loaders created!\n   Train batches: 73\n   Val batches: 2\nğŸ”„ Initializing enhanced model...\nâœ… Enhanced model initialized!\nğŸ“Š Trainable parameters: 10,554,212\nğŸ” Testing enhanced model...\nâœ… Enhanced model forward pass successful!\nğŸš€ Starting progressive training...\nEpoch 1/50 (LR: 2.00e-04)\nTrain Loss: 32.207168, MAE: 4.585047\nVal Loss: 8.584659, MAE: 4.478652\nVal RÂ²: -0.014335, Val LLL: -4.680817\nâœ… New best model! RÂ²: -0.014335\n------------------------------------------------------------\nEpoch 2/50 (LR: 2.00e-04)\nTrain Loss: 26.426207, MAE: 4.318165\nVal Loss: 7.594804, MAE: 4.523350\nVal RÂ²: 0.049396, Val LLL: -4.682032\nâœ… New best model! RÂ²: 0.049396\n------------------------------------------------------------\nEpoch 3/50 (LR: 2.00e-04)\nTrain Loss: 25.982936, MAE: 4.266466\nVal Loss: 7.444829, MAE: 4.723609\nVal RÂ²: 0.042403, Val LLL: -4.686941\n------------------------------------------------------------\nEpoch 4/50 (LR: 2.00e-04)\nTrain Loss: 26.301538, MAE: 4.303831\nVal Loss: 6.761354, MAE: 4.623678\nVal RÂ²: 0.119863, Val LLL: -4.685058\nâœ… New best model! RÂ²: 0.119863\n------------------------------------------------------------\nEpoch 5/50 (LR: 2.00e-04)\nTrain Loss: 25.322716, MAE: 4.222059\nVal Loss: 7.006438, MAE: 4.468774\nVal RÂ²: 0.094530, Val LLL: -4.682731\n------------------------------------------------------------\nEpoch 6/50 (LR: 2.00e-04)\nTrain Loss: 25.432432, MAE: 4.248018\nVal Loss: 7.651228, MAE: 4.491844\nVal RÂ²: 0.029926, Val LLL: -4.682508\n------------------------------------------------------------\nEpoch 7/50 (LR: 2.00e-04)\nTrain Loss: 24.707727, MAE: 4.229245\nVal Loss: 7.288878, MAE: 4.449962\nVal RÂ²: 0.040924, Val LLL: -4.683352\n------------------------------------------------------------\nEpoch 8/50 (LR: 2.00e-04)\nTrain Loss: 25.083523, MAE: 4.215558\nVal Loss: 7.234673, MAE: 4.428091\nVal RÂ²: 0.041120, Val LLL: -4.683355\n------------------------------------------------------------\nEpoch 9/50 (LR: 2.00e-04)\nTrain Loss: 25.376268, MAE: 4.201496\nVal Loss: 7.025794, MAE: 4.348987\nVal RÂ²: 0.060236, Val LLL: -4.682203\n------------------------------------------------------------\nEpoch 10/50 (LR: 2.00e-04)\nTrain Loss: 24.791014, MAE: 4.196701\nVal Loss: 6.529566, MAE: 4.310716\nVal RÂ²: 0.144107, Val LLL: -4.680103\nâœ… New best model! RÂ²: 0.144107\n------------------------------------------------------------\nEpoch 11/50 (LR: 1.00e-04)\nTrain Loss: 24.831103, MAE: 4.209662\nVal Loss: 6.476966, MAE: 4.165816\nVal RÂ²: 0.135187, Val LLL: -4.678639\n------------------------------------------------------------\nEpoch 12/50 (LR: 1.00e-04)\nTrain Loss: 24.223610, MAE: 4.142148\nVal Loss: 6.748641, MAE: 4.283029\nVal RÂ²: 0.078617, Val LLL: -4.681757\n------------------------------------------------------------\nEpoch 13/50 (LR: 1.00e-04)\nTrain Loss: 24.182470, MAE: 4.159576\nVal Loss: 7.257588, MAE: 4.374796\nVal RÂ²: 0.033044, Val LLL: -4.682477\n------------------------------------------------------------\nEpoch 14/50 (LR: 1.00e-04)\nTrain Loss: 24.583916, MAE: 4.182923\nVal Loss: 6.629539, MAE: 4.501631\nVal RÂ²: 0.133234, Val LLL: -4.682414\n------------------------------------------------------------\nEpoch 15/50 (LR: 1.00e-04)\nTrain Loss: 24.099657, MAE: 4.091335\nVal Loss: 6.852094, MAE: 4.611240\nVal RÂ²: 0.077108, Val LLL: -4.687372\n------------------------------------------------------------\nEpoch 16/50 (LR: 1.00e-04)\nTrain Loss: 14.180355, MAE: 4.154723\nVal Loss: 6.849434, MAE: 4.499837\nVal RÂ²: 0.076597, Val LLL: -4.685309\n------------------------------------------------------------\nEpoch 17/50 (LR: 1.00e-04)\nTrain Loss: 14.222088, MAE: 4.168675\nVal Loss: 6.565868, MAE: 4.027783\nVal RÂ²: 0.158973, Val LLL: -4.674009\nâœ… New best model! RÂ²: 0.158973\n------------------------------------------------------------\nEpoch 18/50 (LR: 1.00e-04)\nTrain Loss: 14.203200, MAE: 4.103195\nVal Loss: 7.705445, MAE: 4.606355\nVal RÂ²: -0.057197, Val LLL: -4.688570\n------------------------------------------------------------\nEpoch 19/50 (LR: 1.00e-04)\nTrain Loss: 14.440356, MAE: 4.174750\nVal Loss: 6.415570, MAE: 4.238611\nVal RÂ²: 0.119864, Val LLL: -4.680503\n------------------------------------------------------------\nEpoch 20/50 (LR: 1.00e-04)\nTrain Loss: 13.992080, MAE: 4.098919\nVal Loss: 7.044914, MAE: 4.286090\nVal RÂ²: 0.017108, Val LLL: -4.682750\n------------------------------------------------------------\nEpoch 21/50 (LR: 2.00e-05)\nTrain Loss: 14.005522, MAE: 4.099003\nVal Loss: 7.041556, MAE: 4.408188\nVal RÂ²: 0.022775, Val LLL: -4.684850\n------------------------------------------------------------\nEpoch 22/50 (LR: 2.00e-05)\nTrain Loss: 13.917062, MAE: 4.082290\nVal Loss: 7.002946, MAE: 4.248327\nVal RÂ²: 0.045257, Val LLL: -4.681186\n------------------------------------------------------------\nEpoch 23/50 (LR: 2.00e-05)\nTrain Loss: 13.317334, MAE: 4.016113\nVal Loss: 7.282808, MAE: 4.389804\nVal RÂ²: -0.009973, Val LLL: -4.684247\n------------------------------------------------------------\nEpoch 24/50 (LR: 2.00e-05)\nTrain Loss: 13.256938, MAE: 4.006633\nVal Loss: 6.864350, MAE: 4.342938\nVal RÂ²: 0.062229, Val LLL: -4.682596\n------------------------------------------------------------\nEpoch 25/50 (LR: 2.00e-05)\nTrain Loss: 13.553587, MAE: 4.089628\nVal Loss: 6.939281, MAE: 4.291776\nVal RÂ²: 0.054904, Val LLL: -4.681573\n------------------------------------------------------------\nEpoch 26/50 (LR: 2.00e-05)\nTrain Loss: 13.180377, MAE: 4.009624\nVal Loss: 7.001693, MAE: 4.302414\nVal RÂ²: 0.030113, Val LLL: -4.683463\n------------------------------------------------------------\nEpoch 27/50 (LR: 2.00e-05)\nTrain Loss: 12.567706, MAE: 3.898147\nVal Loss: 7.780433, MAE: 4.581681\nVal RÂ²: -0.073649, Val LLL: -4.687876\n------------------------------------------------------------\nEpoch 28/50 (LR: 2.00e-05)\nTrain Loss: 13.198943, MAE: 4.008196\nVal Loss: 7.446213, MAE: 4.432130\nVal RÂ²: -0.053837, Val LLL: -4.686157\n------------------------------------------------------------\nEpoch 29/50 (LR: 2.00e-05)\nTrain Loss: 13.193029, MAE: 3.982101\nVal Loss: 7.060120, MAE: 4.252721\nVal RÂ²: 0.011216, Val LLL: -4.682190\n------------------------------------------------------------\nEpoch 30/50 (LR: 2.00e-05)\nTrain Loss: 12.785887, MAE: 3.962539\nVal Loss: 7.461847, MAE: 4.451429\nVal RÂ²: -0.026807, Val LLL: -4.685648\n------------------------------------------------------------\nEpoch 31/50 (LR: 2.00e-05)\nTrain Loss: 12.820106, MAE: 3.979722\nVal Loss: 5.967449, MAE: 3.981828\nVal RÂ²: 0.182321, Val LLL: -4.676542\nâœ… New best model! RÂ²: 0.182321\n------------------------------------------------------------\nEpoch 32/50 (LR: 2.00e-05)\nTrain Loss: 12.244053, MAE: 3.907537\nVal Loss: 6.943875, MAE: 4.303650\nVal RÂ²: 0.025764, Val LLL: -4.683741\n------------------------------------------------------------\nEpoch 33/50 (LR: 2.00e-05)\nTrain Loss: 12.330012, MAE: 3.887222\nVal Loss: 7.493227, MAE: 4.543772\nVal RÂ²: -0.027039, Val LLL: -4.687101\n------------------------------------------------------------\nEpoch 34/50 (LR: 2.00e-05)\nTrain Loss: 13.069345, MAE: 4.023992\nVal Loss: 7.166337, MAE: 4.565108\nVal RÂ²: 0.015080, Val LLL: -4.687690\n------------------------------------------------------------\nEpoch 35/50 (LR: 2.00e-05)\nTrain Loss: 12.769868, MAE: 3.967588\nVal Loss: 7.729000, MAE: 4.565038\nVal RÂ²: -0.057006, Val LLL: -4.687961\n------------------------------------------------------------\nEpoch 36/50 (LR: 2.00e-05)\nTrain Loss: 12.159990, MAE: 3.855315\nVal Loss: 7.142991, MAE: 4.560716\nVal RÂ²: -0.015384, Val LLL: -4.688403\n------------------------------------------------------------\nEpoch 37/50 (LR: 2.00e-05)\nTrain Loss: 12.088355, MAE: 3.860973\nVal Loss: 7.868313, MAE: 4.650504\nVal RÂ²: -0.101763, Val LLL: -4.689682\n------------------------------------------------------------\nEpoch 38/50 (LR: 2.00e-05)\nTrain Loss: 11.806589, MAE: 3.819612\nVal Loss: 8.161565, MAE: 4.744646\nVal RÂ²: -0.102890, Val LLL: -4.690396\n------------------------------------------------------------\nEpoch 39/50 (LR: 2.00e-05)\nTrain Loss: 12.064530, MAE: 3.872275\nVal Loss: 7.747082, MAE: 4.683995\nVal RÂ²: -0.062647, Val LLL: -4.689595\n------------------------------------------------------------\nEpoch 40/50 (LR: 2.00e-05)\nTrain Loss: 12.241450, MAE: 3.893698\nVal Loss: 7.909811, MAE: 4.643974\nVal RÂ²: -0.083785, Val LLL: -4.689112\n------------------------------------------------------------\nEpoch 41/50 (LR: 2.00e-05)\nTrain Loss: 12.302766, MAE: 3.896044\nVal Loss: 7.484475, MAE: 4.392176\nVal RÂ²: -0.034919, Val LLL: -4.684537\n------------------------------------------------------------\nEpoch 42/50 (LR: 2.00e-05)\nTrain Loss: 12.601498, MAE: 3.907956\nVal Loss: 7.813330, MAE: 4.614581\nVal RÂ²: -0.084268, Val LLL: -4.688789\n------------------------------------------------------------\nEpoch 43/50 (LR: 2.00e-05)\nTrain Loss: 11.600397, MAE: 3.775597\nVal Loss: 7.040976, MAE: 4.375661\nVal RÂ²: 0.039062, Val LLL: -4.683457\n------------------------------------------------------------\nEpoch 44/50 (LR: 2.00e-05)\nTrain Loss: 11.736322, MAE: 3.794044\nVal Loss: 7.858691, MAE: 4.536894\nVal RÂ²: -0.081130, Val LLL: -4.686867\n------------------------------------------------------------\nEpoch 45/50 (LR: 2.00e-05)\nTrain Loss: 11.460124, MAE: 3.735076\nVal Loss: 6.988587, MAE: 4.386461\nVal RÂ²: 0.061127, Val LLL: -4.683392\n------------------------------------------------------------\nEpoch 46/50 (LR: 2.00e-05)\nTrain Loss: 11.591577, MAE: 3.813832\nVal Loss: 7.764606, MAE: 4.770281\nVal RÂ²: -0.067871, Val LLL: -4.691405\nEarly stopping at epoch 46\nğŸ¯ Training completed!\nBest validation RÂ²: 0.182321\nBest validation MAE: 3.981828\nBest validation LLL: -4.676542\n\nğŸ¯ Final Enhanced Results:\nBest validation RÂ²: 0.182321\nBest validation MAE: 3.981828\nBest validation LLL: -4.676542\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"ğŸš€ MEMORY-OPTIMIZED OSIC Model - Targeting RÂ² > 0.5\")\nprint(\"=\" * 60)\nprint(f\"ğŸ“± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_enhanced_tab_features(df_row):\n    \"\"\"Enhanced tabular features with more information\"\"\"\n    vector = []\n    \n    # Enhanced Age features\n    age = df_row['Age']\n    vector.extend([\n        (age - 30) / 30,  # Normalized age\n        (age - 50) ** 2 / 1000,  # Age squared\n        1 if age > 60 else 0,  # Senior flag\n    ])\n    \n    # Enhanced Sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.extend([1, 0])\n    else:\n        vector.extend([0, 1])\n    \n    # Enhanced Smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # Add baseline FVC information\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            (fvc - 2000) / 1000,\n            fvc / 1000,\n        ])\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma_clipped = np.maximum(sigma, 70)\n    delta = np.abs(actual - predicted)\n    term1 = -np.sqrt(2) * delta / sigma_clipped\n    term2 = -np.log(sigma_clipped * np.sqrt(2))\n    return term1 + term2\n\n# Enhanced coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating enhanced linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) > 2:\n        try:\n            c = np.vstack([weeks, np.ones(len(weeks))]).T\n            a, b = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            \n            fvc_pred = a * weeks + b\n            patient_r2 = 1 - np.sum((fvc - fvc_pred) ** 2) / np.sum((fvc - np.mean(fvc)) ** 2)\n            \n            if patient_r2 > 0.3:\n                A[patient] = a\n            else:\n                A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0]) if len(weeks) > 1 else 0.0\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0]) if len(weeks) > 1 else 0.0\n    else:\n        A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0]) if len(weeks) > 1 else 0.0\n    \n    TAB[patient] = get_enhanced_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with enhanced features\")\n\nclass MemoryEfficientAugmentation:\n    def __init__(self, augment=True, phase=1):\n        if augment:\n            if phase == 1:\n                self.transform = albu.Compose([\n                    albu.Rotate(limit=15, p=0.7),\n                    albu.HorizontalFlip(p=0.5),\n                    albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                    ToTensorV2()\n                ])\n            elif phase == 2:\n                self.transform = albu.Compose([\n                    albu.Rotate(limit=10, p=0.5),\n                    albu.HorizontalFlip(p=0.4),\n                    albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                    albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n                    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                    ToTensorV2()\n                ])\n            else:\n                self.transform = albu.Compose([\n                    albu.Rotate(limit=5, p=0.3),\n                    albu.HorizontalFlip(p=0.3),\n                    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                    ToTensorV2()\n                ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass EfficientDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(EfficientDenseNetModel, self).__init__()\n        \n        # Use DenseNet121 but with gradient checkpointing\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            if i > 100:  # Only unfreeze later layers\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n        \n        # Simple spatial attention to save memory\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(1024, 64, 1),\n            nn.ReLU(),\n            nn.Conv2d(64, 1, 1),\n            nn.Sigmoid()\n        )\n        \n        # Multi-scale pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        # Memory-efficient tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        \n        # Efficient fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 512, 512),  # Reduced dimensions\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()\n        )\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features with gradient checkpointing for memory efficiency\n        img_features = self.features(images)\n        \n        # Apply spatial attention\n        attention_weights = self.spatial_attention(img_features)\n        img_features = img_features * attention_weights\n        \n        # Multi-scale pooling\n        avg_features = self.global_pool(img_features).view(batch_size, -1)\n        max_features = self.max_pool(img_features).view(batch_size, -1)\n        img_features = 0.5 * avg_features + 0.5 * max_features\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass EfficientOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', phase=1):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = MemoryEfficientAugmentation(augment=(split=='train'), phase=phase)\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split} (Phase {phase}): {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 6  # Reduced samples\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image with smaller size\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            # Use smaller image size to save memory\n            img = cv2.resize(img, (384, 384))  # Reduced from 512x512\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass EfficientTrainer:\n    def __init__(self, model, device, lr=2e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train_phase(self, train_loader, val_loader, epochs, phase):\n        if phase == 1:\n            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=1e-6)\n        elif phase == 2:\n            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr/2, weight_decay=1e-5)\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=4)\n        else:\n            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr/4, weight_decay=1e-6)\n            scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training with gradient accumulation for effective larger batch size\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for i, (images, tabular, targets, _) in enumerate(train_loader):\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Phase-specific loss weighting\n                if phase == 1:\n                    loss = F.mse_loss(mean_pred, targets) * 0.7 + self.uncertainty_loss(mean_pred, log_var, targets) * 0.3\n                elif phase == 2:\n                    loss = F.mse_loss(mean_pred, targets) * 0.5 + self.uncertainty_loss(mean_pred, log_var, targets) * 0.5\n                else:\n                    loss = self.uncertainty_loss(mean_pred, log_var, targets) * 0.7 + F.l1_loss(mean_pred, targets) * 0.3\n                \n                loss.backward()\n                \n                # Gradient accumulation every 2 batches\n                if (i + 1) % 2 == 0:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                    optimizer.step()\n                    optimizer.zero_grad()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Step optimizer if there are remaining gradients\n            if len(train_loader) % 2 != 0:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n            \n            # Validation\n            self.model.eval()\n            val_predictions, val_targets = [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, _ = self.model(images, tabular)\n                    val_predictions.extend(mean_pred.cpu().numpy())\n                    val_targets.extend(targets.cpu().numpy())\n            \n            if len(val_predictions) > 0:\n                r2 = r2_score(val_targets, val_predictions)\n                mae = np.mean(np.abs(np.array(val_predictions) - np.array(val_targets)))\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                print(f\"Phase {phase}, Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, RÂ² = {r2:.4f}, MAE = {mae:.4f}\")\n                \n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    torch.save(self.model.state_dict(), f'best_phase{phase}_model.pth')\n                    print(f\"ğŸ¯ NEW BEST! RÂ²: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if phase == 1:\n                    scheduler.step()\n                elif phase == 2:\n                    scheduler.step(r2)\n                \n                if patience_counter >= 6:\n                    print(f\"Early stopping phase {phase}\")\n                    break\n        \n        return self.best_val_r2, self.best_val_mae\n\ndef efficient_main():\n    print(\"ğŸ”„ Creating memory-efficient data loaders...\")\n    \n    # Better stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # 3-Phase Training with memory optimization\n    best_overall_r2 = -float('inf')\n    best_overall_mae = float('inf')\n    \n    for phase in [1, 2, 3]:\n        print(f\"\\nğŸš€ STARTING PHASE {phase} TRAINING\")\n        \n        # Phase-specific datasets\n        train_dataset = EfficientOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train', phase)\n        val_dataset = EfficientOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val', phase)\n        \n        # Smaller batch sizes for memory efficiency\n        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n        \n        # Initialize model\n        if phase == 1:\n            model = EfficientDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n        else:\n            model = EfficientDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n            try:\n                model.load_state_dict(torch.load(f'best_phase{phase-1}_model.pth'))\n                print(f\"âœ… Loaded best model from phase {phase-1}\")\n            except:\n                print(f\"âš ï¸ Could not load phase {phase-1} model, starting fresh\")\n        \n        print(f\"ğŸ“Š Phase {phase} parameters: {sum(p.numel() for p in model.parameters()):,}\")\n        \n        # Test forward pass with memory monitoring\n        try:\n            # Clear memory\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            test_batch = next(iter(train_loader))\n            images, tabular, targets, _ = test_batch\n            images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n            \n            with torch.no_grad():\n                mean_pred, log_var = model(images, tabular)\n            \n            print(f\"âœ… Model forward pass successful!\")\n            print(f\"ğŸ’¾ GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            \n        except Exception as e:\n            print(f\"âŒ Model test failed: {e}\")\n            continue\n        \n        # Phase-specific training\n        trainer = EfficientTrainer(model, DEVICE, lr=2e-4)\n        phase_r2, phase_mae = trainer.train_phase(train_loader, val_loader, epochs=20, phase=phase)\n        \n        if phase_r2 > best_overall_r2:\n            best_overall_r2 = phase_r2\n            best_overall_mae = phase_mae\n        \n        # Clear memory between phases\n        del model, trainer, train_loader, val_loader\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        print(f\"ğŸ¯ Phase {phase} completed: RÂ² = {phase_r2:.4f}, MAE = {phase_mae:.4f}\")\n    \n    print(f\"\\nğŸ”¥ FINAL RESULTS:\")\n    print(f\"Best RÂ² = {best_overall_r2:.4f}\")\n    print(f\"Best MAE = {best_overall_mae:.4f}\")\n    \n    return best_overall_r2, best_overall_mae\n\nif __name__ == \"__main__\":\n    final_r2, final_mae = efficient_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T09:59:03.124917Z","iopub.execute_input":"2025-10-12T09:59:03.125272Z","iopub.status.idle":"2025-10-12T10:09:40.386066Z","shell.execute_reply.started":"2025-10-12T09:59:03.125244Z","shell.execute_reply":"2025-10-12T10:09:40.385199Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ MEMORY-OPTIMIZED OSIC Model - Targeting RÂ² > 0.5\n============================================================\nğŸ“± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating enhanced linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 1074.82it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with enhanced features\nğŸ”„ Creating memory-efficient data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 10\n\nğŸš€ STARTING PHASE 1 TRAINING\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train (Phase 1): 147 patients with images\nDataset val (Phase 1): 27 patients with images\nğŸ“Š Phase 1 parameters: 8,124,739\nâœ… Model forward pass successful!\nğŸ’¾ GPU memory allocated: 7.37 GB\nPhase 1, Epoch 1: Train Loss = 33.6420, RÂ² = 0.1139, MAE = 3.7355\nğŸ¯ NEW BEST! RÂ²: 0.1139\nPhase 1, Epoch 2: Train Loss = 27.6691, RÂ² = -0.1445, MAE = 4.2485\nPhase 1, Epoch 3: Train Loss = 27.1921, RÂ² = -0.0581, MAE = 4.0643\nPhase 1, Epoch 4: Train Loss = 27.6190, RÂ² = -0.0501, MAE = 3.9958\nPhase 1, Epoch 5: Train Loss = 27.1455, RÂ² = -0.1079, MAE = 4.6677\nPhase 1, Epoch 6: Train Loss = 27.8935, RÂ² = 0.0316, MAE = 4.1325\nPhase 1, Epoch 7: Train Loss = 27.1911, RÂ² = -0.0152, MAE = 4.1226\nEarly stopping phase 1\nğŸ¯ Phase 1 completed: RÂ² = 0.1139, MAE = 3.7355\n\nğŸš€ STARTING PHASE 2 TRAINING\nDataset train (Phase 2): 147 patients with images\nDataset val (Phase 2): 27 patients with images\nâœ… Loaded best model from phase 1\nğŸ“Š Phase 2 parameters: 8,124,739\nâœ… Model forward pass successful!\nğŸ’¾ GPU memory allocated: 7.40 GB\nPhase 2, Epoch 1: Train Loss = 21.4383, RÂ² = -0.0363, MAE = 4.2121\nğŸ¯ NEW BEST! RÂ²: -0.0363\nPhase 2, Epoch 2: Train Loss = 20.7308, RÂ² = -0.1287, MAE = 4.2307\nPhase 2, Epoch 3: Train Loss = 20.6526, RÂ² = -0.2213, MAE = 4.7369\nPhase 2, Epoch 4: Train Loss = 21.1543, RÂ² = -0.0308, MAE = 3.9549\nğŸ¯ NEW BEST! RÂ²: -0.0308\nPhase 2, Epoch 5: Train Loss = 21.1767, RÂ² = -0.0018, MAE = 4.2007\nğŸ¯ NEW BEST! RÂ²: -0.0018\nPhase 2, Epoch 6: Train Loss = 20.4431, RÂ² = -0.0377, MAE = 4.1274\nPhase 2, Epoch 7: Train Loss = 20.1636, RÂ² = -0.1138, MAE = 4.4203\nPhase 2, Epoch 8: Train Loss = 20.3243, RÂ² = 0.1351, MAE = 3.8645\nğŸ¯ NEW BEST! RÂ²: 0.1351\nPhase 2, Epoch 9: Train Loss = 20.3034, RÂ² = 0.1970, MAE = 3.8669\nğŸ¯ NEW BEST! RÂ²: 0.1970\nPhase 2, Epoch 10: Train Loss = 20.6604, RÂ² = 0.0116, MAE = 4.1554\nPhase 2, Epoch 11: Train Loss = 20.7023, RÂ² = -0.0052, MAE = 4.2428\nPhase 2, Epoch 12: Train Loss = 20.1388, RÂ² = -0.0560, MAE = 4.3779\nPhase 2, Epoch 13: Train Loss = 19.6584, RÂ² = 0.2042, MAE = 3.9807\nğŸ¯ NEW BEST! RÂ²: 0.2042\nPhase 2, Epoch 14: Train Loss = 19.3890, RÂ² = -0.0569, MAE = 4.5418\nPhase 2, Epoch 15: Train Loss = 20.0231, RÂ² = -0.0219, MAE = 4.3658\nPhase 2, Epoch 16: Train Loss = 19.6197, RÂ² = -0.1555, MAE = 4.3988\nPhase 2, Epoch 17: Train Loss = 19.0560, RÂ² = 0.1298, MAE = 3.8693\nPhase 2, Epoch 18: Train Loss = 19.4246, RÂ² = 0.0806, MAE = 4.1993\nPhase 2, Epoch 19: Train Loss = 18.6753, RÂ² = -0.0030, MAE = 4.0426\nEarly stopping phase 2\nğŸ¯ Phase 2 completed: RÂ² = 0.2042, MAE = 3.9807\n\nğŸš€ STARTING PHASE 3 TRAINING\nDataset train (Phase 3): 147 patients with images\nDataset val (Phase 3): 27 patients with images\nâœ… Loaded best model from phase 2\nğŸ“Š Phase 3 parameters: 8,124,739\nâœ… Model forward pass successful!\nğŸ’¾ GPU memory allocated: 7.37 GB\nPhase 3, Epoch 1: Train Loss = 5.6171, RÂ² = 0.0041, MAE = 4.1053\nğŸ¯ NEW BEST! RÂ²: 0.0041\nPhase 3, Epoch 2: Train Loss = 5.6934, RÂ² = 0.0896, MAE = 4.0745\nğŸ¯ NEW BEST! RÂ²: 0.0896\nPhase 3, Epoch 3: Train Loss = 5.5877, RÂ² = 0.1022, MAE = 4.0192\nğŸ¯ NEW BEST! RÂ²: 0.1022\nPhase 3, Epoch 4: Train Loss = 5.7115, RÂ² = 0.2424, MAE = 3.7909\nğŸ¯ NEW BEST! RÂ²: 0.2424\nPhase 3, Epoch 5: Train Loss = 5.5907, RÂ² = 0.0259, MAE = 4.1394\nPhase 3, Epoch 6: Train Loss = 5.7091, RÂ² = -0.0330, MAE = 4.2964\nPhase 3, Epoch 7: Train Loss = 5.5492, RÂ² = 0.0920, MAE = 4.1622\nPhase 3, Epoch 8: Train Loss = 5.3693, RÂ² = 0.1560, MAE = 3.9625\nPhase 3, Epoch 9: Train Loss = 5.4280, RÂ² = 0.1321, MAE = 3.9557\nPhase 3, Epoch 10: Train Loss = 5.1934, RÂ² = 0.1121, MAE = 3.6964\nEarly stopping phase 3\nğŸ¯ Phase 3 completed: RÂ² = 0.2424, MAE = 3.7909\n\nğŸ”¥ FINAL RESULTS:\nBest RÂ² = 0.2424\nBest MAE = 3.7909\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Aiming fo rbetter R^2","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\n# Configuration\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"ğŸš€ OPTIMIZED OSIC Model - Targeting RÂ² > 0.5\")\nprint(\"=\" * 60)\nprint(f\"ğŸ“± Device: {DEVICE}\")\n\n# Load Data\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    \"\"\"Optimized tabular features - simpler but more effective\"\"\"\n    vector = []\n    \n    # Basic but effective features\n    age = df_row['Age']\n    vector.extend([\n        (age - 50) / 30,  # Centered age\n        age / 100,  # Scaled age\n    ])\n    \n    # Simple sex encoding\n    if df_row['Sex'] == 'Male':\n        vector.append(1.0)\n    else:\n        vector.append(0.0)\n    \n    # Simple smoking status\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0, 0, 1])\n    else:\n        vector.extend([0, 0, 0])\n    \n    # FVC features\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,  # Normalized FVC\n            (fvc - 2500) / 1000,  # Centered FVC\n        ])\n    \n    # Percent predicted (approximate)\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        \n        # Approximate percent predicted FVC\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n            \n        vector.append(min(pp_fvc, 2.0))  # Cap at 200%\n    \n    return np.array(vector)\n\ndef calculate_lll(actual, predicted, sigma):\n    \"\"\"Calculate Log Laplace Likelihood\"\"\"\n    sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n\n# Improved coefficient calculation\nA = {} \nTAB = {} \nP = []\n\nprint(\"Calculating optimized linear decay coefficients...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    \n    if len(weeks) >= 2:\n        try:\n            # Simple robust slope calculation\n            if len(weeks) == 2:\n                slope = (fvc[1] - fvc[0]) / (weeks[1] - weeks[0])\n            else:\n                # Use Theil-Sen estimator for robustness\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i+1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slope = (fvc[j] - fvc[i]) / (weeks[j] - weeks[i])\n                            slopes.append(slope)\n                slope = np.median(slopes) if slopes else 0.0\n            \n            A[patient] = slope\n        except:\n            A[patient] = 0.0\n    else:\n        A[patient] = 0.0\n    \n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients with optimized features\")\n\n# Analyze target distribution\ndecay_values = np.array(list(A.values()))\nprint(f\"Target statistics: mean={decay_values.mean():.4f}, std={decay_values.std():.4f}\")\nprint(f\"Target range: [{decay_values.min():.4f}, {decay_values.max():.4f}]\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super(OptimizedDenseNetModel, self).__init__()\n        \n        # DenseNet121 backbone\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # Freeze early layers, unfreeze later layers\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100  # Only unfreeze later layers\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Simple but effective tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Feature fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        # Output heads\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()  # Constrain output\n        )\n        \n        # Initialize output layers for better convergence\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.mean_head, self.log_var_head]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        \n        # Extract image features\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(batch_size, -1)\n        \n        # Process tabular data\n        tab_features = self.tabular_processor(tabular)\n        \n        # Feature fusion\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        # Predict mean and log variance\n        mean_pred = self.mean_head(fused_features)\n        log_var = self.log_var_head(fused_features)\n        \n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        # Prepare image paths\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    \n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 8\n        else:\n            return len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        if self.split == 'train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n            \n        patient = self.valid_patients[patient_idx]\n        \n        # Get random image\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        \n        # Load and preprocess image\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        # Get tabular features\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        \n        # Get target (clipped to reasonable range)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        \n        return img_tensor, tab_features, target, patient\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            if len(img.shape) == 3:\n                img = img[img.shape[0]//2]\n            \n            img = cv2.resize(img, (384, 384))\n            \n            # Normalize\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            # Apply CLAHE\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            \n            # Convert to 3-channel\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            \n            return img\n            \n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384, 384, 3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n        \n    def uncertainty_loss(self, mean_pred, log_var, targets):\n        var = torch.exp(log_var)\n        mse_loss = (mean_pred - targets) ** 2\n        return 0.5 * (mse_loss / var + log_var).mean()\n    \n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        \n        patience_counter = 0\n        \n        for epoch in range(epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                \n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                \n                # Combined loss\n                mse_loss = F.mse_loss(mean_pred, targets)\n                uncertainty_loss = self.uncertainty_loss(mean_pred, log_var, targets)\n                \n                # Start with more MSE focus, transition to uncertainty\n                if epoch < 20:\n                    loss = 0.7 * mse_loss + 0.3 * uncertainty_loss\n                else:\n                    loss = 0.3 * mse_loss + 0.7 * uncertainty_loss\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_batches += 1\n            \n            # Validation - FIXED: Handle scalar predictions properly\n            self.model.eval()\n            val_predictions, val_targets, val_log_vars = [], [], []\n            \n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    \n                    # Convert to numpy properly (handle both scalar and tensor cases)\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    \n                    # Ensure we have arrays, not scalars\n                    if mean_pred_np.ndim == 0:  # scalar\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:  # array\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            \n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                \n                # Calculate metrics\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                \n                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}, Loss={avg_train_loss:.4f}\")\n                print(f\"          RÂ²={r2:.4f}, MAE={mae:.4f}, LLL={avg_lll:.4f}\")\n                \n                # Update scheduler\n                scheduler.step(r2)\n                \n                # Save best model\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'best_optimized_model.pth')\n                    print(f\"ğŸ¯ NEW BEST! RÂ²: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                \n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                \n                print(\"-\" * 50)\n        \n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"ğŸ”„ Creating optimized data loaders...\")\n    \n    # Simple stratified split\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    \n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    \n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    \n    # Get tabular dimension\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    \n    # Clear GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    \n    # Data loaders - ensure batch size > 1 to avoid scalar issues\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Initialize model\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Test forward pass\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        \n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        \n        print(f\"âœ… Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"ğŸ’¾ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        \n    except Exception as e:\n        print(f\"âŒ Model test failed: {e}\")\n        return\n    \n    # Train model\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    \n    print(f\"\\nğŸ”¥ FINAL RESULTS:\")\n    print(f\"Best RÂ² = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    \n    return best_r2, best_mae, best_lll\n\nif __name__ == \"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:04:48.455084Z","iopub.execute_input":"2025-10-12T11:04:48.456071Z","iopub.status.idle":"2025-10-12T11:10:44.943932Z","shell.execute_reply.started":"2025-10-12T11:04:48.456017Z","shell.execute_reply":"2025-10-12T11:10:44.943065Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ OPTIMIZED OSIC Model - Targeting RÂ² > 0.5\n============================================================\nğŸ“± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating optimized linear decay coefficients...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 1130.46it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with optimized features\nTarget statistics: mean=-4.8107, std=6.7150\nTarget range: [-39.0741, 11.1389]\nğŸ”„ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nğŸ“Š Model parameters: 7,827,138\nâœ… Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nğŸ’¾ GPU memory: 0.39 GB\nEpoch 1: LR=1.00e-04, Loss=44.2228\n          RÂ²=-0.0730, MAE=4.5866, LLL=-5.2994\nğŸ¯ NEW BEST! RÂ²: -0.0730\n--------------------------------------------------\nEpoch 2: LR=1.00e-04, Loss=31.5946\n          RÂ²=0.1229, MAE=4.2971, LLL=-4.9051\nğŸ¯ NEW BEST! RÂ²: 0.1229\n--------------------------------------------------\nEpoch 3: LR=1.00e-04, Loss=30.5441\n          RÂ²=0.1681, MAE=4.0795, LLL=-4.4679\nğŸ¯ NEW BEST! RÂ²: 0.1681\n--------------------------------------------------\nEpoch 4: LR=1.00e-04, Loss=27.6565\n          RÂ²=0.0747, MAE=4.3061, LLL=-4.7673\n--------------------------------------------------\nEpoch 5: LR=1.00e-04, Loss=25.7505\n          RÂ²=0.2119, MAE=4.1291, LLL=-4.5855\nğŸ¯ NEW BEST! RÂ²: 0.2119\n--------------------------------------------------\nEpoch 6: LR=1.00e-04, Loss=25.4851\n          RÂ²=-0.4063, MAE=5.6597, LLL=-5.8311\n--------------------------------------------------\nEpoch 7: LR=1.00e-04, Loss=24.4778\n          RÂ²=-0.0672, MAE=4.7531, LLL=-4.9753\n--------------------------------------------------\nEpoch 8: LR=1.00e-04, Loss=23.1414\n          RÂ²=-0.7046, MAE=5.6824, LLL=-5.7817\n--------------------------------------------------\nEpoch 9: LR=1.00e-04, Loss=22.8172\n          RÂ²=0.1093, MAE=4.0444, LLL=-4.3556\n--------------------------------------------------\nEpoch 10: LR=1.00e-04, Loss=20.5032\n          RÂ²=-0.0211, MAE=4.5264, LLL=-4.7784\n--------------------------------------------------\nEpoch 11: LR=1.00e-04, Loss=21.3819\n          RÂ²=-0.7177, MAE=5.6050, LLL=-5.6971\n--------------------------------------------------\nEpoch 12: LR=5.00e-05, Loss=19.2403\n          RÂ²=-0.3512, MAE=4.8765, LLL=-5.0574\n--------------------------------------------------\nEpoch 13: LR=5.00e-05, Loss=18.7382\n          RÂ²=0.3080, MAE=3.7376, LLL=-4.0865\nğŸ¯ NEW BEST! RÂ²: 0.3080\n--------------------------------------------------\nEpoch 14: LR=5.00e-05, Loss=17.3323\n          RÂ²=-0.1241, MAE=4.5810, LLL=-4.8079\n--------------------------------------------------\nEpoch 15: LR=5.00e-05, Loss=17.5943\n          RÂ²=0.0562, MAE=4.4326, LLL=-4.6709\n--------------------------------------------------\nEpoch 16: LR=5.00e-05, Loss=17.6418\n          RÂ²=0.0924, MAE=4.2659, LLL=-4.5350\n--------------------------------------------------\nEpoch 17: LR=5.00e-05, Loss=14.5233\n          RÂ²=-0.0446, MAE=4.9358, LLL=-5.1129\n--------------------------------------------------\nEpoch 18: LR=5.00e-05, Loss=17.2467\n          RÂ²=-0.0216, MAE=4.5473, LLL=-4.7710\n--------------------------------------------------\nEpoch 19: LR=5.00e-05, Loss=15.1867\n          RÂ²=0.0854, MAE=4.4361, LLL=-4.6793\n--------------------------------------------------\nEpoch 20: LR=2.50e-05, Loss=14.3313\n          RÂ²=-0.1285, MAE=4.7279, LLL=-4.9326\n--------------------------------------------------\nEpoch 21: LR=2.50e-05, Loss=7.4355\n          RÂ²=0.1670, MAE=4.0871, LLL=-4.3658\n--------------------------------------------------\nEpoch 22: LR=2.50e-05, Loss=8.2252\n          RÂ²=-0.7127, MAE=5.8822, LLL=-5.9120\n--------------------------------------------------\nEpoch 23: LR=2.50e-05, Loss=7.3918\n          RÂ²=-0.0396, MAE=4.5252, LLL=-4.7439\nEarly stopping at epoch 23\n\nğŸ”¥ FINAL RESULTS:\nBest RÂ² = 0.3080\nBest MAE = 3.7376\nBest LLL = -4.0865\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# R^2 NOT ABLE TO PUSH MORE THAN THIS ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T12:34:57.972151Z","iopub.execute_input":"2025-10-12T12:34:57.972542Z","iopub.status.idle":"2025-10-12T12:34:57.976775Z","shell.execute_reply.started":"2025-10-12T12:34:57.972512Z","shell.execute_reply":"2025-10-12T12:34:57.975968Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# LAST TRY WITH LLL AS THE MAIN METRIC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T12:42:16.102759Z","iopub.execute_input":"2025-10-12T12:42:16.103114Z","iopub.status.idle":"2025-10-12T12:42:16.107208Z","shell.execute_reply.started":"2025-10-12T12:42:16.103084Z","shell.execute_reply":"2025-10-12T12:42:16.106424Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"ğŸš€ Optimized OSIC Model - LLL as Main Loss\")\nprint(\"=\"*60)\nprint(f\"ğŸ“± Device: {DEVICE}\")\n\ntrain_df = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\nprint(f\"Loaded dataset with shape: {train_df.shape}\")\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([\n        (age - 50)/30,\n        age / 100,\n    ])\n    vector.append(1.0 if df_row['Sex']=='Male' else 0.0)\n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked':\n        vector.extend([1,0,0])\n    elif smoking_status == 'Ex-smoker':\n        vector.extend([0,1,0])\n    elif smoking_status == 'Currently smokes':\n        vector.extend([0,0,1])\n    else:\n        vector.extend([0,0,0])\n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([\n            fvc / 3000,\n            (fvc - 2500)/1000,\n        ])\n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc = df_row['FVC']\n        age = df_row['Age']\n        sex = df_row['Sex']\n        if sex == 'Male':\n            pp_fvc = fvc / (27.63 - 0.112*age) if age>0 else 0.8\n        else:\n            pp_fvc = fvc / (21.78 - 0.101*age) if age>0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    return np.array(vector)\n\ndef calculate_lll_loss(mean_pred, log_var, targets):\n    # Numerically stable programmatic LLL negative for loss minimization\n    var = torch.exp(log_var)\n    delta = torch.abs(mean_pred - targets)\n    lll = - ( - torch.sqrt(torch.tensor(2.0)) * delta / (var.sqrt() + 1e-6) - torch.log(var.sqrt() * torch.sqrt(torch.tensor(2.0))) )\n    return lll.mean()\n\ndef calculate_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    return -np.sqrt(2)*delta/sigma - np.log(sigma*np.sqrt(2))\n\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating decays ...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient']==patient].copy().sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) >=2:\n        try:\n            if len(weeks)==2:\n                slope = (fvc[1]-fvc[0])/(weeks[1]-weeks[0])\n            else:\n                slopes=[]\n                for i in range(len(weeks)):\n                    for j in range(i+1,len(weeks)):\n                        if weeks[j]!=weeks[i]:\n                            slopes.append((fvc[j]-fvc[i])/(weeks[j]-weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            A[patient] = slope\n        except:\n            A[patient]=0.0\n    else:\n        A[patient]=0.0\n    TAB[patient] = get_optimized_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients.\")\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10,p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0,20.0), p=0.3),\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=10, dropout_rate=0.2):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        for i,param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(),\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,64), nn.ReLU(),\n            nn.Linear(64,1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256,32), nn.ReLU(),\n            nn.Linear(32,1), nn.Tanh()\n        )\n        self._initialize_weights()\n    def _initialize_weights(self):\n        for m in [self.mean_head,self.log_var_head]:\n            if isinstance(m,nn.Linear):\n                nn.init.normal_(m.weight,0,0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias,0.0)\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_features = self.features(images)\n        img_features = self.global_pool(img_features).view(b,-1)\n        tab_features = self.tabular_processor(tabular)\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused)\n        log_var = self.log_var_head(fused)\n        return mean_pred.squeeze(), log_var.squeeze()\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184','ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                image_files = [f for f in patient_dir.iterdir() if f.suffix.lower()=='.dcm']\n                if image_files:\n                    self.patient_images[patient] = image_files\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split=='train':\n            return len(self.valid_patients)*8\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        if self.split=='train':\n            patient_idx = idx % len(self.valid_patients)\n        else:\n            patient_idx = idx\n        patient = self.valid_patients[patient_idx]\n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images) if available_images else available_images[0]\n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_features, target, patient\n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape)==3:\n                img = img[img.shape[0]//2]\n            img = cv2.resize(img,(384,384))\n            img_min,img_max = img.min(), img.max()\n            if img_max>img_min:\n                img = (img-img_min)/(img_max-img_min)*255\n            else:\n                img = np.zeros_like(img)\n            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img,img,img],axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((384,384,3), dtype=np.uint8)\n\nclass OptimizedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_r2 = -float('inf')\n        self.best_val_mae = float('inf')\n        self.best_val_lll = -float('inf')\n    def train(self, train_loader, val_loader, epochs=50):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n        )\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_loss = 0.0\n            train_batches = 0\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, log_var = self.model(images, tabular)\n                # Use negative LLL as loss\n                var = torch.exp(log_var)\n                delta = torch.abs(mean_pred - targets)\n                # Calculate negative log likelihood loss (Laplace)\n                loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                loss = loss.mean()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                train_batches += 1\n            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n            self.model.eval()\n            val_loss_sum = 0.0\n            val_batches = 0\n            val_predictions, val_targets, val_log_vars = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, log_var = self.model(images, tabular)\n                    var = torch.exp(log_var)\n                    delta = torch.abs(mean_pred - targets)\n                    val_loss = torch.sqrt(torch.tensor(2.0)) * delta / (torch.sqrt(var) + 1e-6) + torch.log(torch.sqrt(var) * torch.sqrt(torch.tensor(2.0)) + 1e-6)\n                    val_loss = val_loss.mean()\n                    val_loss_sum += val_loss.item()\n                    val_batches += 1\n                    mean_pred_np = mean_pred.cpu().numpy()\n                    log_var_np = log_var.cpu().numpy()\n                    targets_np = targets.cpu().numpy()\n                    if mean_pred_np.ndim == 0:\n                        val_predictions.append(mean_pred_np.item())\n                        val_log_vars.append(log_var_np.item())\n                        val_targets.append(targets_np.item())\n                    else:\n                        val_predictions.extend(mean_pred_np.tolist())\n                        val_log_vars.extend(log_var_np.tolist())\n                        val_targets.extend(targets_np.tolist())\n            avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else 0\n            if len(val_predictions) > 0:\n                val_pred_np = np.array(val_predictions)\n                val_target_np = np.array(val_targets)\n                val_log_var_np = np.array(val_log_vars)\n                val_sigma_np = np.exp(val_log_var_np / 2)\n                r2 = r2_score(val_target_np, val_pred_np)\n                mae = np.mean(np.abs(val_pred_np - val_target_np))\n                rmse = np.sqrt(np.mean((val_pred_np - val_target_np) ** 2))\n                lll_values = calculate_lll(val_target_np, val_pred_np, val_sigma_np)\n                avg_lll = np.mean(lll_values)\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1}: LR={current_lr:.2e}\")\n                print(f\"          Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n                print(f\"          RÂ²={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, LLL={avg_lll:.4f}\")\n                scheduler.step(r2)\n                if r2 > self.best_val_r2:\n                    self.best_val_r2 = r2\n                    self.best_val_mae = mae\n                    self.best_val_lll = avg_lll\n                    torch.save(self.model.state_dict(), 'best_optimized_model.pth')\n                    print(f\"ğŸ¯ NEW BEST! RÂ²: {r2:.4f}\")\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                if patience_counter >= 10:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n                print(\"-\"*50)\n        return self.best_val_r2, self.best_val_mae, self.best_val_lll\n\ndef optimized_main():\n    print(\"ğŸ”„ Creating optimized data loaders...\")\n    patients_list = list(P)\n    decay_values = [A[patient] for patient in patients_list]\n    decay_bins = pd.cut(decay_values, bins=4, labels=False)\n    train_patients, val_patients = train_test_split(\n        patients_list, test_size=0.15, random_state=42, stratify=decay_bins\n    )\n    print(f\"Train: {len(train_patients)}, Val: {len(val_patients)}\")\n    tabular_dim = len(TAB[train_patients[0]])\n    print(f\"Tabular feature dimension: {tabular_dim}\")\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_dataset = OptimizedOSICDataset(train_patients, A, TAB, TRAIN_DIR, 'train')\n    val_dataset = OptimizedOSICDataset(val_patients, A, TAB, TRAIN_DIR, 'val')\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(DEVICE)\n    print(f\"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        test_batch = next(iter(train_loader))\n        images, tabular, targets, _ = test_batch\n        images, tabular = images.to(DEVICE), tabular.to(DEVICE)\n        with torch.no_grad():\n            mean_pred, log_var = model(images, tabular)\n        print(\"âœ… Model forward pass successful!\")\n        print(f\"Output shapes - Mean: {mean_pred.shape}, Log Var: {log_var.shape}\")\n        print(f\"ğŸ’¾ GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"âŒ Model test failed: {e}\")\n        return\n    trainer = OptimizedTrainer(model, DEVICE, lr=1e-4)\n    best_r2, best_mae, best_lll = trainer.train(train_loader, val_loader, epochs=50)\n    print(\"\\nğŸ”¥ FINAL RESULTS:\")\n    print(f\"Best RÂ² = {best_r2:.4f}\")\n    print(f\"Best MAE = {best_mae:.4f}\")\n    print(f\"Best LLL = {best_lll:.4f}\")\n    return best_r2, best_mae, best_lll\n\nif __name__==\"__main__\":\n    final_r2, final_mae, final_lll = optimized_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T12:42:34.222310Z","iopub.execute_input":"2025-10-12T12:42:34.222616Z","iopub.status.idle":"2025-10-12T12:52:31.813809Z","shell.execute_reply.started":"2025-10-12T12:42:34.222595Z","shell.execute_reply":"2025-10-12T12:52:31.812875Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Optimized OSIC Model - LLL as Main Loss\n============================================================\nğŸ“± Device: cuda\nLoaded dataset with shape: (1549, 7)\nCalculating decays ...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 1187.93it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nğŸ”„ Creating optimized data loaders...\nTrain: 149, Val: 27\nTabular feature dimension: 9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 149 patients with images\nDataset val: 25 patients with images\nğŸ“Š Model parameters: 7,827,138\nâœ… Model forward pass successful!\nOutput shapes - Mean: torch.Size([8]), Log Var: torch.Size([8])\nğŸ’¾ GPU memory: 0.27 GB\nEpoch 1: LR=1.00e-04\n          Train Loss=5.8900, Val Loss=4.8620\n          RÂ²=-0.0357, MAE=4.5566, RMSE=5.5385, LLL=-4.9142\nğŸ¯ NEW BEST! RÂ²: -0.0357\n--------------------------------------------------\nEpoch 2: LR=1.00e-04\n          Train Loss=4.6655, Val Loss=4.1888\n          RÂ²=0.1320, MAE=4.0214, RMSE=5.0703, LLL=-4.3725\nğŸ¯ NEW BEST! RÂ²: 0.1320\n--------------------------------------------------\nEpoch 3: LR=1.00e-04\n          Train Loss=4.4928, Val Loss=4.9445\n          RÂ²=-0.0641, MAE=4.7120, RMSE=5.6138, LLL=-4.9824\n--------------------------------------------------\nEpoch 4: LR=1.00e-04\n          Train Loss=4.4559, Val Loss=5.4866\n          RÂ²=-0.1115, MAE=4.7631, RMSE=5.7375, LLL=-4.9617\n--------------------------------------------------\nEpoch 5: LR=1.00e-04\n          Train Loss=4.3827, Val Loss=5.3298\n          RÂ²=-0.0701, MAE=4.9195, RMSE=5.6297, LLL=-5.0892\n--------------------------------------------------\nEpoch 6: LR=1.00e-04\n          Train Loss=4.2767, Val Loss=5.1068\n          RÂ²=-0.1016, MAE=4.7841, RMSE=5.7120, LLL=-4.9667\n--------------------------------------------------\nEpoch 7: LR=1.00e-04\n          Train Loss=4.2122, Val Loss=4.3027\n          RÂ²=-0.1302, MAE=4.4394, RMSE=5.7857, LLL=-4.6755\n--------------------------------------------------\nEpoch 8: LR=1.00e-04\n          Train Loss=4.1501, Val Loss=4.5611\n          RÂ²=0.1751, MAE=4.0369, RMSE=4.9429, LLL=-4.3120\nğŸ¯ NEW BEST! RÂ²: 0.1751\n--------------------------------------------------\nEpoch 9: LR=1.00e-04\n          Train Loss=4.1067, Val Loss=4.2426\n          RÂ²=0.0701, MAE=4.2747, RMSE=5.2479, LLL=-4.5181\n--------------------------------------------------\nEpoch 10: LR=1.00e-04\n          Train Loss=3.9683, Val Loss=4.4383\n          RÂ²=0.0220, MAE=4.4478, RMSE=5.3819, LLL=-4.6680\n--------------------------------------------------\nEpoch 11: LR=1.00e-04\n          Train Loss=3.8902, Val Loss=4.0248\n          RÂ²=0.1420, MAE=4.3315, RMSE=5.0410, LLL=-4.5672\n--------------------------------------------------\nEpoch 12: LR=1.00e-04\n          Train Loss=3.8330, Val Loss=4.5516\n          RÂ²=0.2260, MAE=3.9422, RMSE=4.7878, LLL=-4.2306\nğŸ¯ NEW BEST! RÂ²: 0.2260\n--------------------------------------------------\nEpoch 13: LR=1.00e-04\n          Train Loss=3.9706, Val Loss=4.6464\n          RÂ²=0.0109, MAE=4.5090, RMSE=5.4124, LLL=-4.7225\n--------------------------------------------------\nEpoch 14: LR=1.00e-04\n          Train Loss=3.7959, Val Loss=4.6415\n          RÂ²=-0.0366, MAE=4.3593, RMSE=5.5408, LLL=-4.5900\n--------------------------------------------------\nEpoch 15: LR=1.00e-04\n          Train Loss=3.7485, Val Loss=4.5717\n          RÂ²=0.1055, MAE=4.3477, RMSE=5.1472, LLL=-4.5790\n--------------------------------------------------\nEpoch 16: LR=1.00e-04\n          Train Loss=3.7070, Val Loss=4.2176\n          RÂ²=0.0701, MAE=4.0374, RMSE=5.2480, LLL=-4.3121\n--------------------------------------------------\nEpoch 17: LR=1.00e-04\n          Train Loss=3.7102, Val Loss=4.8308\n          RÂ²=-0.1746, MAE=4.7490, RMSE=5.8981, LLL=-4.9221\n--------------------------------------------------\nEpoch 18: LR=1.00e-04\n          Train Loss=3.5533, Val Loss=3.8949\n          RÂ²=0.1935, MAE=3.8333, RMSE=4.8875, LLL=-4.1392\n--------------------------------------------------\nEpoch 19: LR=5.00e-05\n          Train Loss=3.4886, Val Loss=4.5901\n          RÂ²=0.1796, MAE=4.0762, RMSE=4.9294, LLL=-4.3483\n--------------------------------------------------\nEpoch 20: LR=5.00e-05\n          Train Loss=3.4229, Val Loss=5.7947\n          RÂ²=-0.0758, MAE=4.6986, RMSE=5.6446, LLL=-4.8806\n--------------------------------------------------\nEpoch 21: LR=5.00e-05\n          Train Loss=3.3154, Val Loss=4.0762\n          RÂ²=0.2336, MAE=3.8464, RMSE=4.7644, LLL=-4.1487\nğŸ¯ NEW BEST! RÂ²: 0.2336\n--------------------------------------------------\nEpoch 22: LR=5.00e-05\n          Train Loss=3.3759, Val Loss=4.8922\n          RÂ²=-0.0031, MAE=4.7212, RMSE=5.4506, LLL=-4.8986\n--------------------------------------------------\nEpoch 23: LR=5.00e-05\n          Train Loss=3.2811, Val Loss=4.7461\n          RÂ²=0.1836, MAE=4.2279, RMSE=4.9174, LLL=-4.4776\n--------------------------------------------------\nEpoch 24: LR=5.00e-05\n          Train Loss=3.3935, Val Loss=4.6541\n          RÂ²=0.0039, MAE=4.6889, RMSE=5.4315, LLL=-4.8710\n--------------------------------------------------\nEpoch 25: LR=5.00e-05\n          Train Loss=3.3587, Val Loss=3.7919\n          RÂ²=0.2929, MAE=3.8235, RMSE=4.5763, LLL=-4.1290\nğŸ¯ NEW BEST! RÂ²: 0.2929\n--------------------------------------------------\nEpoch 26: LR=5.00e-05\n          Train Loss=3.2915, Val Loss=4.2851\n          RÂ²=0.2631, MAE=3.7403, RMSE=4.6717, LLL=-4.0567\n--------------------------------------------------\nEpoch 27: LR=5.00e-05\n          Train Loss=3.2412, Val Loss=4.5546\n          RÂ²=0.1836, MAE=4.1929, RMSE=4.9171, LLL=-4.4446\n--------------------------------------------------\nEpoch 28: LR=5.00e-05\n          Train Loss=3.2467, Val Loss=3.3568\n          RÂ²=0.2942, MAE=3.5670, RMSE=4.5719, LLL=-3.9075\nğŸ¯ NEW BEST! RÂ²: 0.2942\n--------------------------------------------------\nEpoch 29: LR=5.00e-05\n          Train Loss=3.2188, Val Loss=3.8629\n          RÂ²=0.3853, MAE=3.6115, RMSE=4.2669, LLL=-3.9469\nğŸ¯ NEW BEST! RÂ²: 0.3853\n--------------------------------------------------\nEpoch 30: LR=5.00e-05\n          Train Loss=3.0604, Val Loss=4.3224\n          RÂ²=0.1197, MAE=4.2479, RMSE=5.1060, LLL=-4.4925\n--------------------------------------------------\nEpoch 31: LR=5.00e-05\n          Train Loss=3.0496, Val Loss=4.2708\n          RÂ²=0.0518, MAE=4.4661, RMSE=5.2994, LLL=-4.6812\n--------------------------------------------------\nEpoch 32: LR=5.00e-05\n          Train Loss=3.0587, Val Loss=4.7074\n          RÂ²=0.0179, MAE=4.5522, RMSE=5.3932, LLL=-4.7531\n--------------------------------------------------\nEpoch 33: LR=5.00e-05\n          Train Loss=3.1276, Val Loss=4.0996\n          RÂ²=0.1607, MAE=4.2391, RMSE=4.9857, LLL=-4.4855\n--------------------------------------------------\nEpoch 34: LR=5.00e-05\n          Train Loss=3.0665, Val Loss=4.1425\n          RÂ²=0.1802, MAE=4.0892, RMSE=4.9276, LLL=-4.3561\n--------------------------------------------------\nEpoch 35: LR=5.00e-05\n          Train Loss=3.0628, Val Loss=4.2117\n          RÂ²=0.1355, MAE=4.0174, RMSE=5.0601, LLL=-4.2938\n--------------------------------------------------\nEpoch 36: LR=2.50e-05\n          Train Loss=3.0044, Val Loss=4.2891\n          RÂ²=0.2045, MAE=3.9122, RMSE=4.8538, LLL=-4.2033\n--------------------------------------------------\nEpoch 37: LR=2.50e-05\n          Train Loss=2.8241, Val Loss=3.5216\n          RÂ²=0.2419, MAE=3.9667, RMSE=4.7384, LLL=-4.2515\n--------------------------------------------------\nEpoch 38: LR=2.50e-05\n          Train Loss=2.9955, Val Loss=4.2395\n          RÂ²=0.1451, MAE=4.0406, RMSE=5.0320, LLL=-4.3142\n--------------------------------------------------\nEpoch 39: LR=2.50e-05\n          Train Loss=2.8967, Val Loss=4.0297\n          RÂ²=-0.0310, MAE=4.2939, RMSE=5.5258, LLL=-4.5318\nEarly stopping at epoch 39\n\nğŸ”¥ FINAL RESULTS:\nBest RÂ² = 0.3853\nBest MAE = 3.6115\nBest LLL = -3.9469\n","output_type":"stream"}],"execution_count":43}]}
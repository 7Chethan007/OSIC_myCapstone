{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**V2 Testing**\n# Critical implementation bugs (fix first)\n\n\n1. pydicom API misuse\n\n- AttributeError: module 'pydicom' has no attribute 'read_file'. Use the supported API: pydicom.dcmread(...) (older read_file deprecations).\n- Verify pydicom version and use dcmread. Also use .pixel_array only after confirming hasattr.\n\n2. Random projection inside forward pass (major bug)\n\n- tab_proj = F.linear(tab_expanded, torch.randn(1024, 512).to(images.device))\n- You are using a new random matrix every forward pass. That destroys learning of cross-modal attention. Replace with a trainable nn.Linear(512, 1024) (initialized once). This is probably the single biggest structural bug.\n\n3. Inconsistent sigma / scale handling and arbitrary multipliers\n\n- In training you clamp sigma = exp(log_var/2) to min 2.0 â€” while evaluation uses sigma floor 70.0. In submission you set confidence_val = max(exp(log_var/2) * 70, 70) (why multiply by 70?). This mismatch creates meaningless confidences and ruins LLL.\n- Fix: keep internal sigma in natural units of FVC; do not multiply by arbitrary constants. Only apply the contest evaluation floor (70) at scoring time â€” not during training.\n\n4. Submission pipeline clipping hides bug\n\n- You clamp predictions to [800,6000] during submission â€” all predictions hitting 800 likely reflect upstream underprediction or wrong scale. Remove clipping while debugging; investigate raw predictions distribution first.\n\n5. Hard-coded patient exclusions & path issues\n\n- Filtering out two patient IDs silently in dataset init is suspicious. Document why or remove. Ensure mapping patient â†’ image files is correct and complete.\n\n6. ModelWithConfidence referenced but not defined â†’ NameError later. Keep code consistent.","metadata":{}},{"cell_type":"code","source":"# Install required packages\nimport subprocess\nimport sys\n\ndef install_package(package):\n    try:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\nprint(\"Installing required packages...\")\npackages = [\n    'pydicom',\n    'pylibjpeg',\n    'pylibjpeg-libjpeg', \n    'gdcm',\n    'opencv-python-headless',\n    'scikit-learn',\n    'albumentations',\n    'tqdm',\n    'seaborn'\n]\n\nfor pkg in packages:\n    if install_package(pkg):\n        print(f\"âœ… {pkg} installed\")\n    else:\n        print(f\"âš ï¸ {pkg} installation failed (may already be installed)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T04:36:39.414202Z","iopub.execute_input":"2025-09-05T04:36:39.414595Z","iopub.status.idle":"2025-09-05T04:37:05.864157Z","shell.execute_reply.started":"2025-09-05T04:36:39.414563Z","shell.execute_reply":"2025-09-05T04:37:05.863239Z"}},"outputs":[{"name":"stdout","text":"Installing required packages...\nâœ… pydicom installed\nâœ… pylibjpeg installed\nâœ… pylibjpeg-libjpeg installed\nâœ… gdcm installed\nâœ… opencv-python-headless installed\nâœ… scikit-learn installed\nâœ… albumentations installed\nâœ… tqdm installed\nâœ… seaborn installed\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Import libraries\nimport os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport random\nfrom tqdm import tqdm \nfrom datetime import timedelta, datetime\nfrom pathlib import Path\nimport json\nimport warnings\nimport pickle\nimport glob\nfrom math import sqrt, log\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Image processing\nfrom skimage import measure, morphology, segmentation\nfrom skimage.transform import resize\nfrom scipy.ndimage import binary_dilation, binary_erosion\nfrom skimage.measure import label, regionprops\nfrom sklearn.cluster import KMeans\nfrom skimage.segmentation import clear_border\n\n# Deep learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Albumentations for medical augmentations\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport lightgbm as lgb\n\n# Check for TPU/GPU acceleration\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    HAS_TPU = True\nexcept ImportError:\n    HAS_TPU = False\n\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(42)\n\n# Configuration - Detect and use best available accelerator\nif HAS_TPU:\n    DEVICE = xm.xla_device()\n    print(\"Using TPU accelerator\")\nelif torch.cuda.is_available():\n    DEVICE = torch.device(\"cuda\")\n    print(\"Using GPU accelerator\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nTEST_DIR = DATA_DIR / \"test\"\n\nprint(\"Pulmonary Fibrosis Progression Analysis - OPTIMIZED VERSION\")\nprint(f\"Device: {DEVICE}\")\n\n# =============================================================================\n# PART 1: DATA LOADING AND EDA\n# =============================================================================\n\n# Load datasets\ntrain_df = pd.read_csv(DATA_DIR / 'train.csv')\ntry:\n    test_df = pd.read_csv(DATA_DIR / 'test.csv')\n    print(f'Train: {train_df.shape[0]} rows, Test: {test_df.shape[0]} rows')\nexcept:\n    print(f'Train: {train_df.shape[0]} rows, Test: file not found')\n    test_df = None\n\nprint(\"\\nTrain data sample:\")\nprint(train_df.head())\n\nprint(\"\\nTrain data statistics:\")\nprint(train_df.describe())\n\n# Basic EDA\nprint(f'\\nUnique patients in training data: {train_df[\"Patient\"].nunique()}')\nprint(f'Total observations: {len(train_df)}')\nprint(f'Average observations per patient: {len(train_df)/train_df[\"Patient\"].nunique():.2f}')\n\n# Check for missing values\nprint(f'\\nMissing values:')\nprint(train_df.isnull().sum())\n\n# =============================================================================\n# PART 2: FEATURE ENGINEERING\n# =============================================================================\n\nprint(\"Processing tabular features...\")\n\n# Create baseline features for each patient\nbaseline_features = {}\npatient_slopes = {}\npatient_intercepts = {}\n\nfor patient in train_df['Patient'].unique():\n    patient_data = train_df[train_df['Patient'] == patient].copy().sort_values('Weeks')\n    \n    # Get baseline measurement (first visit)\n    baseline = patient_data.iloc[0]\n    baseline_features[patient] = {\n        'Age': baseline['Age'],\n        'Sex': baseline['Sex'],\n        'SmokingStatus': baseline['SmokingStatus'],\n        'BaselineFVC': baseline['FVC'],\n        'BaselineWeeks': baseline['Weeks'],\n        'Percent': baseline['Percent'] if 'Percent' in baseline else 50.0  # Fallback\n    }\n    \n    # Calculate slope and intercept if multiple measurements\n    if len(patient_data) > 1:\n        weeks = patient_data['Weeks'].values\n        fvc = patient_data['FVC'].values\n        \n        # Linear regression: FVC = slope * weeks + intercept\n        A = np.vstack([weeks, np.ones(len(weeks))]).T\n        slope, intercept = np.linalg.lstsq(A, fvc, rcond=None)[0]\n        \n        patient_slopes[patient] = slope\n        patient_intercepts[patient] = intercept\n    else:\n        patient_slopes[patient] = 0.0\n        patient_intercepts[patient] = baseline['FVC']\n\n# Create enhanced tabular features\ndef get_enhanced_tabular_features(patient_id, row=None):\n    \"\"\"Get enhanced tabular features with proper encoding\"\"\"\n    features = baseline_features[patient_id].copy()\n    \n    # Standardize age\n    features['Age'] = (features['Age'] - 50) / 20  # Standardize around mean\n    \n    # Encode sex (0 for Male, 1 for Female)\n    features['Sex'] = 1 if features['Sex'] == 'Female' else 0\n    \n    # One-hot encode smoking status\n    smoking_status = features['SmokingStatus']\n    features['Smoking_Never'] = 1 if smoking_status == 'Never smoked' else 0\n    features['Smoking_Ex'] = 1 if smoking_status == 'Ex-smoker' else 0\n    features['Smoking_Current'] = 1 if smoking_status == 'Currently smokes' else 0\n    \n    # Standardize baseline FVC\n    features['BaselineFVC'] = (features['BaselineFVC'] - 2500) / 1000\n    \n    # Standardize baseline weeks\n    features['BaselineWeeks'] = features['BaselineWeeks'] / 100\n    \n    # Standardize Percent\n    features['Percent'] = (features['Percent'] - 50) / 20\n    \n    # Add slope and intercept\n    features['Slope'] = patient_slopes[patient_id] / 10  # Scale slope\n    features['Intercept'] = (patient_intercepts[patient_id] - 2500) / 1000\n    \n    # Remove the original smoking status\n    del features['SmokingStatus']\n    \n    # If we have a row with current week, add week delta\n    if row is not None:\n        week_delta = (row['Weeks'] - features['BaselineWeeks'] * 100) / 50\n        features['WeekDelta'] = week_delta\n    \n    return np.array(list(features.values()), dtype=np.float32)\n\n# Calculate slopes for each patient\nA = patient_slopes\nTAB = {patient: get_enhanced_tabular_features(patient) for patient in baseline_features.keys()}\nP = list(baseline_features.keys())\n\nprint(f\"Processed {len(P)} patients with enhanced features\")\n\n# =============================================================================\n# PART 3: ENHANCED DICOM PROCESSING WITH HU PRESERVATION\n# =============================================================================\n\ndef get_pixels_hu(dcm):\n    \"\"\"Convert DICOM pixel array to Hounsfield Units\"\"\"\n    try:\n        # Get pixel array\n        pixel_array = dcm.pixel_array.astype(np.float32)\n        \n        # Apply rescale intercept and slope if available\n        intercept = getattr(dcm, 'RescaleIntercept', 0)\n        slope = getattr(dcm, 'RescaleSlope', 1)\n        \n        pixel_array = pixel_array * slope + intercept\n        \n        return pixel_array\n    except:\n        return np.zeros((512, 512), dtype=np.float32)\n\ndef load_dicom_with_hu(path):\n    \"\"\"Load DICOM and preserve Hounsfield Units\"\"\"\n    try:\n        dcm = pydicom.dcmread(str(path), force=True)\n        hu_image = get_pixels_hu(dcm)\n        \n        # Window to lung range [-1200, 600]\n        hu_image = np.clip(hu_image, -1200, 600)\n        \n        # Normalize to [-1, 1]\n        hu_image = (hu_image + 300) / 900  # Center around -300, scale by 900\n        \n        return hu_image\n    except Exception as e:\n        return np.zeros((512, 512), dtype=np.float32) - 1  # Return -1 filled array\n\ndef load_three_slices(patient_dir, slice_idx=None, target_size=(256, 256)):\n    \"\"\"Load three adjacent slices for 2.5D input with consistent sizing\"\"\"\n    try:\n        # Get all DICOM files for patient\n        dicom_files = sorted(list(patient_dir.glob('*.dcm')))\n        if not dicom_files:\n            return None\n            \n        # Use middle slice if not specified\n        if slice_idx is None:\n            slice_idx = len(dicom_files) // 2\n            \n        # Get three slices around the index\n        slices = []\n        for i in range(max(0, slice_idx-1), min(len(dicom_files), slice_idx+2)):\n            slice_img = load_dicom_with_hu(dicom_files[i])\n            \n            # Resize to target size\n            if slice_img.shape != target_size:\n                slice_img = cv2.resize(slice_img, target_size, interpolation=cv2.INTER_AREA)\n                \n            slices.append(slice_img)\n            \n        # If we couldn't get 3 slices, duplicate existing ones\n        while len(slices) < 3:\n            slices.append(slices[-1] if slices else np.zeros(target_size, dtype=np.float32) - 1)\n            \n        # Stack slices as channels\n        stacked = np.stack(slices, axis=-1)\n        return stacked\n        \n    except Exception as e:\n        # Return dummy 3-slice image with correct size\n        dummy_slice = np.zeros(target_size, dtype=np.float32) - 1\n        return np.stack([dummy_slice] * 3, axis=-1)\n\n# =============================================================================\n# PART 4: MEDICAL AUGMENTATIONS FOR HU IMAGES\n# =============================================================================\n\nclass MedicalAugmentation:\n    def __init__(self, augment=True, target_size=(256, 256)):\n        self.target_size = target_size\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=5, p=0.3),  # Reduced rotation for medical images\n                albu.HorizontalFlip(p=0.3),\n                albu.ShiftScaleRotate(shift_limit=0.03, scale_limit=0.05, rotate_limit=5, p=0.3),\n                albu.GaussNoise(var_limit=(0.01, 0.05), p=0.2),  # Reduced noise for HU\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        # Ensure image is the correct size\n        if image.shape[:2] != self.target_size:\n            image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_AREA)\n        return self.transform(image=image)['image']\n\n# =============================================================================\n# PART 5: DATASET CLASS WITH 2.5D INPUT\n# =============================================================================\n\nclass OSICDenseNetDataset(Dataset):\n    \"\"\"Dataset class with 2.5D input and robust error handling\"\"\"\n    \n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True, target_size=(256, 256)):\n        self.patients = patients\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augment = augment\n        self.target_size = target_size\n        self.augmentor = MedicalAugmentation(augment=augment, target_size=target_size)\n        \n        # Preload patient directories and file lists\n        self.patient_dirs = {}\n        valid_patients = []\n        \n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            \n            if not patient_dir.exists():\n                continue\n                \n            dicom_files = list(patient_dir.glob('*.dcm'))\n            \n            if dicom_files:\n                self.patient_dirs[patient] = patient_dir\n                valid_patients.append(patient)\n        \n        self.valid_patients = valid_patients\n        print(f\"Dataset {split}: {len(self.valid_patients)} valid patients\")\n    \n    def __len__(self):\n        return len(self.valid_patients) * (4 if self.split == 'train' else 1)\n    \n    def __getitem__(self, idx):\n        try:\n            patient_idx = idx % len(self.valid_patients)\n            patient = self.valid_patients[patient_idx]\n            \n            # Load 2.5D image (3 slices)\n            img = load_three_slices(self.patient_dirs[patient], target_size=self.target_size)\n            \n            # Apply augmentations\n            img_tensor = self.augmentor(img)\n            \n            # Get features\n            tab_features = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n            target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n            \n            return img_tensor, tab_features, target, patient\n            \n        except Exception as e:\n            # Return dummy data with consistent sizes\n            dummy_img = torch.zeros((3, self.target_size[0], self.target_size[1]), dtype=torch.float32)\n            dummy_tab = torch.zeros(len(self.TAB_dict[patient]), dtype=torch.float32)\n            dummy_target = torch.tensor(0.0, dtype=torch.float32)\n            return dummy_img, dummy_tab, dummy_target, \"dummy_patient\"\n\n# =============================================================================\n# PART 6: IMPROVED MODEL ARCHITECTURE\n# =============================================================================\n\nclass EfficientNetModel(nn.Module):\n    \"\"\"More efficient model using EfficientNet backbone\"\"\"\n    \n    def __init__(self, tabular_dim=10):\n        super(EfficientNetModel, self).__init__()\n        \n        # EfficientNet backbone\n        self.backbone = models.efficientnet_b0(pretrained=True)\n        \n        # Modify first convolution to accept 3 channels properly\n        original_first_conv = self.backbone.features[0][0]\n        self.backbone.features[0][0] = nn.Conv2d(\n            3, original_first_conv.out_channels, \n            kernel_size=original_first_conv.kernel_size,\n            stride=original_first_conv.stride,\n            padding=original_first_conv.padding,\n            bias=original_first_conv.bias\n        )\n        \n        # Initialize with pretrained weights (average across RGB channels)\n        with torch.no_grad():\n            self.backbone.features[0][0].weight[:, :3] = original_first_conv.weight.clone()\n            if original_first_conv.bias is not None:\n                self.backbone.features[0][0].bias = original_first_conv.bias.clone()\n        \n        # Get number of features from backbone\n        self.num_image_features = self.backbone.classifier[1].in_features\n        \n        # Tabular processor\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n        \n        # Fusion and prediction\n        self.fusion = nn.Sequential(\n            nn.Linear(self.num_image_features + 128, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU()\n        )\n        \n        self.mean_head = nn.Linear(128, 1)\n        self.log_var_head = nn.Linear(128, 1)\n        \n    def forward(self, images, tabular):\n        # Image features\n        img_features = self.backbone.features(images)\n        img_features = F.adaptive_avg_pool2d(img_features, (1, 1))\n        img_features = img_features.view(img_features.size(0), -1)\n        \n        # Tabular features\n        tab_features = self.tabular_processor(tabular)\n        \n        # Fusion\n        combined = torch.cat([img_features, tab_features], dim=1)\n        fused = self.fusion(combined)\n        \n        # Predictions\n        mean_pred = self.mean_head(fused).squeeze(-1)\n        log_var = self.log_var_head(fused).squeeze(-1)\n        \n        return mean_pred, log_var\n\n# Initialize model\ntabular_dim = len(TAB[list(TAB.keys())[0]])\nmodel = EfficientNetModel(tabular_dim=tabular_dim).to(DEVICE)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# =============================================================================\n# PART 7: DATA SPLIT AND LOADERS\n# =============================================================================\n\n# Split patients using GroupKFold for better validation\npatients_list = list(P)\nkfold = GroupKFold(n_splits=5)\n\n# For simplicity, use the first fold\ntrain_idx, val_idx = next(kfold.split(patients_list, groups=patients_list))\ntrain_patients = [patients_list[i] for i in train_idx]\nval_patients = [patients_list[i] for i in val_idx]\n\nprint(f\"Train patients: {len(train_patients)}, Validation patients: {len(val_patients)}\")\n\n# Create datasets with consistent target size (smaller for faster training)\nTARGET_SIZE = (256, 256)\ntrain_dataset = OSICDenseNetDataset(\n    patients=train_patients, A_dict=A, TAB_dict=TAB, \n    data_dir=TRAIN_DIR, split='train', augment=True, target_size=TARGET_SIZE\n)\n\nval_dataset = OSICDenseNetDataset(\n    patients=val_patients, A_dict=A, TAB_dict=TAB,\n    data_dir=TRAIN_DIR, split='val', augment=False, target_size=TARGET_SIZE\n)\n\n# Create data loaders with appropriate batch size\nBATCH_SIZE = 16 if HAS_TPU or torch.cuda.is_available() else 4\nNUM_WORKERS = 4 if HAS_TPU or torch.cuda.is_available() else 2\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n    num_workers=NUM_WORKERS, pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n    num_workers=NUM_WORKERS, pin_memory=True\n)\n\nprint(f\"Data loaders created: {len(train_loader)} train, {len(val_loader)} val batches\")\nprint(f\"Using batch size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n\n# =============================================================================\n# PART 8: TRAINING WITH IMPROVED LOSS AND METRICS\n# =============================================================================\n\ndef laplace_log_likelihood(y_true, y_pred, sigma, sigma_min=70):\n    \"\"\"Compute Laplace Log Likelihood with sigma clipping\"\"\"\n    sigma = np.maximum(sigma, sigma_min)\n    delta = np.abs(y_true - y_pred)\n    return -np.sqrt(2) * delta / sigma - np.log(np.sqrt(2) * sigma)\n\nclass ImprovedTrainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer, mode='min', patience=5, factor=0.5, verbose=True\n        )\n        self.best_loss = float('inf')\n        self.scaler = GradScaler()\n        \n        # For TPU\n        self.is_tpu = hasattr(device, 'type') and device.type == 'xla'\n    \n    def gaussian_nll_loss(self, mean_pred, log_var, targets):\n        \"\"\"Gaussian negative log likelihood loss\"\"\"\n        return 0.5 * (torch.mean(torch.exp(-log_var) * (mean_pred - targets)**2 + log_var))\n    \n    def compute_metrics(self, mean_pred, log_var, targets):\n        \"\"\"Compute various metrics for evaluation\"\"\"\n        # Convert to numpy for metric calculation\n        mean_pred_np = mean_pred.detach().cpu().numpy()\n        log_var_np = log_var.detach().cpu().numpy()\n        targets_np = targets.detach().cpu().numpy()\n        \n        # Calculate sigma\n        sigma_np = np.sqrt(np.exp(log_var_np))\n        \n        # Metrics\n        mse = mean_squared_error(targets_np, mean_pred_np)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(targets_np, mean_pred_np)\n        \n        # Laplace Log Likelihood\n        lll = np.mean(laplace_log_likelihood(targets_np, mean_pred_np, sigma_np))\n        \n        return {\n            'mse': mse,\n            'rmse': rmse,\n            'r2': r2,\n            'lll': lll\n        }\n    \n    def train_epoch(self, loader):\n        self.model.train()\n        total_loss = 0\n        all_metrics = {'mse': 0, 'rmse': 0, 'r2': 0, 'lll': 0}\n        \n        for images, tabular, targets, _ in tqdm(loader, desc=\"Training\"):\n            if self.is_tpu:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n            else:\n                images, tabular, targets = images.to(self.device, non_blocking=True), \\\n                                         tabular.to(self.device, non_blocking=True), \\\n                                         targets.to(self.device, non_blocking=True)\n            \n            self.optimizer.zero_grad()\n            \n            if self.is_tpu:\n                # TPU doesn't support AMP, use regular training\n                mean_pred, log_var = self.model(images, tabular)\n                loss = self.gaussian_nll_loss(mean_pred, log_var, targets)\n                loss.backward()\n                xm.optimizer_step(self.optimizer)\n            else:\n                # Use AMP for GPU\n                with autocast():\n                    mean_pred, log_var = self.model(images, tabular)\n                    loss = self.gaussian_nll_loss(mean_pred, log_var, targets)\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            \n            total_loss += loss.item()\n            \n            # Compute metrics\n            metrics = self.compute_metrics(mean_pred, log_var, targets)\n            for k in all_metrics:\n                all_metrics[k] += metrics[k]\n        \n        # Average metrics\n        for k in all_metrics:\n            all_metrics[k] /= len(loader)\n        \n        return total_loss / len(loader), all_metrics\n    \n    def validate(self, loader):\n        self.model.eval()\n        total_loss = 0\n        all_metrics = {'mse': 0, 'rmse': 0, 'r2': 0, 'lll': 0}\n        \n        with torch.no_grad():\n            for images, tabular, targets, _ in tqdm(loader, desc=\"Validation\"):\n                if self.is_tpu:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                else:\n                    images, tabular, targets = images.to(self.device, non_blocking=True), \\\n                                             tabular.to(self.device, non_blocking=True), \\\n                                             targets.to(self.device, non_blocking=True)\n                \n                mean_pred, log_var = self.model(images, tabular)\n                loss = self.gaussian_nll_loss(mean_pred, log_var, targets)\n                total_loss += loss.item()\n                \n                # Compute metrics\n                metrics = self.compute_metrics(mean_pred, log_var, targets)\n                for k in all_metrics:\n                    all_metrics[k] += metrics[k]\n        \n        # Average metrics\n        for k in all_metrics:\n            all_metrics[k] /= len(loader)\n        \n        return total_loss / len(loader), all_metrics\n    \n    def train(self, train_loader, val_loader, epochs=20):\n        for epoch in range(epochs):\n            train_loss, train_metrics = self.train_epoch(train_loader)\n            val_loss, val_metrics = self.validate(val_loader)\n            \n            self.scheduler.step(val_loss)\n            \n            print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n            print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n            print(\"Train Metrics - MSE: {mse:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}, LLL: {lll:.4f}\".format(**train_metrics))\n            print(\"Val Metrics   - MSE: {mse:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}, LLL: {lll:.4f}\".format(**val_metrics))\n            \n            if val_loss < self.best_loss:\n                self.best_loss = val_loss\n                if self.is_tpu:\n                    xm.save(model.state_dict(), 'best_model.pth')\n                else:\n                    torch.save(model.state_dict(), 'best_model.pth')\n                print(\"âœ… New best model saved!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T04:37:27.480998Z","iopub.execute_input":"2025-09-05T04:37:27.481306Z","iopub.status.idle":"2025-09-05T04:37:28.482987Z","shell.execute_reply.started":"2025-09-05T04:37:27.481282Z","shell.execute_reply":"2025-09-05T04:37:28.482328Z"}},"outputs":[{"name":"stdout","text":"Using GPU accelerator\nPulmonary Fibrosis Progression Analysis - OPTIMIZED VERSION\nDevice: cuda\nTrain: 1549 rows, Test: 5 rows\n\nTrain data sample:\n                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus\n0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker\n1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker\n2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker\n3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker\n4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker\n\nTrain data statistics:\n             Weeks          FVC      Percent          Age\ncount  1549.000000  1549.000000  1549.000000  1549.000000\nmean     31.861846  2690.479019    77.672654    67.188509\nstd      23.247550   832.770959    19.823261     7.057395\nmin      -5.000000   827.000000    28.877577    49.000000\n25%      12.000000  2109.000000    62.832700    63.000000\n50%      28.000000  2641.000000    75.676937    68.000000\n75%      47.000000  3171.000000    88.621065    72.000000\nmax     133.000000  6399.000000   153.145378    88.000000\n\nUnique patients in training data: 176\nTotal observations: 1549\nAverage observations per patient: 8.80\n\nMissing values:\nPatient          0\nWeeks            0\nFVC              0\nPercent          0\nAge              0\nSex              0\nSmokingStatus    0\ndtype: int64\nProcessing tabular features...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients with enhanced features\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 166MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Model parameters: 6,183,462\nTrain patients: 140, Validation patients: 36\nDataset train: 140 valid patients\nDataset val: 36 valid patients\nData loaders created: 35 train, 3 val batches\nUsing batch size: 16, Workers: 4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Start training\nprint(\"ðŸš€ Starting training...\")\ntrainer = ImprovedTrainer(model, DEVICE, lr=1e-4)\ntrainer.train(train_loader, val_loader, epochs=15)\n\nprint(\"ðŸŽ¯ Training completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T04:37:30.577980Z","iopub.execute_input":"2025-09-05T04:37:30.578270Z","iopub.status.idle":"2025-09-05T04:38:46.542041Z","shell.execute_reply.started":"2025-09-05T04:37:30.578246Z","shell.execute_reply":"2025-09-05T04:38:46.541162Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.27it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15:\nTrain Loss: 20.8499, Val Loss: 8.3679\nTrain Metrics - MSE: 55.3575, RMSE: 7.1712, RÂ²: -0.8363, LLL: -4.7058\nVal Metrics   - MSE: 58.8569, RMSE: 7.2804, RÂ²: -1.0385, LLL: -4.7073\nâœ… New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.62it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15:\nTrain Loss: 4.9857, Val Loss: 2.4407\nTrain Metrics - MSE: 52.9374, RMSE: 7.1170, RÂ²: -0.6516, LLL: -4.7031\nVal Metrics   - MSE: 53.6437, RMSE: 6.9135, RÂ²: -0.8032, LLL: -4.6992\nâœ… New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.13it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15:\nTrain Loss: 2.5966, Val Loss: 2.4206\nTrain Metrics - MSE: 47.6279, RMSE: 6.7179, RÂ²: -0.5522, LLL: -4.6975\nVal Metrics   - MSE: 51.9134, RMSE: 6.7936, RÂ²: -0.7358, LLL: -4.6968\nâœ… New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.70it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/15:\nTrain Loss: 2.2675, Val Loss: 2.4318\nTrain Metrics - MSE: 45.7181, RMSE: 6.5302, RÂ²: -0.4477, LLL: -4.6932\nVal Metrics   - MSE: 48.9706, RMSE: 6.5784, RÂ²: -0.6066, LLL: -4.6921\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.68it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/15:\nTrain Loss: 2.1659, Val Loss: 2.4462\nTrain Metrics - MSE: 42.0486, RMSE: 6.3645, RÂ²: -0.2936, LLL: -4.6886\nVal Metrics   - MSE: 45.8945, RMSE: 6.3384, RÂ²: -0.4675, LLL: -4.6873\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.87it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15:\nTrain Loss: 2.1510, Val Loss: 2.3625\nTrain Metrics - MSE: 38.8802, RMSE: 6.0682, RÂ²: -0.1577, LLL: -4.6839\nVal Metrics   - MSE: 42.0252, RMSE: 6.0237, RÂ²: -0.2952, LLL: -4.6821\nâœ… New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.82it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15:\nTrain Loss: 2.0319, Val Loss: 2.5257\nTrain Metrics - MSE: 35.2183, RMSE: 5.7714, RÂ²: -0.1082, LLL: -4.6797\nVal Metrics   - MSE: 40.0504, RMSE: 5.8479, RÂ²: -0.2055, LLL: -4.6798\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.56it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15:\nTrain Loss: 1.9366, Val Loss: 2.5989\nTrain Metrics - MSE: 32.2159, RMSE: 5.4778, RÂ²: 0.0520, LLL: -4.6747\nVal Metrics   - MSE: 37.7073, RMSE: 5.6441, RÂ²: -0.1016, LLL: -4.6762\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.81it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15:\nTrain Loss: 1.9247, Val Loss: 2.6804\nTrain Metrics - MSE: 31.9921, RMSE: 5.4008, RÂ²: 0.0562, LLL: -4.6714\nVal Metrics   - MSE: 36.7573, RMSE: 5.5663, RÂ²: -0.0638, LLL: -4.6756\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.67it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15:\nTrain Loss: 1.8663, Val Loss: 2.6361\nTrain Metrics - MSE: 30.6661, RMSE: 5.4419, RÂ²: 0.0615, LLL: -4.6698\nVal Metrics   - MSE: 35.6992, RMSE: 5.4745, RÂ²: -0.0171, LLL: -4.6752\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.08it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15:\nTrain Loss: 1.8370, Val Loss: 2.7239\nTrain Metrics - MSE: 29.9174, RMSE: 5.2536, RÂ²: 0.0877, LLL: -4.6680\nVal Metrics   - MSE: 36.1159, RMSE: 5.5247, RÂ²: -0.0481, LLL: -4.6767\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.77it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15:\nTrain Loss: 1.8260, Val Loss: 2.9323\nTrain Metrics - MSE: 30.0045, RMSE: 5.2070, RÂ²: 0.0942, LLL: -4.6670\nVal Metrics   - MSE: 37.2539, RMSE: 5.6133, RÂ²: -0.0946, LLL: -4.6777\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.84it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15:\nTrain Loss: 1.7908, Val Loss: 3.0885\nTrain Metrics - MSE: 29.2810, RMSE: 5.2203, RÂ²: 0.1500, LLL: -4.6655\nVal Metrics   - MSE: 36.2617, RMSE: 5.5219, RÂ²: -0.0448, LLL: -4.6748\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.66it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15:\nTrain Loss: 1.7239, Val Loss: 3.1881\nTrain Metrics - MSE: 28.8702, RMSE: 5.2189, RÂ²: 0.1030, LLL: -4.6640\nVal Metrics   - MSE: 36.1208, RMSE: 5.5065, RÂ²: -0.0344, LLL: -4.6751\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.86it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.05it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/15:\nTrain Loss: 1.7162, Val Loss: 3.0900\nTrain Metrics - MSE: 30.6464, RMSE: 5.3004, RÂ²: 0.0514, LLL: -4.6643\nVal Metrics   - MSE: 34.9573, RMSE: 5.4035, RÂ²: 0.0092, LLL: -4.6732\nðŸŽ¯ Training completed!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =============================================================================\n# PART 9: BASELINE MODEL (LightGBM)\n# =============================================================================\n\nprint(\"Training LightGBM baseline for comparison...\")\n\n# Prepare data for LightGBM\nX = []\ny = []\ngroups = []\n\nfor patient in P:\n    features = TAB[patient]\n    X.append(features)\n    y.append(A[patient])\n    groups.append(patient)\n\nX = np.array(X)\ny = np.array(y)\n\n# Train LightGBM model\nlgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42)\nlgb_model.fit(X, y)\n\n# Evaluate\nlgb_preds = lgb_model.predict(X)\nlgb_mae = np.mean(np.abs(lgb_preds - y))\nlgb_mse = mean_squared_error(y, lgb_preds)\nlgb_rmse = np.sqrt(lgb_mse)\nlgb_r2 = r2_score(y, lgb_preds)\n\nprint(f\"LightGBM Baseline - MAE: {lgb_mae:.4f}, MSE: {lgb_mse:.4f}, RMSE: {lgb_rmse:.4f}, RÂ²: {lgb_r2:.4f}\")\n\n# =============================================================================\n# PART 10: IMPROVED SUBMISSION GENERATION\n# =============================================================================\n\ndef create_improved_submission(model, test_dir, output_file='submission.csv'):\n    \"\"\"Create improved submission file with proper uncertainty handling\"\"\"\n    print(\"Creating improved submission...\")\n    \n    # Load test data\n    test_df = pd.read_csv(DATA_DIR / 'test.csv')\n    submissions = []\n    \n    model.eval()\n    augmentor = MedicalAugmentation(augment=False, target_size=TARGET_SIZE)\n    \n    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n        patient_id = row['Patient']\n        weeks = row['Weeks']\n        \n        try:\n            patient_dir = Path(test_dir) / patient_id\n            \n            # Load 2.5D image\n            img = load_three_slices(patient_dir, target_size=TARGET_SIZE)\n            if img is None:\n                raise ValueError(\"No DICOM files found\")\n                \n            img_tensor = augmentor(img).unsqueeze(0).to(DEVICE)\n            \n            # Get tabular features\n            tab_features = get_enhanced_tabular_features(patient_id, row)\n            tab_tensor = torch.tensor(tab_features).float().unsqueeze(0).to(DEVICE)\n            \n            with torch.no_grad():\n                mean_pred, log_var = model(img_tensor, tab_tensor)\n                fvc_pred = mean_pred.item()\n                sigma = np.sqrt(np.exp(log_var.item()))\n                \n                # Apply competition sigma floor (70) only at submission time\n                confidence = max(sigma, 70.0)\n            \n            # For each required week in the test set\n            patient_week = f\"{patient_id}_{weeks}\"\n            \n            # Use the model's prediction directly (no slope adjustment)\n            submissions.append({\n                'Patient_Week': patient_week,\n                'FVC': fvc_pred,\n                'Confidence': confidence\n            })\n                \n        except Exception as e:\n            # Fallback to baseline prediction\n            if patient_id in TAB:\n                tab_features = TAB[patient_id]\n                fvc_pred = lgb_model.predict(tab_features.reshape(1, -1))[0]\n            else:\n                fvc_pred = 2500  # Average FVC\n                \n            submissions.append({\n                'Patient_Week': f\"{patient_id}_{weeks}\",\n                'FVC': fvc_pred,\n                'Confidence': 200.0  # Conservative uncertainty\n            })\n    \n    # Create submission file\n    submission_df = pd.DataFrame(submissions)\n    submission_df.to_csv(output_file, index=False)\n    print(f\"Submission saved to {output_file}\")\n    return submission_df\n\n# Generate submission\nif test_df is not None and TEST_DIR.exists():\n    submission = create_improved_submission(model, TEST_DIR, 'submission.csv')\n    print(\"âœ… Submission ready!\")\n    print(submission.head())\nelse:\n    print(\"No test data found - skipping submission\")\n\nprint(\"ðŸŽ‰ All done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T04:38:46.543462Z","iopub.execute_input":"2025-09-05T04:38:46.543721Z","iopub.status.idle":"2025-09-05T04:38:46.770721Z","shell.execute_reply.started":"2025-09-05T04:38:46.543693Z","shell.execute_reply":"2025-09-05T04:38:46.769987Z"}},"outputs":[{"name":"stdout","text":"Training LightGBM baseline for comparison...\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000076 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 308\n[LightGBM] [Info] Number of data points in the train set: 176, number of used features: 9\n[LightGBM] [Info] Start training from score -4.524301\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nLightGBM Baseline - MAE: 0.7934, MSE: 1.9886, RMSE: 1.4102, RÂ²: 0.9469\nCreating improved submission...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 28.19it/s]","output_type":"stream"},{"name":"stdout","text":"Submission saved to submission.csv\nâœ… Submission ready!\n                   Patient_Week       FVC  Confidence\n0   ID00419637202311204720264_6 -2.044669       200.0\n1  ID00421637202311550012437_15 -2.059186       200.0\n2   ID00422637202311677017371_6 -4.415662       200.0\n3  ID00423637202312137826377_17 -9.395618       200.0\n4   ID00426637202313170790466_0 -0.821343       200.0\nðŸŽ‰ All done!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}

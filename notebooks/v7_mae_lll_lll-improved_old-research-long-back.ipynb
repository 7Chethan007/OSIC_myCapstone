{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Progresive Training","metadata":{}},{"cell_type":"markdown","source":"**mae based**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imports and environment setup\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport cv2\nimport pydicom\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom sklearn.model_selection import train_test_split\n\n# Set seeds for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything()\n\n# Paths and device setup\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nTEST_DIR = DATA_DIR / \"test\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# Load train.csv\ntrain_df = pd.read_csv(DATA_DIR / \"train.csv\")\nprint(f\"Loaded train dataset: {train_df.shape}\")\n\n# Tabular feature extraction function\ndef get_tab_features(row):\n    age_scaled = (row['Age'] - 30) / 30\n    sex_encoded = 0 if row['Sex'] == 'Male' else 1\n    smoking = row['SmokingStatus']\n    smoke_map = {\n        'Never smoked': [0,0],\n        'Ex-smoker': [1,1],\n        'Currently smokes': [0,1]\n    }\n    smoke_vec = smoke_map.get(smoking, [1,0])\n    return np.array([age_scaled, sex_encoded, smoke_vec[0], smoke_vec[1]])\n\n# Compute linear decay coefficients per patient\nA = {}\nTAB = {}\nP = []\nprint(\"Calculating linear decay coefficients per patient...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, _ = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0])\n    else:\n        A[patient] = 0.0\n    TAB[patient] = get_tab_features(sub.iloc[0])\n    P.append(patient)\nprint(f\"Processed {len(P)} patients\")\n\n# Medical Image Augmentations\nclass MedicalAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=15, p=0.7),\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                albu.GaussNoise(var_limit=(10, 50), p=0.5),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\n# Dataset class\nclass OSICDenseNetDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = MedicalAugmentation(augment=augment)\n        self.patient_images = {}\n        for p in self.patients:\n            p_dir = self.data_dir / p\n            if p_dir.exists():\n                images = [f for f in p_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if images:\n                    self.patient_images[p] = images\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 6  # augment repeat\n        else:\n            return len(self.valid_patients)\n    def __getitem__(self, idx):\n        patient_idx = idx % len(self.valid_patients) if self.split == 'train' else idx\n        patient = self.valid_patients[patient_idx]\n        images = self.patient_images[patient]\n        img_path = np.random.choice(images) if len(images) > 1 else images[0]\n        img = self.load_and_preprocess_dicom(img_path)\n        img_tensor = self.augmentor(img)\n        tab = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab, target, patient\n    def load_and_preprocess_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape) == 3:\n                img = img[img.shape[0] // 2]\n            img = cv2.resize(img, (512, 512))\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\n# Spatial Attention Module\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        attn = torch.cat([avg_out, max_out], dim=1)\n        attn = self.conv1(attn)\n        attn = self.sigmoid(attn)\n        return x * attn   # ✅ multiply with original features, not with reduced map\n\n\n# Model with spatial attention + cross-modal attention + quantile regression output\nclass WorkingDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=4, dropout_rate=0.4):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        self.spatial_attention = SpatialAttention()\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU()\n        )\n        embed_dim = 1024\n        self.query_proj = nn.Linear(embed_dim, embed_dim)\n        self.key_proj = nn.Linear(512, embed_dim)\n        self.value_proj = nn.Linear(512, embed_dim)\n        self.cross_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2, batch_first=True)\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(embed_dim + 512, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate / 2)\n        )\n        # Main prediction head (mean FVC)\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        # Quantile regression heads\n        self.lower_quantile_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.upper_quantile_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        img_features = self.features(images)\n        img_features = self.spatial_attention(img_features)\n        img_features = F.adaptive_avg_pool2d(img_features, (1, 1)).view(batch_size, -1)  # (B, 1024)\n        tab_features = self.tabular_processor(tabular)  # (B, 512)\n        queries = self.query_proj(img_features).unsqueeze(1)  # (B, 1, 1024)\n        keys = self.key_proj(tab_features).unsqueeze(1)       # (B, 1, 1024)\n        values = self.value_proj(tab_features).unsqueeze(1)   # (B, 1, 1024)\n        attended, _ = self.cross_attention(query=queries, key=keys, value=values)\n        attended = attended.squeeze(1)  # (B, 1024)\n        combined = torch.cat([attended, tab_features], dim=1)  # (B, 1536)\n        fused = self.fusion_layer(combined)                    # (B, 256)\n        mean_fvc = self.mean_head(fused).squeeze(1)            # (B,)\n        lower_q = self.lower_quantile_head(fused).squeeze(1)   # (B,)\n        upper_q = self.upper_quantile_head(fused).squeeze(1)   # (B,)\n        return mean_fvc, lower_q, upper_q\n\n# Quantile loss implementation\ndef quantile_loss(predictions, targets, quantile):\n    errors = targets - predictions\n    return torch.mean(torch.max(quantile * errors, (quantile - 1) * errors))\n\n# Combined loss for training\ndef combined_loss(mean_pred, lower_pred, upper_pred, target):\n    mse = F.mse_loss(mean_pred, target)\n    lower_loss = quantile_loss(lower_pred, target, 0.2)\n    upper_loss = quantile_loss(upper_pred, target, 0.8)\n    return mse + lower_loss + upper_loss\n\n# R² score implementation in PyTorch\ndef r2_score_torch(y_pred, y_true):\n    ss_res = torch.sum((y_true - y_pred) ** 2)\n    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n    return 1 - ss_res / (ss_tot + 1e-8)\n\n# Dataset splitting for training/validation\ntrain_patients, val_patients = train_test_split(P, test_size=0.2, random_state=42)\n\n# Instantiate datasets & loaders\ntrain_dataset = OSICDenseNetDataset(train_patients, A, TAB, TRAIN_DIR, split='train', augment=True)\nval_dataset = OSICDenseNetDataset(val_patients, A, TAB, TRAIN_DIR, split='val', augment=False)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\nprint(f\"Training with {len(train_loader)} batches, validating with {len(val_loader)} batches\")\n\n# Training loop class\nclass Trainer:\n    def __init__(self, model, device, learning_rate=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = learning_rate\n        self.best_val_mae = float('inf')\n    def train(self, train_loader, val_loader, epochs=30, patience=8):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, verbose=True)\n        patience_counter = 0\n        for epoch in range(epochs):\n            self.model.train()\n            train_losses, train_maes, train_r2s = [], [], []\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, lower_pred, upper_pred = self.model(images, tabular)\n                loss = combined_loss(mean_pred, lower_pred, upper_pred, targets)\n                mae = F.l1_loss(mean_pred, targets)\n                r2 = r2_score_torch(mean_pred, targets)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_losses.append(loss.item())\n                train_maes.append(mae.item())\n                train_r2s.append(r2.item())\n            avg_train_loss = np.mean(train_losses)\n            avg_train_mae = np.mean(train_maes)\n            avg_train_r2 = np.mean(train_r2s)\n            # Validation\n            self.model.eval()\n            val_losses, val_maes, val_r2s = [], [], []\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, lower_pred, upper_pred = self.model(images, tabular)\n                    loss = combined_loss(mean_pred, lower_pred, upper_pred, targets)\n                    mae = F.l1_loss(mean_pred, targets)\n                    r2 = r2_score_torch(mean_pred, targets)\n                    val_losses.append(loss.item())\n                    val_maes.append(mae.item())\n                    val_r2s.append(r2.item())\n            avg_val_loss = np.mean(val_losses)\n            avg_val_mae = np.mean(val_maes)\n            avg_val_r2 = np.mean(val_r2s)\n            print(f\"Epoch {epoch+1}/{epochs} - \"\n                  f\"Train loss: {avg_train_loss:.4f}, MAE: {avg_train_mae:.4f}, R²: {avg_train_r2:.4f} | \"\n                  f\"Val loss: {avg_val_loss:.4f}, MAE: {avg_val_mae:.4f}, R²: {avg_val_r2:.4f}\")\n            scheduler.step(avg_val_mae)\n            if avg_val_mae < self.best_val_mae:\n                self.best_val_mae = avg_val_mae\n                torch.save(self.model.state_dict(), \"best_working_model.pth\")\n                print(\"Best model saved!\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n        return self.best_val_mae\n\n# Instantiate model and trainer\nmodel = WorkingDenseNetModel(tabular_dim=4).to(DEVICE)\ntrainer = Trainer(model, DEVICE, learning_rate=1e-4)\n\n# Start training\nbest_mae = trainer.train(train_loader, val_loader, epochs=30, patience=8)\nprint(f\"Training complete with best validation MAE: {best_mae:.4f}\")\n\n# Test-time augmentation predictor\nclass TTAPredictor:\n    def __init__(self, model, augmentor, device, num_augmentations=5):\n        self.model = model\n        self.augmentor = augmentor\n        self.device = device\n        self.num_augmentations = num_augmentations\n        self.model.eval()\n    def predict(self, image, tabular_features):\n        means, lowers, uppers = [], [], []\n        with torch.no_grad():\n            img_tensor = self.augmentor(image=image)['image'].to(self.device)\n            tab_tensor = torch.tensor(tabular_features).float().unsqueeze(0).to(self.device)\n            m, l, u = self.model(img_tensor.unsqueeze(0), tab_tensor)\n            means.append(m.item())\n            lowers.append(l.item())\n            uppers.append(u.item())\n            for _ in range(self.num_augmentations):\n                aug_img = self.augmentor(image=image)['image'].to(self.device)\n                m, l, u = self.model(aug_img.unsqueeze(0), tab_tensor)\n                means.append(m.item())\n               \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T14:10:45.706555Z","iopub.execute_input":"2025-09-11T14:10:45.706893Z","iopub.status.idle":"2025-09-11T14:18:13.508995Z","shell.execute_reply.started":"2025-09-11T14:10:45.706866Z","shell.execute_reply":"2025-09-11T14:18:13.508053Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded train dataset: (1549, 7)\nCalculating linear decay coefficients per patient...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 176/176 [00:00<00:00, 1352.89it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 138 patients with images\nDataset val: 36 patients with images\nTraining with 103 batches, validating with 5 batches\nEpoch 1/30 - Train loss: 51.0877, MAE: 5.0442, R²: -0.9643 | Val loss: 68.2587, MAE: 5.7449, R²: -0.5497\nBest model saved!\nEpoch 2/30 - Train loss: 38.4311, MAE: 4.2857, R²: -0.3855 | Val loss: 56.0758, MAE: 5.0694, R²: -0.1019\nBest model saved!\nEpoch 3/30 - Train loss: 33.4124, MAE: 4.0172, R²: -0.1599 | Val loss: 52.8034, MAE: 4.9098, R²: -0.0648\nBest model saved!\nEpoch 4/30 - Train loss: 32.6211, MAE: 3.9740, R²: -0.2348 | Val loss: 54.1547, MAE: 4.9816, R²: -0.0732\nEpoch 5/30 - Train loss: 32.4139, MAE: 4.0162, R²: -0.1434 | Val loss: 53.8372, MAE: 4.9236, R²: -0.0445\nEpoch 6/30 - Train loss: 32.5289, MAE: 4.0367, R²: -0.2262 | Val loss: 56.7123, MAE: 5.0993, R²: -0.0964\nEpoch 7/30 - Train loss: 32.4337, MAE: 4.0171, R²: -0.1252 | Val loss: 53.8248, MAE: 4.9535, R²: -0.0692\nEpoch 8/30 - Train loss: 32.1055, MAE: 3.9782, R²: -0.1686 | Val loss: 55.7477, MAE: 5.0227, R²: -0.0938\nEpoch 9/30 - Train loss: 31.6708, MAE: 3.9354, R²: -0.1461 | Val loss: 53.2058, MAE: 4.8980, R²: -0.0542\nBest model saved!\nEpoch 10/30 - Train loss: 32.4052, MAE: 3.9847, R²: -0.1151 | Val loss: 53.8397, MAE: 4.9802, R²: -0.0641\nEpoch 11/30 - Train loss: 31.8705, MAE: 3.9719, R²: -0.1530 | Val loss: 53.7641, MAE: 5.0084, R²: -0.0784\nEpoch 12/30 - Train loss: 32.1941, MAE: 4.0143, R²: -0.1394 | Val loss: 53.1080, MAE: 4.9725, R²: -0.0853\nEpoch 13/30 - Train loss: 31.7438, MAE: 3.9041, R²: -0.1113 | Val loss: 54.2704, MAE: 5.0011, R²: -0.0739\nEpoch 14/30 - Train loss: 32.3163, MAE: 4.0052, R²: -0.1420 | Val loss: 54.2770, MAE: 5.0391, R²: -0.0507\nEpoch 15/30 - Train loss: 31.4384, MAE: 3.9847, R²: -0.1119 | Val loss: 54.8450, MAE: 5.0645, R²: -0.0749\nEpoch 16/30 - Train loss: 31.8997, MAE: 3.9679, R²: -0.1454 | Val loss: 55.3889, MAE: 5.0948, R²: -0.0707\nEpoch 17/30 - Train loss: 32.4410, MAE: 4.0101, R²: -0.1846 | Val loss: 54.9193, MAE: 4.9874, R²: -0.0586\nEarly stopping at epoch 17\nTraining complete with best validation MAE: 4.8980\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**LLL based**\nJust a small typo in the end","metadata":{}},{"cell_type":"code","source":"# Imports and environment setup\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\nfrom tqdm import tqdm\n\nimport cv2\nimport pydicom\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\n\n# Set seeds for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()\n\n# Paths and device setup\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nTEST_DIR = DATA_DIR / \"test\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# Load train.csv\ntrain_df = pd.read_csv(DATA_DIR / \"train.csv\")\nprint(f\"Loaded train dataset: {train_df.shape}\")\n\n# Tabular feature extraction function\ndef get_tab_features(row):\n    age_scaled = (row['Age'] - 30) / 30\n    sex_encoded = 0 if row['Sex'] == 'Male' else 1\n    smoking = row['SmokingStatus']\n    smoke_map = {\n        'Never smoked': [0,0],\n        'Ex-smoker': [1,1],\n        'Currently smokes': [0,1]\n    }\n    smoke_vec = smoke_map.get(smoking, [1,0])\n    return np.array([age_scaled, sex_encoded, smoke_vec[0], smoke_vec[1]])\n\n# Compute linear decay coefficients per patient\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating linear decay coefficients per patient...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n\n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, _ = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0])\n    else:\n        A[patient] = 0.0\n    TAB[patient] = get_tab_features(sub.iloc[0])\n    P.append(patient)\n\nprint(f\"Processed {len(P)} patients\")\n\n# Medical Image Augmentations\nclass MedicalAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=15, p=0.7),\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                albu.GaussNoise(var_limit=(10, 50), p=0.5),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\n# Dataset class\nclass OSICDenseNetDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = MedicalAugmentation(augment=augment)\n        self.patient_images = {}\n        for p in self.patients:\n            p_dir = self.data_dir / p\n            if p_dir.exists():\n                images = [f for f in p_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if images:\n                    self.patient_images[p] = images\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n\n    def __len__(self):\n        if self.split == 'train':\n            return len(self.valid_patients) * 6  # augment repeat\n        else:\n            return len(self.valid_patients)\n\n    def __getitem__(self, idx):\n        patient_idx = idx % len(self.valid_patients) if self.split == 'train' else idx\n        patient = self.valid_patients[patient_idx]\n        images = self.patient_images[patient]\n        img_path = np.random.choice(images) if len(images) > 1 else images[0]\n        img = self.load_and_preprocess_dicom(img_path)\n        img_tensor = self.augmentor(img)\n        tab = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab, target, patient\n\n    def load_and_preprocess_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape) == 3:\n                img = img[img.shape[0] // 2]\n            img = cv2.resize(img, (512, 512))\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\n# Model with spatial attention + cross-modal attention + quantile regression output\nclass WorkingDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim=4, dropout_rate=0.4):\n        super().__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        self.spatial_attention = SpatialAttention()\n\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU()\n        )\n\n        embed_dim = 1024\n        self.query_proj = nn.Linear(embed_dim, embed_dim)\n        self.key_proj = nn.Linear(512, embed_dim)\n        self.value_proj = nn.Linear(512, embed_dim)\n\n        self.cross_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2, batch_first=True)\n\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(embed_dim + 512, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate / 2)\n        )\n\n        # Main prediction head (mean FVC)\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n        # Quantile regression heads\n        self.lower_quantile_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.upper_quantile_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, images, tabular):\n        batch_size = images.size(0)\n        img_features = self.features(images)\n        img_features = self.spatial_attention(img_features)\n        img_features = F.adaptive_avg_pool2d(img_features, (1, 1)).view(batch_size, -1)  # (B, 1024)\n\n        tab_features = self.tabular_processor(tabular)  # (B, 512)\n\n        queries = self.query_proj(img_features).unsqueeze(1)  # (B, 1, 1024)\n        keys = self.key_proj(tab_features).unsqueeze(1)       # (B, 1, 1024)\n        values = self.value_proj(tab_features).unsqueeze(1)   # (B, 1, 1024)\n\n        attended, _ = self.cross_attention(query=queries, key=keys, value=values)\n        attended = attended.squeeze(1)  # (B, 1024)\n\n        combined = torch.cat([attended, tab_features], dim=1)  # (B, 1536)\n        fused = self.fusion_layer(combined)                    # (B, 256)\n\n        mean_fvc = self.mean_head(fused).squeeze(1)            # (B,)\n        lower_q = self.lower_quantile_head(fused).squeeze(1)   # (B,)\n        upper_q = self.upper_quantile_head(fused).squeeze(1)   # (B,)\n\n        return mean_fvc, lower_q, upper_q\n\n# Quantile loss implementation\ndef quantile_loss(predictions, targets, quantile):\n    errors = targets - predictions\n    return torch.mean(torch.max(quantile * errors, (quantile - 1) * errors))\n\n# Combined loss for training\ndef combined_loss(mean_pred, lower_pred, upper_pred, target):\n    mse = F.mse_loss(mean_pred, target)\n    lower_loss = quantile_loss(lower_pred, target, 0.2)\n    upper_loss = quantile_loss(upper_pred, target, 0.8)\n    return mse + lower_loss + upper_loss\n\n# Dataset splitting for training/validation\ntrain_patients, val_patients = train_test_split(P, test_size=0.2, random_state=42)\n\n# Instantiate datasets & loaders\ntrain_dataset = OSICDenseNetDataset(train_patients, A, TAB, TRAIN_DIR, split='train', augment=True)\nval_dataset = OSICDenseNetDataset(val_patients, A, TAB, TRAIN_DIR, split='val', augment=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Training with {len(train_loader)} batches, validating with {len(val_loader)} batches\")\n\n# Training loop class\nclass Trainer:\n    def __init__(self, model, device, learning_rate=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = learning_rate\n        self.best_val_lll = -float('inf')  # Track best (max) LLL\n\n    def laplace_log_likelihood_metric(self, pred_mean, pred_lower, pred_upper, targets):\n        # Approximate log variance from quantile upper/lower bounds, avoiding direct use of predicted variance\n        sigma = (pred_upper - pred_lower) / 2\n        sigma = np.clip(sigma, 1e-3, 1e3)\n        delta = np.abs(pred_mean - targets)\n        score = -np.log(2 * sigma) - delta / sigma\n        return np.mean(score)\n\n    def train(self, train_loader, val_loader, epochs=30, patience=8):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=4, verbose=True)\n        patience_counter = 0\n\n        for epoch in range(epochs):\n            self.model.train()\n            train_losses, train_maes = [], []\n\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, lower_pred, upper_pred = self.model(images, tabular)\n                loss = combined_loss(mean_pred, lower_pred, upper_pred, targets)\n                mae = F.l1_loss(mean_pred, targets)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_losses.append(loss.item())\n                train_maes.append(mae.item())\n\n            avg_train_loss = np.mean(train_losses)\n            avg_train_mae = np.mean(train_maes)\n\n            # Validation\n            self.model.eval()\n            val_losses, val_maes = [], []\n            val_pred_means = []\n            val_pred_lowers = []\n            val_pred_uppers = []\n            val_targets_list = []\n\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, lower_pred, upper_pred = self.model(images, tabular)\n                    loss = combined_loss(mean_pred, lower_pred, upper_pred, targets)\n                    mae = F.l1_loss(mean_pred, targets)\n\n                    val_losses.append(loss.item())\n                    val_maes.append(mae.item())\n\n                    val_pred_means.extend(mean_pred.cpu().numpy())\n                    val_pred_lowers.extend(lower_pred.cpu().numpy())\n                    val_pred_uppers.extend(upper_pred.cpu().numpy())\n                    val_targets_list.extend(targets.cpu().numpy())\n\n            val_pred_means = np.array(val_pred_means)\n            val_pred_lowers = np.array(val_pred_lowers)\n            val_pred_uppers = np.array(val_pred_uppers)\n            val_targets_list = np.array(val_targets_list)\n\n            avg_val_loss = np.mean(val_losses)\n            avg_val_mae = np.mean(val_maes)\n\n            # Calculate LLL and R^2\n            val_lll = self.laplace_log_likelihood_metric(val_pred_means, val_pred_lowers, val_pred_uppers, val_targets_list)\n            ss_res = np.sum((val_targets_list - val_pred_means) ** 2)\n            ss_tot = np.sum((val_targets_list - np.mean(val_targets_list)) ** 2)\n            val_r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else float('-inf')\n\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            print(f\"Train Loss: {avg_train_loss:.6f}, MAE: {avg_train_mae:.6f}\")\n            print(f\"Val Loss: {avg_val_loss:.6f}, MAE: {avg_val_mae:.6f}, R²: {val_r2:.6f}, LLL (main): {val_lll:.6f}\")\n\n            # Scheduler step and model save based on max LLL\n            scheduler.step(val_lll)\n            if val_lll > self.best_val_lll:\n                self.best_val_lll = val_lll\n                torch.save(self.model.state_dict(), 'best_working_model.pth')\n                print(\"✅ New best model saved (based on LLL)!\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n\n        return self.best_val_lll\n\n\n# Instantiate model and trainer\nmodel = WorkingDenseNetModel(tabular_dim=4).to(DEVICE)\ntrainer = Trainer(model, DEVICE, learning_rate=1e-4)\n\n# Start training\nbest_mae = trainer.train(train_loader, val_loader, epochs=30, patience=8)\nprint(f\"Training complete with best validation MAE: {best_mae:.4f}\")\n\n# Test-time augmentation predictor\nclass TTAPredictor:\n    def __init__(self, model, augmentor, device, num_augmentations=5):\n        self.model = model\n        self.augmentor = augmentor\n        self.device = device\n        self.num_augmentations = num_augmentations\n        self.model.eval()\n\n    def predict(self, image, tabular_features):\n        means, lowers, uppers = [], [], []\n        with torch.no_grad():\n            img_tensor = self.augmentor(image=image)['image'].to(self.device)\n            tab_tensor = torch.tensor(tabular_features).float().unsqueeze(0).to(self.device)\n\n            m, l, u = self.model(img_tensor.unsqueeze(0), tab_tensor)\n            means.append(m.item())\n            lowers.append(l.item())\n            uppers.append(u.item())\n\n            for _ in range(self.num_augmentations):\n                aug_img = self.augmentor(image=image)['image'].to(self.device)\n                m, l, u = self.model(aug_img.unsqueeze(0), tab_tensor)\n                means.append(m.item())\n                lowers.append(l.item())\n                uppers.append(u.item())\n\n        mean_pred = np.median(means)\n        lower_pred = np.median(lowers)\n        upper_pred = np.median(uppers)\n        confidence = upper_pred - lower_pred\n        return mean_pred, confidence\n\ntest_augmentor = MedicalAugmentation(augment=False)\ntta_predictor = TTAPredictor(model, test_augmentor, DEVICE)\n\n# Optional: implement submission function, evaluation, or extend with confidence heads per your requirement\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:47:50.967715Z","iopub.execute_input":"2025-09-11T13:47:50.968403Z","iopub.status.idle":"2025-09-11T13:54:26.695405Z","shell.execute_reply.started":"2025-09-11T13:47:50.968372Z","shell.execute_reply":"2025-09-11T13:54:26.694193Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded train dataset: (1549, 7)\nCalculating linear decay coefficients per patient...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 176/176 [00:00<00:00, 1305.71it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 138 patients with images\nDataset val: 36 patients with images\nTraining with 103 batches, validating with 5 batches\nEpoch 1/30\nTrain Loss: 51.087677, MAE: 5.044207\nVal Loss: 68.258679, MAE: 5.744914, R²: -0.279940, LLL (main): -9.970015\n✅ New best model saved (based on LLL)!\nEpoch 2/30\nTrain Loss: 38.431060, MAE: 4.285720\nVal Loss: 56.075777, MAE: 5.069439, R²: -0.077374, LLL (main): -3.848270\n✅ New best model saved (based on LLL)!\nEpoch 3/30\nTrain Loss: 33.412413, MAE: 4.017181\nVal Loss: 52.803370, MAE: 4.909763, R²: -0.006549, LLL (main): -3.456856\n✅ New best model saved (based on LLL)!\nEpoch 4/30\nTrain Loss: 32.621137, MAE: 3.973981\nVal Loss: 54.154743, MAE: 4.981632, R²: -0.034543, LLL (main): -3.431411\n✅ New best model saved (based on LLL)!\nEpoch 5/30\nTrain Loss: 32.413872, MAE: 4.016197\nVal Loss: 53.837245, MAE: 4.923565, R²: -0.033564, LLL (main): -3.517206\nEpoch 6/30\nTrain Loss: 32.528891, MAE: 4.036727\nVal Loss: 56.712250, MAE: 5.099263, R²: -0.095244, LLL (main): -3.639167\nEpoch 7/30\nTrain Loss: 32.433672, MAE: 4.017052\nVal Loss: 53.824781, MAE: 4.953484, R²: -0.029279, LLL (main): -3.429345\n✅ New best model saved (based on LLL)!\nEpoch 8/30\nTrain Loss: 32.105521, MAE: 3.978194\nVal Loss: 55.747689, MAE: 5.022667, R²: -0.072765, LLL (main): -3.540575\nEpoch 9/30\nTrain Loss: 31.814941, MAE: 3.947483\nVal Loss: 52.731729, MAE: 4.927510, R²: -0.004846, LLL (main): -3.458482\nEpoch 10/30\nTrain Loss: 32.748876, MAE: 4.008094\nVal Loss: 54.326962, MAE: 5.027144, R²: -0.036309, LLL (main): -3.481529\nEpoch 11/30\nTrain Loss: 31.978274, MAE: 3.995310\nVal Loss: 53.007717, MAE: 4.980703, R²: -0.003765, LLL (main): -3.491876\nEpoch 12/30\nTrain Loss: 32.494949, MAE: 4.048956\nVal Loss: 52.531683, MAE: 4.970708, R²: 0.006047, LLL (main): -3.465579\nEpoch 13/30\nTrain Loss: 32.026494, MAE: 3.932927\nVal Loss: 53.461370, MAE: 4.947028, R²: -0.018935, LLL (main): -3.478791\nEpoch 14/30\nTrain Loss: 32.251473, MAE: 4.022737\nVal Loss: 53.931179, MAE: 4.993780, R²: -0.032854, LLL (main): -3.435563\nEpoch 15/30\nTrain Loss: 31.880619, MAE: 4.007133\nVal Loss: 54.098784, MAE: 4.980486, R²: -0.035028, LLL (main): -3.451183\nEarly stopping at epoch 15\nTraining complete with best validation MAE: -3.4293\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1st lll trying to push r^2 to be between 0.1 to 0.5**","metadata":{}},{"cell_type":"code","source":"# Imports and environment setup\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport cv2\nimport pydicom\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\n\n# Seed for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()\n\n# Paths and device\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nTEST_DIR = DATA_DIR / \"test\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# Load train.csv\ntrain_df = pd.read_csv(DATA_DIR / \"train.csv\")\nprint(f\"Loaded train dataset: {train_df.shape}\")\n\n# Tabular feature extraction\ndef get_tab_features(row):\n    age_scaled = (row['Age'] - 30) / 30\n    sex_encoded = 0 if row['Sex'] == 'Male' else 1\n    smoke_map = {\n        'Never smoked': [0, 0],\n        'Ex-smoker': [1, 1],\n        'Currently smokes': [0, 1]\n    }\n    smoking = row['SmokingStatus']\n    smoke_vec = smoke_map.get(smoking, [1, 0])\n    return np.array([age_scaled, sex_encoded, smoke_vec[0], smoke_vec[1]])\n\n# Compute linear decay coefficients\nA = {}\nTAB = {}\nP = []\n\nprint(\"Calculating linear decay coefficients per patient...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, _ = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0])\n    else:\n        A[patient] = 0.0\n    TAB[patient] = get_tab_features(sub.iloc[0])\n    P.append(patient)\nprint(f\"Processed {len(P)} patients.\")\n\n# Medical Image Augmentation\nclass MedicalAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=15, p=0.7),\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                albu.GaussNoise(var_limit=(10, 50), p=0.5),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\n# Dataset\nclass OSICDenseNetDataset(Dataset):\n    def __init__(self, patients, A_dict, TAB_dict, data_dir, split='train', augment=True):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = MedicalAugmentation(augment=augment)\n        self.patient_images = {}\n        for p in self.patients:\n            pd = self.data_dir / p\n            if pd.exists():\n                imgs = [f for f in pd.iterdir() if f.suffix.lower() == '.dcm']\n                if imgs:\n                    self.patient_images[p] = imgs\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n\n    def __len__(self):\n        return len(self.valid_patients) * 6 if self.split == 'train' else len(self.valid_patients)\n\n    def __getitem__(self, idx):\n        idx_mod = idx % len(self.valid_patients) if self.split == 'train' else idx\n        patient = self.valid_patients[idx_mod]\n        images = self.patient_images[patient]\n        img_path = np.random.choice(images) if len(images) > 1 else images[0]\n        img = self.load_and_preprocess_dicom(img_path)\n        img_tensor = self.augmentor(img)\n        tab_feat = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        tgt = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_feat, tgt, patient\n\n    def load_and_preprocess_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape) == 3:\n                img = img[img.shape[0] // 2]\n            img = cv2.resize(img, (512, 512))\n            mn, mx = img.min(), img.max()\n            if mx > mn:\n                img = (img - mn) / (mx - mn) * 255\n            else:\n                img = np.zeros_like(img)\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\n# Attention Module\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        x_cat = self.conv1(x_cat)\n        return x * self.sigmoid(x_cat)\n\n# Model with uncertainty added as auxiliary output\nclass WorkingDenseNetModel(nn.Module):\n    def __init__(self, tab_dim=4, dropout=0.4):\n        super().__init__()\n        dnet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = dnet.features\n        self.spatial_attention = SpatialAttention()\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tab_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU()\n        )\n        edim = 1024\n        self.query_proj = nn.Linear(edim, edim)\n        self.key_proj = nn.Linear(512, edim)\n        self.value_proj = nn.Linear(512, edim)\n        self.cross_attention = nn.MultiheadAttention(embed_dim=edim, num_heads=8, dropout=0.2, batch_first=True)\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(edim + 512, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(dropout / 2)\n        )\n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.uncertainty_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 1),\n            nn.Softplus()  # ensures positive uncertainty\n        )\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_feat = self.features(images)\n        img_feat = self.spatial_attention(img_feat)\n        img_feat = F.adaptive_avg_pool2d(img_feat, (1, 1)).view(b, -1)\n        tab_feat = self.tabular_processor(tabular)\n        q = self.query_proj(img_feat).unsqueeze(1)\n        k = self.key_proj(tab_feat).unsqueeze(1)\n        v = self.value_proj(tab_feat).unsqueeze(1)\n        attn_out, _ = self.cross_attention(q, k, v)\n        attn_out = attn_out.squeeze(1)\n        combined = torch.cat([attn_out, tab_feat], dim=1)\n        fused_feat = self.fusion_layer(combined)\n        mean_pred = self.mean_head(fused_feat).squeeze(1)\n        uncertainty = self.uncertainty_head(fused_feat).squeeze(1)\n        return mean_pred, uncertainty\n\n# Loss combining MSE as primary and uncertainty weighted auxiliary loss\ndef combined_loss(mean_pred, uncertainty, targets, unc_weight=0.1):\n    mse = F.mse_loss(mean_pred, targets)\n    # Encourage uncertainty to reflect prediction error\n    unc_loss = torch.mean(((mean_pred - targets).pow(2) / (uncertainty + 1e-6)) + torch.log(uncertainty + 1e-6))\n    return mse + unc_weight * unc_loss\n\n# Laplace Log Likelihood metric for reporting\ndef laplace_log_likelihood_metric(mean_pred, uncertainty, targets):\n    sigma = np.sqrt(uncertainty)\n    sigma = np.clip(sigma, 1e-3, 1e3)\n    delta = np.abs(mean_pred - targets)\n    score = -np.log(2 * sigma) - delta / sigma\n    return np.mean(score)\n\n# Dataset split\nfrom sklearn.model_selection import train_test_split\ntrain_patients, val_patients = train_test_split(P, test_size=0.2, random_state=42)\n\ntrain_dataset = OSICDenseNetDataset(train_patients, A, TAB, TRAIN_DIR, split='train', augment=True)\nval_dataset = OSICDenseNetDataset(val_patients, A, TAB, TRAIN_DIR, split='val', augment=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Training with {len(train_loader)} batches, validating with {len(val_loader)} batches.\")\n\n# Trainer class with MSE+uncertainty loss and LLL + R² reporting\nclass Trainer:\n    def __init__(self, model, device, lr=1e-4):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.best_val_lll = -float('inf')\n\n    def train(self, train_loader, val_loader, epochs=30, patience=8):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=4, verbose=True\n        )\n        patience_counter = 0\n\n        for epoch in range(epochs):\n            self.model.train()\n            train_losses, train_maes = [], []\n\n            for images, tabular, targets, _ in train_loader:\n                images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                mean_pred, uncertainty = self.model(images, tabular)\n                loss = combined_loss(mean_pred, uncertainty, targets)\n                mae = F.l1_loss(mean_pred, targets)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_losses.append(loss.item())\n                train_maes.append(mae.item())\n\n            avg_train_loss = np.mean(train_losses)\n            avg_train_mae = np.mean(train_maes)\n\n            self.model.eval()\n            val_losses, val_maes = [], []\n            val_preds, val_vars, val_targets = [], [], []\n\n            with torch.no_grad():\n                for images, tabular, targets, _ in val_loader:\n                    images, tabular, targets = images.to(self.device), tabular.to(self.device), targets.to(self.device)\n                    mean_pred, uncertainty = self.model(images, tabular)\n                    loss = combined_loss(mean_pred, uncertainty, targets)\n                    mae = F.l1_loss(mean_pred, targets)\n\n                    val_losses.append(loss.item())\n                    val_maes.append(mae.item())\n                    val_preds.extend(mean_pred.cpu().numpy())\n                    val_vars.extend(uncertainty.cpu().numpy())\n                    val_targets.extend(targets.cpu().numpy())\n\n            val_preds = np.array(val_preds)\n            val_vars = np.array(val_vars)\n            val_targets = np.array(val_targets)\n\n            avg_val_loss = np.mean(val_losses)\n            avg_val_mae = np.mean(val_maes)\n\n            # Calculate LLL and R2\n            val_lll = laplace_log_likelihood_metric(val_preds, val_vars, val_targets)\n            ss_res = np.sum((val_targets - val_preds) ** 2)\n            ss_tot = np.sum((val_targets - np.mean(val_targets)) ** 2)\n            val_r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else float('-inf')\n\n            print(\n                f\"Epoch {epoch+1}/{epochs} | \"\n                f\"Train Loss: {avg_train_loss:.4f}, MAE: {avg_train_mae:.4f} | \"\n                f\"Val Loss: {avg_val_loss:.4f}, MAE: {avg_val_mae:.4f}, R²: {val_r2:.4f}, LLL (main): {val_lll:.4f}\"\n            )\n\n            # Scheduler and early stopping based on LLL (maximize)\n            scheduler.step(val_lll)\n\n            if val_lll > self.best_val_lll:\n                self.best_val_lll = val_lll\n                torch.save(self.model.state_dict(), \"best_working_model.pth\")\n                print(\"✅ New best model saved based on LLL!\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n\n        return self.best_val_lll\n\n# Initialize and train\nmodel = WorkingDenseNetModel(tab_dim=4).to(DEVICE)\ntrainer = Trainer(model, DEVICE, lr=1e-4)\nbest_lll = trainer.train(train_loader, val_loader, epochs=30, patience=8)\nprint(f\"Training complete. Best validation LLL: {best_lll:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T14:18:13.510886Z","iopub.execute_input":"2025-09-11T14:18:13.511200Z","iopub.status.idle":"2025-09-11T14:31:25.325852Z","shell.execute_reply.started":"2025-09-11T14:18:13.511175Z","shell.execute_reply":"2025-09-11T14:31:25.325035Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded train dataset: (1549, 7)\nCalculating linear decay coefficients per patient...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 176/176 [00:00<00:00, 1165.67it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 138 patients with images\nDataset val: 36 patients with images\nTraining with 103 batches, validating with 5 batches.\nEpoch 1/30 | Train Loss: 51.0170, MAE: 5.0713 | Val Loss: 66.9944, MAE: 5.7251, R²: -0.2759, LLL (main): -5.6618\n✅ New best model saved based on LLL!\nEpoch 2/30 | Train Loss: 36.1358, MAE: 4.2566 | Val Loss: 53.2650, MAE: 5.0861, R²: -0.0407, LLL (main): -4.5835\n✅ New best model saved based on LLL!\nEpoch 3/30 | Train Loss: 31.5575, MAE: 4.0260 | Val Loss: 49.5153, MAE: 4.8130, R²: 0.0172, LLL (main): -3.9880\n✅ New best model saved based on LLL!\nEpoch 4/30 | Train Loss: 30.9680, MAE: 4.0346 | Val Loss: 54.1459, MAE: 5.0264, R²: -0.0846, LLL (main): -4.1635\nEpoch 5/30 | Train Loss: 30.4053, MAE: 4.0412 | Val Loss: 51.4621, MAE: 4.9320, R²: -0.0332, LLL (main): -3.9351\n✅ New best model saved based on LLL!\nEpoch 6/30 | Train Loss: 30.9999, MAE: 4.0573 | Val Loss: 51.8511, MAE: 4.9948, R²: -0.0316, LLL (main): -3.8778\n✅ New best model saved based on LLL!\nEpoch 7/30 | Train Loss: 30.6704, MAE: 4.0314 | Val Loss: 51.7616, MAE: 4.9636, R²: -0.0418, LLL (main): -3.7128\n✅ New best model saved based on LLL!\nEpoch 8/30 | Train Loss: 30.4980, MAE: 4.0118 | Val Loss: 51.4090, MAE: 4.9293, R²: -0.0292, LLL (main): -3.6050\n✅ New best model saved based on LLL!\nEpoch 9/30 | Train Loss: 30.2988, MAE: 4.0306 | Val Loss: 50.4961, MAE: 4.9981, R²: -0.0174, LLL (main): -3.6138\nEpoch 10/30 | Train Loss: 30.2933, MAE: 4.0411 | Val Loss: 50.4229, MAE: 4.9368, R²: -0.0165, LLL (main): -3.5274\n✅ New best model saved based on LLL!\nEpoch 11/30 | Train Loss: 29.9154, MAE: 3.9718 | Val Loss: 49.9450, MAE: 4.9902, R²: -0.0113, LLL (main): -3.5202\n✅ New best model saved based on LLL!\nEpoch 12/30 | Train Loss: 29.8944, MAE: 3.9834 | Val Loss: 51.9779, MAE: 5.1463, R²: -0.0511, LLL (main): -3.5846\nEpoch 13/30 | Train Loss: 30.0724, MAE: 4.0110 | Val Loss: 53.5914, MAE: 5.1246, R²: -0.0905, LLL (main): -3.5275\nEpoch 14/30 | Train Loss: 29.8138, MAE: 3.9897 | Val Loss: 51.8124, MAE: 4.9779, R²: -0.0562, LLL (main): -3.4460\n✅ New best model saved based on LLL!\nEpoch 15/30 | Train Loss: 29.6101, MAE: 3.9751 | Val Loss: 51.1138, MAE: 4.9965, R²: -0.0380, LLL (main): -3.4376\n✅ New best model saved based on LLL!\nEpoch 16/30 | Train Loss: 29.6308, MAE: 3.9873 | Val Loss: 51.4017, MAE: 5.0303, R²: -0.0437, LLL (main): -3.4277\n✅ New best model saved based on LLL!\nEpoch 17/30 | Train Loss: 29.6815, MAE: 3.9936 | Val Loss: 49.8213, MAE: 4.9671, R²: -0.0071, LLL (main): -3.3991\n✅ New best model saved based on LLL!\nEpoch 18/30 | Train Loss: 29.2104, MAE: 3.9652 | Val Loss: 53.1470, MAE: 5.1734, R²: -0.0806, LLL (main): -3.4849\nEpoch 19/30 | Train Loss: 29.4251, MAE: 3.9517 | Val Loss: 51.1245, MAE: 4.9635, R²: -0.0370, LLL (main): -3.3999\nEpoch 20/30 | Train Loss: 30.4714, MAE: 4.0738 | Val Loss: 51.4196, MAE: 5.0403, R²: -0.0459, LLL (main): -3.4205\nEpoch 21/30 | Train Loss: 28.6623, MAE: 3.9560 | Val Loss: 52.3591, MAE: 5.1247, R²: -0.0626, LLL (main): -3.4360\nEpoch 22/30 | Train Loss: 29.3126, MAE: 3.9425 | Val Loss: 51.2463, MAE: 5.0218, R²: -0.0430, LLL (main): -3.3776\n✅ New best model saved based on LLL!\nEpoch 23/30 | Train Loss: 28.6148, MAE: 3.9369 | Val Loss: 53.1394, MAE: 5.0994, R²: -0.0865, LLL (main): -3.4112\nEpoch 24/30 | Train Loss: 29.0619, MAE: 3.9658 | Val Loss: 51.9601, MAE: 5.1524, R²: -0.0486, LLL (main): -3.4030\nEpoch 25/30 | Train Loss: 28.6758, MAE: 3.9325 | Val Loss: 51.6126, MAE: 5.1095, R²: -0.0405, LLL (main): -3.4006\nEpoch 26/30 | Train Loss: 28.6906, MAE: 3.9103 | Val Loss: 52.8172, MAE: 5.1789, R²: -0.0784, LLL (main): -3.4165\nEpoch 27/30 | Train Loss: 29.3637, MAE: 3.9619 | Val Loss: 52.6957, MAE: 5.1615, R²: -0.0773, LLL (main): -3.3950\nEpoch 28/30 | Train Loss: 29.3913, MAE: 3.9454 | Val Loss: 52.5917, MAE: 5.1201, R²: -0.0705, LLL (main): -3.3998\nEpoch 29/30 | Train Loss: 28.8894, MAE: 3.9280 | Val Loss: 52.1941, MAE: 5.1056, R²: -0.0623, LLL (main): -3.3938\nEpoch 30/30 | Train Loss: 28.6884, MAE: 3.9360 | Val Loss: 52.4249, MAE: 5.1505, R²: -0.0634, LLL (main): -3.4091\nEarly stopping at epoch 30\nTraining complete. Best validation LLL: -3.3776\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Here is a set of advanced, practical strategies with implementation guidance that have proven effective in similar medical regression tasks and can be implemented within your Kaggle time budget:**\n\n1. Use a Multi-Task Learning approach:\nAdd an auxiliary regression target to jointly predict the raw FVC value (or percentile) alongside decay slope \nA\nA. This helps the model learn finer representations.\n\nE.g., predict both \nA\nA (decay rate) and baseline FVC.\n\nCombine their losses weighted appropriately.\n\n2. Use Longitudinal Information or Patient Embeddings:\nYour current model uses single image slices without explicit temporal or patient-specific embeddings beyond tabular data. Enhancing temporal modeling with:\n\nAdding trainable patient embeddings or encoding patient ID with learnable embedding vectors.\n\nUsing sequences of images as input, e.g., with temporal CNN, RNN, or transformers, to better capture progression trends.\n\n3. Refine Loss Function by Adding an MAE/MSE term weighted more heavily:\nBecause you want better predictive fit (and R² reflects that), increase the weight of standard MSE or MAE loss compared to uncertainty loss, or first pretrain model on pure regression loss then finetune uncertainty.","metadata":{}},{"cell_type":"code","source":"# Imports and environment setup\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport cv2\nimport pydicom\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom sklearn.model_selection import train_test_split\n\n# Seed for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything()\n\n# Paths and device\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nTEST_DIR = DATA_DIR / \"test\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# Load train.csv\ntrain_df = pd.read_csv(DATA_DIR / \"train.csv\")\nprint(f\"Loaded train dataset: {train_df.shape}\")\n\n# Tabular feature extraction\ndef get_tab_features(row):\n    age_scaled = (row['Age'] - 30) / 30\n    sex_encoded = 0 if row['Sex'] == 'Male' else 1\n    smoke_map = {\n        'Never smoked': [0, 0],\n        'Ex-smoker': [1, 1],\n        'Currently smokes': [0, 1]\n    }\n    smoking = row['SmokingStatus']\n    smoke_vec = smoke_map.get(smoking, [1, 0])\n    return np.array([age_scaled, sex_encoded, smoke_vec[0], smoke_vec[1]])\n\n# Compute linear decay coefficients per patient\nA = {}\nTAB = {}\nBASELINE_FVC = {}\nP = []\n\nprint(\"Calculating linear decay coefficients and baseline FVC per patient...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, b = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n            BASELINE_FVC[patient] = b\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0])\n            BASELINE_FVC[patient] = fvc[0]\n    else:\n        A[patient] = 0.0\n        BASELINE_FVC[patient] = fvc[0]\n    TAB[patient] = get_tab_features(sub.iloc[0])\n    P.append(patient)\nprint(f\"Processed {len(P)} patients.\")\n\n# Medical Image Augmentation\nclass MedicalAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=15, p=0.7),\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                albu.GaussNoise(var_limit=(10, 50), p=0.5),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\n# Dataset\nclass OSICDenseNetMultiTaskDataset(Dataset):\n    def __init__(self, patients, A_dict, BASELINE_dict, TAB_dict, data_dir, split='train', augment=True):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.BASELINE_dict = BASELINE_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = MedicalAugmentation(augment=augment)\n        self.patient_images = {}\n        for p in self.patients:\n            pd = self.data_dir / p\n            if pd.exists():\n                imgs = [f for f in pd.iterdir() if f.suffix.lower() == '.dcm']\n                if imgs:\n                    self.patient_images[p] = imgs\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n\n    def __len__(self):\n        return len(self.valid_patients) * 6 if self.split == 'train' else len(self.valid_patients)\n\n    def __getitem__(self, idx):\n        idx_mod = idx % len(self.valid_patients) if self.split == 'train' else idx\n        patient = self.valid_patients[idx_mod]\n        images = self.patient_images[patient]\n        img_path = np.random.choice(images) if len(images) > 1 else images[0]\n        img = self.load_and_preprocess_dicom(img_path)\n        img_tensor = self.augmentor(img)\n        tab_feat = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n        decay_target = torch.tensor(self.A_dict[patient], dtype=torch.float32)\n        baseline_target = torch.tensor(self.BASELINE_dict[patient], dtype=torch.float32)\n        return img_tensor, tab_feat, decay_target, baseline_target\n\n    def load_and_preprocess_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape) == 3:\n                img = img[img.shape[0] // 2]\n            img = cv2.resize(img, (512, 512))\n            mn, mx = img.min(), img.max()\n            if mx > mn:\n                img = (img - mn) / (mx - mn) * 255\n            else:\n                img = np.zeros_like(img)\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\n# Attention Module\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        x_cat = self.conv1(x_cat)\n        return x * self.sigmoid(x_cat)\n\n# Multi-task model with decay rate and baseline FVC prediction\nclass MultiTaskDenseNetModel(nn.Module):\n    def __init__(self, tab_dim=4, dropout=0.4):\n        super().__init__()\n        dnet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = dnet.features\n        self.spatial_attention = SpatialAttention()\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tab_dim, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU()\n        )\n        edim = 1024\n        self.query_proj = nn.Linear(edim, edim)\n        self.key_proj = nn.Linear(512, edim)\n        self.value_proj = nn.Linear(512, edim)\n        self.cross_attention = nn.MultiheadAttention(embed_dim=edim, num_heads=8, dropout=0.2, batch_first=True)\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(edim + 512, 768), nn.BatchNorm1d(768), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(768, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout / 2)\n        )\n        # Predict decay rate A\n        self.decay_head = nn.Sequential(\n            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 64), nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        # Predict baseline FVC\n        self.baseline_fvc_head = nn.Sequential(\n            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 64), nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_feat = self.features(images)\n        img_feat = self.spatial_attention(img_feat)\n        img_feat = F.adaptive_avg_pool2d(img_feat, (1, 1)).view(b, -1)\n        tab_feat = self.tabular_processor(tabular)\n        q = self.query_proj(img_feat).unsqueeze(1)\n        k = self.key_proj(tab_feat).unsqueeze(1)\n        v = self.value_proj(tab_feat).unsqueeze(1)\n        attn_out, _ = self.cross_attention(q, k, v)\n        attn_out = attn_out.squeeze(1)\n        combined = torch.cat([attn_out, tab_feat], dim=1)\n        fused_feat = self.fusion_layer(combined)\n        decay_pred = self.decay_head(fused_feat).squeeze(1)\n        baseline_pred = self.baseline_fvc_head(fused_feat).squeeze(1)\n        return decay_pred, baseline_pred\n\n# Multi-task combined MSE loss with alpha weighting\ndef multitask_loss(decay_pred, baseline_pred, decay_target, baseline_target, alpha=0.7):\n    decay_loss = F.mse_loss(decay_pred, decay_target)\n    baseline_loss = F.mse_loss(baseline_pred, baseline_target)\n    return alpha * decay_loss + (1 - alpha) * baseline_loss\n\n# Laplace Log Likelihood metric for reporting using normal approximation of residuals with std dev from data\ndef laplace_log_likelihood_metric(pred, target, eps=1e-6):\n    # residual std dev estimated from residual error per batch (used for reporting only)\n    residuals = np.abs(pred - target)\n    sigma = np.maximum(np.std(residuals), eps)\n    delta = np.abs(pred - target)\n    score = -np.log(2 * sigma) - delta / sigma\n    return np.mean(score)\n\n# Dataset splitting\ntrain_patients, val_patients = train_test_split(P, test_size=0.2, random_state=42)\n\ntrain_dataset = OSICDenseNetMultiTaskDataset(train_patients, A, BASELINE_FVC, TAB, TRAIN_DIR, augment=True, split='train')\nval_dataset = OSICDenseNetMultiTaskDataset(val_patients, A, BASELINE_FVC, TAB, TRAIN_DIR, augment=False, split='val')\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Training with {len(train_loader)} batches, validating with {len(val_loader)} batches.\")\n\n# Trainer class monitoring R² as main metric with LLL printed every epoch\nclass MultiTaskTrainer:\n    def __init__(self, model, device, lr=1e-4, alpha=0.7):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.alpha = alpha\n        self.best_val_r2 = -float('inf')\n\n    def train(self, train_loader, val_loader, epochs=25, patience=6):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n        patience_counter = 0\n\n        for epoch in range(epochs):\n            self.model.train()\n            train_losses = []\n\n            for images, tabular, decay_targets, baseline_targets in train_loader:\n                images, tabular = images.to(self.device), tabular.to(self.device)\n                decay_targets, baseline_targets = decay_targets.to(self.device), baseline_targets.to(self.device)\n\n                optimizer.zero_grad()\n                decay_pred, baseline_pred = self.model(images, tabular)\n                loss = multitask_loss(decay_pred, baseline_pred, decay_targets, baseline_targets, alpha=self.alpha)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_losses.append(loss.item())\n\n            avg_train_loss = np.mean(train_losses)\n\n            self.model.eval()\n            val_decay_preds, val_baseline_preds = [], []\n            val_decay_targets, val_baseline_targets = [], []\n\n            with torch.no_grad():\n                for images, tabular, decay_targets, baseline_targets in val_loader:\n                    images, tabular = images.to(self.device), tabular.to(self.device)\n                    decay_targets, baseline_targets = decay_targets.to(self.device), baseline_targets.to(self.device)\n\n                    decay_pred, baseline_pred = self.model(images, tabular)\n                    val_decay_preds.extend(decay_pred.cpu().numpy())\n                    val_baseline_preds.extend(baseline_pred.cpu().numpy())\n                    val_decay_targets.extend(decay_targets.cpu().numpy())\n                    val_baseline_targets.extend(baseline_targets.cpu().numpy())\n\n            val_decay_preds = np.array(val_decay_preds)\n            val_decay_targets = np.array(val_decay_targets)\n\n            ss_res = np.sum((val_decay_targets - val_decay_preds) ** 2)\n            ss_tot = np.sum((val_decay_targets - np.mean(val_decay_targets)) ** 2)\n            val_r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else float('-inf')\n\n            # Calculate LLL (reported every epoch, not used for early stop here)\n            val_lll = laplace_log_likelihood_metric(val_decay_preds, val_decay_targets)\n\n            print(\n                f\"Epoch {epoch+1}/{epochs} | \"\n                f\"Train Loss: {avg_train_loss:.4f} | \"\n                f\"Val R² (decay): {val_r2:.4f} | \"\n                f\"LLL (reported): {val_lll:.4f}\"\n            )\n\n            scheduler.step(val_r2)\n\n            if val_r2 > self.best_val_r2:\n                self.best_val_r2 = val_r2\n                torch.save(self.model.state_dict(), \"best_multitask_model.pth\")\n                print(\"✅ New best model saved based on R²!\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n\n        return self.best_val_r2\n\n# Initialize model and trainer\nmodel = MultiTaskDenseNetModel(tab_dim=4).to(DEVICE)\ntrainer = MultiTaskTrainer(model, DEVICE, lr=1e-4, alpha=0.7)\n\n# Train\nbest_r2 = trainer.train(train_loader, val_loader, epochs=25, patience=6)\nprint(f\"Training complete. Best validation R²: {best_r2:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T14:31:25.327679Z","iopub.execute_input":"2025-09-11T14:31:25.328246Z","iopub.status.idle":"2025-09-11T14:35:23.404515Z","shell.execute_reply.started":"2025-09-11T14:31:25.328221Z","shell.execute_reply":"2025-09-11T14:35:23.403701Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded train dataset: (1549, 7)\nCalculating linear decay coefficients and baseline FVC per patient...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 176/176 [00:00<00:00, 1333.15it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 138 patients with images\nDataset val: 36 patients with images\nTraining with 103 batches, validating with 5 batches.\nEpoch 1/25 | Train Loss: 2661169.8471 | Val R² (decay): -0.2947 | LLL (reported): -3.4533\n✅ New best model saved based on R²!\nEpoch 2/25 | Train Loss: 2657601.7621 | Val R² (decay): -0.1562 | LLL (reported): -3.3961\n✅ New best model saved based on R²!\nEpoch 3/25 | Train Loss: 2633535.4430 | Val R² (decay): -0.0591 | LLL (reported): -3.3695\n✅ New best model saved based on R²!\nEpoch 4/25 | Train Loss: 2602165.7464 | Val R² (decay): -0.1813 | LLL (reported): -3.3938\nEpoch 5/25 | Train Loss: 2551404.4211 | Val R² (decay): -0.1923 | LLL (reported): -3.4261\nEpoch 6/25 | Train Loss: 2476379.3847 | Val R² (decay): -0.1963 | LLL (reported): -3.4291\nEpoch 7/25 | Train Loss: 2387398.4345 | Val R² (decay): -0.1774 | LLL (reported): -3.4425\nEpoch 8/25 | Train Loss: 2304322.7439 | Val R² (decay): -0.1407 | LLL (reported): -3.4127\nEpoch 9/25 | Train Loss: 2236971.8507 | Val R² (decay): -0.1653 | LLL (reported): -3.4379\nEarly stopping at epoch 9\nTraining complete. Best validation R²: -0.0591\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Doc previous work and strategies that was done long long back from google doc**","metadata":{}},{"cell_type":"code","source":"# Imports and environment setup\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport cv2\nimport pydicom\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom sklearn.model_selection import train_test_split\n\n# Seed for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()\n\n# Paths and device setup\nDATA_DIR = Path(\"../input/osic-pulmonary-fibrosis-progression\")\nTRAIN_DIR = DATA_DIR / \"train\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# Load training data\ntrain_df = pd.read_csv(DATA_DIR / \"train.csv\")\nprint(f\"Train dataset loaded: {train_df.shape}\")\n\n# Tabular features extraction\ndef get_tab_features(row):\n    age_scaled = (row['Age'] - 30) / 30\n    sex_encoded = 0 if row['Sex'] == 'Male' else 1\n    smoke_map = {\n        'Never smoked': [0, 0],\n        'Ex-smoker': [1, 1],\n        'Currently smokes': [0, 1]\n    }\n    smoking = row['SmokingStatus']\n    smoke_vec = smoke_map.get(smoking, [1, 0])\n    return np.array([age_scaled, sex_encoded, smoke_vec[0], smoke_vec[1]])\n\n# Calculate linear decay coefficients and baseline FVC per patient\nA = {}\nBASELINE_FVC = {}\nTAB = {}\npatients_list = []\n\nprint(\"Calculating decay coefficients and baseline FVC...\")\nfor patient in tqdm(train_df['Patient'].unique()):\n    sub = train_df[train_df['Patient'] == patient].sort_values('Weeks')\n    fvc = sub['FVC'].values\n    weeks = sub['Weeks'].values\n    if len(weeks) > 1:\n        c = np.vstack([weeks, np.ones(len(weeks))]).T\n        try:\n            a, b = np.linalg.lstsq(c, fvc, rcond=None)[0]\n            A[patient] = a\n            BASELINE_FVC[patient] = b\n        except:\n            A[patient] = (fvc[-1] - fvc[0]) / (weeks[-1] - weeks[0])\n            BASELINE_FVC[patient] = fvc[0]\n    else:\n        A[patient] = 0.0\n        BASELINE_FVC[patient] = fvc[0]\n    TAB[patient] = get_tab_features(sub.iloc[0])\n    patients_list.append(patient)\nprint(f\"Processed {len(patients_list)} patients\")\n\n# Compute normalization stats for targets\ndecay_values = np.array(list(A.values()), dtype=np.float32)\nbaseline_values = np.array(list(BASELINE_FVC.values()), dtype=np.float32)\ndecay_mean, decay_std = decay_values.mean(), decay_values.std()\nbaseline_mean, baseline_std = baseline_values.mean(), baseline_values.std()\nprint(f\"Decay mean/std: {decay_mean:.3f}/{decay_std:.3f}\")\nprint(f\"Baseline mean/std: {baseline_mean:.3f}/{baseline_std:.3f}\")\n\n# Medical image augmentation class\nclass MedicalAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=15, p=0.7),\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                albu.GaussNoise(var_limit=(10, 50), p=0.5),\n                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n                albu.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.3),\n                albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\n# Dataset class for multi-task regression: decay slope and baseline FVC\nclass OSICDenseNetMultiTaskDataset(Dataset):\n    def __init__(self, patients, A_dict, BASELINE_dict, TAB_dict, data_dir, split='train', augment=True):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.A_dict = A_dict\n        self.BASELINE_dict = BASELINE_dict\n        self.TAB_dict = TAB_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = MedicalAugmentation(augment=augment)\n        self.patient_images = {}\n        for p in self.patients:\n            p_dir = self.data_dir / p\n            if p_dir.exists():\n                images = [f for f in p_dir.iterdir() if f.suffix.lower() == '.dcm']\n                if images:\n                    self.patient_images[p] = images\n        self.valid_patients = [p for p in self.patients if p in self.patient_images]\n        print(f\"Dataset {split}: {len(self.valid_patients)} patients with images\")\n\n    def __len__(self):\n        return len(self.valid_patients) * 6 if self.split == 'train' else len(self.valid_patients)\n\n    def __getitem__(self, idx):\n        idx_mod = idx % len(self.valid_patients) if self.split == 'train' else idx\n        patient = self.valid_patients[idx_mod]\n        images = self.patient_images[patient]\n        img_path = np.random.choice(images) if len(images) > 1 else images[0]\n        img = self.load_and_preprocess_dicom(img_path)\n        img_tensor = self.augmentor(img)\n        tab_feat = torch.tensor(self.TAB_dict[patient], dtype=torch.float32)\n\n        # ✅ Normalized targets\n        decay_target = (torch.tensor(self.A_dict[patient], dtype=torch.float32) - decay_mean) / decay_std\n        baseline_target = (torch.tensor(self.BASELINE_dict[patient], dtype=torch.float32) - baseline_mean) / baseline_std\n\n        return img_tensor, tab_feat, decay_target, baseline_target\n\n    def load_and_preprocess_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            if len(img.shape) == 3:\n                img = img[img.shape[0] // 2]\n            img = cv2.resize(img, (512, 512))\n            mn, mx = img.min(), img.max()\n            if mx > mn:\n                img = (img - mn) / (mx - mn) * 255\n            else:\n                img = np.zeros_like(img)\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading DICOM {path}: {e}\")\n            return np.zeros((512, 512, 3), dtype=np.uint8)\n\n# Spatial attention module\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        x_cat = self.conv(x_cat)\n        return x * self.sigmoid(x_cat)\n\n# Multi-task model predicting decay slope and baseline FVC\nclass MultiTaskDenseNetModel(nn.Module):\n    def __init__(self, tab_dim=4, dropout=0.4):\n        super().__init__()\n        dnet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = dnet.features\n        self.spatial_attention = SpatialAttention()\n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tab_dim, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU()\n        )\n        edim = 1024\n        self.query_proj = nn.Linear(edim, edim)\n        self.key_proj = nn.Linear(512, edim)\n        self.value_proj = nn.Linear(512, edim)\n        self.cross_attention = nn.MultiheadAttention(embed_dim=edim, num_heads=8, dropout=0.2, batch_first=True)\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(edim + 512, 768), nn.BatchNorm1d(768), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(768, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout / 2)\n        )\n        self.decay_head = nn.Sequential(\n            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 64), nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.baseline_fvc_head = nn.Sequential(\n            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 64), nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, images, tabular):\n        b = images.size(0)\n        img_feat = self.features(images)\n        img_feat = self.spatial_attention(img_feat)\n        img_feat = F.adaptive_avg_pool2d(img_feat, (1, 1)).view(b, -1)\n        tab_feat = self.tabular_processor(tabular)\n        q = self.query_proj(img_feat).unsqueeze(1)\n        k = self.key_proj(tab_feat).unsqueeze(1)\n        v = self.value_proj(tab_feat).unsqueeze(1)\n        attn_out, _ = self.cross_attention(q, k, v)\n        attn_out = attn_out.squeeze(1)\n        combined = torch.cat([attn_out, tab_feat], dim=1)\n        fused_feat = self.fusion_layer(combined)\n        decay_pred = self.decay_head(fused_feat).squeeze(1)\n        baseline_pred = self.baseline_fvc_head(fused_feat).squeeze(1)\n        return decay_pred, baseline_pred\n\n# Loss function: weighted MSE\ndef multitask_loss(decay_pred, baseline_pred, decay_target, baseline_target, alpha=0.7):\n    decay_loss = F.mse_loss(decay_pred, decay_target)\n    baseline_loss = F.mse_loss(baseline_pred, baseline_target)\n    return alpha * decay_loss + (1 - alpha) * baseline_loss\n\n# Laplace Log Likelihood metric calculated every epoch, regardless of usage in training\ndef laplace_log_likelihood_metric(pred, target, eps=1e-6):\n    residuals = np.abs(pred - target)\n    sigma = np.maximum(np.std(residuals), eps)\n    delta = np.abs(pred - target)\n    score = -np.log(2 * sigma) - delta / sigma\n    return np.mean(score)\n\n# Dataset splitting\ntrain_patients, val_patients = train_test_split(patients_list, test_size=0.2, random_state=42)\ntrain_dataset = OSICDenseNetMultiTaskDataset(train_patients, A, BASELINE_FVC, TAB, TRAIN_DIR, augment=True, split='train')\nval_dataset = OSICDenseNetMultiTaskDataset(val_patients, A, BASELINE_FVC, TAB, TRAIN_DIR, augment=False, split='val')\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Training with {len(train_loader)} batches, validating with {len(val_loader)} batches.\")\n\n# Trainer class printing LLL every epoch plus tracking R² for early stopping\nclass MultiTaskTrainer:\n    def __init__(self, model, device, lr=1e-4, alpha=0.7):\n        self.model = model\n        self.device = device\n        self.lr = lr\n        self.alpha = alpha\n        self.best_val_r2 = -float('inf')\n\n    def train(self, train_loader, val_loader, epochs=25, patience=6):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n        patience_counter = 0\n\n        for epoch in range(epochs):\n            self.model.train()\n            train_losses = []\n\n            for images, tabular, decay_targets, baseline_targets in train_loader:\n                images, tabular = images.to(self.device), tabular.to(self.device)\n                decay_targets, baseline_targets = decay_targets.to(self.device), baseline_targets.to(self.device)\n\n                optimizer.zero_grad()\n                decay_pred, baseline_pred = self.model(images, tabular)\n                loss = multitask_loss(decay_pred, baseline_pred, decay_targets, baseline_targets, alpha=self.alpha)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_losses.append(loss.item())\n\n            avg_train_loss = np.mean(train_losses)\n\n            self.model.eval()\n            val_decay_preds, val_baseline_preds = [], []\n            val_decay_targets, val_baseline_targets = [], []\n\n            with torch.no_grad():\n                for images, tabular, decay_targets, baseline_targets in val_loader:\n                    images, tabular = images.to(self.device), tabular.to(self.device)\n                    decay_targets, baseline_targets = decay_targets.to(self.device), baseline_targets.to(self.device)\n\n                    decay_pred, baseline_pred = self.model(images, tabular)\n                    val_decay_preds.extend(decay_pred.cpu().numpy())\n                    val_baseline_preds.extend(baseline_pred.cpu().numpy())\n                    val_decay_targets.extend(decay_targets.cpu().numpy())\n                    val_baseline_targets.extend(baseline_targets.cpu().numpy())\n\n            val_decay_preds = np.array(val_decay_preds)\n            val_decay_targets = np.array(val_decay_targets)\n\n            ss_res = np.sum((val_decay_targets - val_decay_preds) ** 2)\n            ss_tot = np.sum((val_decay_targets - np.mean(val_decay_targets)) ** 2)\n            val_r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else float('-inf')\n\n            val_lll = laplace_log_likelihood_metric(val_decay_preds, val_decay_targets)\n\n            print(\n                f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | \"\n                f\"Val R² (decay): {val_r2:.4f} | LLL (reported): {val_lll:.4f}\"\n            )\n\n            scheduler.step(val_r2)\n\n            if val_r2 > self.best_val_r2:\n                self.best_val_r2 = val_r2\n                torch.save(self.model.state_dict(), \"best_multitask_model.pth\")\n                print(\"✅ New best model saved based on R²!\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n\n        return self.best_val_r2\n\n# Instantiate and train model\nmodel = MultiTaskDenseNetModel(tab_dim=4).to(DEVICE)\ntrainer = MultiTaskTrainer(model, DEVICE, lr=1e-4, alpha=0.7)\nbest_r2 = trainer.train(train_loader, val_loader, epochs=25, patience=6)\nprint(f\"Training complete. Best validation R²: {best_r2:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T14:46:19.961601Z","iopub.execute_input":"2025-09-11T14:46:19.962421Z","iopub.status.idle":"2025-09-11T14:49:24.747644Z","shell.execute_reply.started":"2025-09-11T14:46:19.962390Z","shell.execute_reply":"2025-09-11T14:49:24.746906Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain dataset loaded: (1549, 7)\nCalculating decay coefficients and baseline FVC...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 176/176 [00:00<00:00, 1357.83it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients\nDecay mean/std: -4.524/6.118\nBaseline mean/std: 2817.542/835.401\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Dataset train: 138 patients with images\nDataset val: 36 patients with images\nTraining with 103 batches, validating with 5 batches.\nEpoch 1/25 | Train Loss: 0.8692 | Val R² (decay): -0.0046 | LLL (reported): -1.5115\n✅ New best model saved based on R²!\nEpoch 2/25 | Train Loss: 0.8185 | Val R² (decay): -0.0125 | LLL (reported): -1.5359\nEpoch 3/25 | Train Loss: 0.7874 | Val R² (decay): -0.0294 | LLL (reported): -1.5628\nEpoch 4/25 | Train Loss: 0.7837 | Val R² (decay): -0.0335 | LLL (reported): -1.5590\nEpoch 5/25 | Train Loss: 0.7918 | Val R² (decay): -0.0179 | LLL (reported): -1.5404\nEpoch 6/25 | Train Loss: 0.7842 | Val R² (decay): -0.0373 | LLL (reported): -1.5557\nEpoch 7/25 | Train Loss: 0.7965 | Val R² (decay): -0.0432 | LLL (reported): -1.5647\nEarly stopping at epoch 7\nTraining complete. Best validation R²: -0.0046\n","output_type":"stream"}],"execution_count":12}]}
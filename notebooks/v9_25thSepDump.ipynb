{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 20604,
          "databundleVersionId": 1357052,
          "isSourceIdPinned": false,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "OSIC Pulmonary Fibrosis Progression  DenseNet",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "2iqlaog6yl33"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "osic_pulmonary_fibrosis_progression_path = kagglehub.competition_download('osic-pulmonary-fibrosis-progression')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Qvj4pEEZyl3-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "oE2INAI3yl4A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "ClaudAI  -- >   Tabular only"
      ],
      "metadata": {
        "id": "8zUvwtktyl4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION 1: IMPORTS AND CONFIGURATION\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class Config:\n",
        "    SEED = 42\n",
        "    TRAIN_SPLIT = 0.7\n",
        "    VAL_SPLIT = 0.15\n",
        "    TEST_SPLIT = 0.15\n",
        "    BATCH_SIZE = 64\n",
        "    MODEL_ARCHITECTURE = \"ResNet\"  # Options: \"ResNet\", \"DenseNet\", \"EfficientNet\"\n",
        "    LEARNING_RATE = 1e-3\n",
        "    NUM_EPOCHS = 100\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    DROPOUT_RATE = 0.3\n",
        "    PATIENCE = 15\n",
        "    MIN_DELTA = 0.001\n",
        "    HIDDEN_DIMS = [512, 256, 128, 64]\n",
        "    USE_BATCH_NORM = True\n",
        "\n",
        "cfg = Config()\n",
        "set_seed(cfg.SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# SECTION 2: DATA LOADING AND PREPROCESSING\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"\n",
        "    Load data from train.csv and create proper train/val/test splits\n",
        "    Modify this function based on your actual dataset structure\n",
        "    \"\"\"\n",
        "    # Load your actual dataset - modify path as needed\n",
        "    try:\n",
        "        # Try to load the actual dataset\n",
        "        df = pd.read_csv('/kaggle/input/your-dataset/train.csv')\n",
        "        print(f\"Loaded dataset with shape: {df.shape}\")\n",
        "    except:\n",
        "        # Fallback: Create realistic synthetic data for demonstration\n",
        "        print(\"Creating synthetic dataset for demonstration...\")\n",
        "        np.random.seed(cfg.SEED)\n",
        "        n_samples = 2000\n",
        "        n_features = 20\n",
        "\n",
        "        # Create correlated features with some noise\n",
        "        X = np.random.randn(n_samples, n_features)\n",
        "        # Create a meaningful target with some non-linear relationships\n",
        "        y = (3 * X[:, 0] + 2 * X[:, 1] - 1.5 * X[:, 2] +\n",
        "             0.5 * X[:, 0] * X[:, 1] + 0.3 * X[:, 2]**2 +\n",
        "             np.random.randn(n_samples) * 0.1)\n",
        "\n",
        "        # Create DataFrame\n",
        "        feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "        df = pd.DataFrame(X, columns=feature_names)\n",
        "        df['target'] = y\n",
        "\n",
        "    # Separate features and target\n",
        "    if 'target' in df.columns:\n",
        "        target_col = 'target'\n",
        "    else:\n",
        "        # Modify this to match your actual target column name\n",
        "        target_col = df.columns[-1]  # Assume last column is target\n",
        "\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col].values\n",
        "\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "    print(f\"Target statistics: mean={y.mean():.4f}, std={y.std():.4f}\")\n",
        "\n",
        "    # Create train/val/test splits\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(cfg.VAL_SPLIT + cfg.TEST_SPLIT),\n",
        "        random_state=cfg.SEED, shuffle=True\n",
        "    )\n",
        "\n",
        "    val_size = cfg.VAL_SPLIT / (cfg.VAL_SPLIT + cfg.TEST_SPLIT)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=(1-val_size),\n",
        "        random_state=cfg.SEED, shuffle=True\n",
        "    )\n",
        "\n",
        "    print(f\"Split sizes - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "def create_preprocessing_pipeline(X_train):\n",
        "    \"\"\"Create sklearn preprocessing pipeline\"\"\"\n",
        "    numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
        "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "    preprocessors = []\n",
        "\n",
        "    if len(numeric_features) > 0:\n",
        "        numeric_transformer = Pipeline([\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "        preprocessors.append(('num', numeric_transformer, numeric_features))\n",
        "\n",
        "    if len(categorical_features) > 0:\n",
        "        from sklearn.preprocessing import OneHotEncoder\n",
        "        categorical_transformer = Pipeline([\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "        ])\n",
        "        preprocessors.append(('cat', categorical_transformer, categorical_features))\n",
        "\n",
        "    if preprocessors:\n",
        "        preprocessing_pipeline = ColumnTransformer(\n",
        "            transformers=preprocessors,\n",
        "            remainder='passthrough'\n",
        "        )\n",
        "    else:\n",
        "        preprocessing_pipeline = StandardScaler()\n",
        "\n",
        "    return preprocessing_pipeline\n",
        "\n",
        "# SECTION 3: ADVANCED MODEL ARCHITECTURES\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(out_dim) if cfg.USE_BATCH_NORM else nn.Identity()\n",
        "        self.bn2 = nn.BatchNorm1d(out_dim) if cfg.USE_BATCH_NORM else nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.shortcut = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.fc1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.bn2(self.fc2(out))\n",
        "        out += shortcut\n",
        "        return F.relu(out)\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_dim, growth_rate=32, num_layers=3, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        current_dim = in_dim\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            layer = nn.Sequential(\n",
        "                nn.BatchNorm1d(current_dim) if cfg.USE_BATCH_NORM else nn.Identity(),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(current_dim, growth_rate),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "            current_dim += growth_rate\n",
        "\n",
        "        self.output_dim = current_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            new_feature = layer(torch.cat(features, dim=1))\n",
        "            features.append(new_feature)\n",
        "        return torch.cat(features, dim=1)\n",
        "\n",
        "class ResNetRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if cfg.USE_BATCH_NORM else nn.Identity()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(ResNetBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.output_layer = nn.Linear(prev_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_bn(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.output_layer(x).squeeze()\n",
        "\n",
        "class DenseNetRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if cfg.USE_BATCH_NORM else nn.Identity()\n",
        "\n",
        "        blocks = []\n",
        "        current_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims[:-1]:\n",
        "            block = DenseBlock(current_dim, growth_rate=hidden_dim//4, num_layers=3, dropout_rate=dropout_rate)\n",
        "            blocks.append(block)\n",
        "            current_dim = block.output_dim\n",
        "\n",
        "            # Transition layer\n",
        "            transition = nn.Sequential(\n",
        "                nn.BatchNorm1d(current_dim) if cfg.USE_BATCH_NORM else nn.Identity(),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(current_dim, hidden_dims[-1]),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            blocks.append(transition)\n",
        "            current_dim = hidden_dims[-1]\n",
        "\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "        self.output_layer = nn.Linear(current_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_bn(x)\n",
        "        x = self.features(x)\n",
        "        return self.output_layer(x).squeeze()\n",
        "\n",
        "class EfficientNetRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if cfg.USE_BATCH_NORM else nn.Identity()\n",
        "\n",
        "        # Efficient scaling: gradually reduce dimensions\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            # Add squeeze-and-excitation-like attention\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim) if cfg.USE_BATCH_NORM else nn.Identity(),\n",
        "                nn.SiLU(),  # Swish activation\n",
        "                nn.Dropout(dropout_rate * (0.5 + 0.5 * i / len(hidden_dims)))  # Progressive dropout\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.output_layer = nn.Linear(prev_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_bn(x)\n",
        "        x = self.layers(x)\n",
        "        return self.output_layer(x).squeeze()\n",
        "\n",
        "def create_model(model_name, input_dim):\n",
        "    \"\"\"Factory function to create models\"\"\"\n",
        "    models = {\n",
        "        \"ResNet\": ResNetRegressor,\n",
        "        \"DenseNet\": DenseNetRegressor,\n",
        "        \"EfficientNet\": EfficientNetRegressor\n",
        "    }\n",
        "\n",
        "    if model_name not in models:\n",
        "        raise ValueError(f\"Unsupported model: {model_name}. Choose from {list(models.keys())}\")\n",
        "\n",
        "    model = models[model_name](\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=cfg.HIDDEN_DIMS,\n",
        "        dropout_rate=cfg.DROPOUT_RATE\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Initialize weights\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    model.apply(init_weights)\n",
        "    return model\n",
        "\n",
        "# SECTION 4: TRAINING AND EVALUATION\n",
        "\n",
        "def laplace_log_likelihood(actual, predicted, confidence=1.0):\n",
        "    \"\"\"Robust Laplace Log Likelihood metric\"\"\"\n",
        "    actual, predicted = np.array(actual), np.array(predicted)\n",
        "    confidence = np.maximum(confidence, 1e-6)  # Avoid division by zero\n",
        "    delta = np.abs(actual - predicted)\n",
        "    metric = -np.sqrt(2) * delta / confidence - np.log(np.sqrt(2) * confidence)\n",
        "    return np.mean(metric)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            if self.restore_best_weights:\n",
        "                self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.restore_best_weights and self.best_weights:\n",
        "                    model.load_state_dict(self.best_weights)\n",
        "                print(f\"\\nEarly stopping triggered after {self.counter} epochs without improvement.\")\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        all_preds.extend(output.detach().cpu().numpy())\n",
        "        all_targets.extend(target.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "    return avg_loss, r2\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "    lll = laplace_log_likelihood(all_targets, all_preds, confidence=np.std(all_preds))\n",
        "    return avg_loss, r2, lll\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, scheduler, early_stopper):\n",
        "    train_losses, val_losses, train_r2s, val_r2s = [], [], [], []\n",
        "\n",
        "    for epoch in range(cfg.NUM_EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_r2 = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        train_losses.append(train_loss)\n",
        "        train_r2s.append(train_r2)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_r2, val_lll = validate_epoch(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        val_r2s.append(val_r2)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 5 == 0 or epoch < 10:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch {epoch+1:3d}: Train Loss={train_loss:.4f} R²={train_r2:.4f} | \"\n",
        "                  f\"Val Loss={val_loss:.4f} R²={val_r2:.4f} LLL={val_lll:.4f} | LR={current_lr:.2e}\")\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopper(val_loss, model)\n",
        "        if early_stopper.early_stop:\n",
        "            break\n",
        "\n",
        "    return model, (train_losses, val_losses, train_r2s, val_r2s)\n",
        "\n",
        "# SECTION 5: MAIN EXECUTION AND EVALUATION\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"ROBUST ML PIPELINE - STARTING EXECUTION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load and prepare data\n",
        "    print(\"\\n1. Loading and preparing data...\")\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = load_and_prepare_data()\n",
        "\n",
        "    # Create preprocessing pipeline\n",
        "    print(\"\\n2. Creating preprocessing pipeline...\")\n",
        "    preprocessing_pipeline = create_preprocessing_pipeline(X_train)\n",
        "\n",
        "    # Fit preprocessing on training data only\n",
        "    X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
        "    X_val_processed = preprocessing_pipeline.transform(X_val)\n",
        "    X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "    # Scale targets for better convergence\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    target_scaler = StandardScaler()\n",
        "    y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "    y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).ravel()\n",
        "    y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "    print(f\"Processed feature dimension: {X_train_processed.shape[1]}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_train_processed),\n",
        "        torch.FloatTensor(y_train_scaled)\n",
        "    )\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_val_processed),\n",
        "        torch.FloatTensor(y_val_scaled)\n",
        "    )\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_test_processed),\n",
        "        torch.FloatTensor(y_test_scaled)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    print(f\"\\n3. Creating {cfg.MODEL_ARCHITECTURE} model...\")\n",
        "    model = create_model(cfg.MODEL_ARCHITECTURE, X_train_processed.shape[1])\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Setup training components\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=cfg.LEARNING_RATE,\n",
        "        weight_decay=cfg.WEIGHT_DECAY\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=cfg.PATIENCE//2, verbose=True\n",
        "    )\n",
        "    early_stopper = EarlyStopping(patience=cfg.PATIENCE, min_delta=cfg.MIN_DELTA)\n",
        "\n",
        "    # Train model\n",
        "    print(f\"\\n4. Training model for up to {cfg.NUM_EPOCHS} epochs...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    model, history = train_and_validate(\n",
        "        model, train_loader, val_loader, optimizer, criterion, scheduler, early_stopper\n",
        "    )\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL EVALUATION ON TEST SET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model.eval()\n",
        "    test_preds, test_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "            test_preds.extend(output.cpu().numpy())\n",
        "            test_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    # Convert back to original scale\n",
        "    test_preds_original = target_scaler.inverse_transform(np.array(test_preds).reshape(-1, 1)).ravel()\n",
        "    test_targets_original = target_scaler.inverse_transform(np.array(test_targets).reshape(-1, 1)).ravel()\n",
        "\n",
        "    # Calculate metrics on original scale\n",
        "    mae = mean_absolute_error(test_targets_original, test_preds_original)\n",
        "    mse = mean_squared_error(test_targets_original, test_preds_original)\n",
        "    r2 = r2_score(test_targets_original, test_preds_original)\n",
        "    lll = laplace_log_likelihood(test_targets_original, test_preds_original,\n",
        "                                confidence=np.std(test_preds_original))\n",
        "\n",
        "    print(f\"\\nFinal Test Metrics:\")\n",
        "    print(f\"MAE:  {mae:.4f}\")\n",
        "    print(f\"MSE:  {mse:.4f}\")\n",
        "    print(f\"R²:   {r2:.4f}\")\n",
        "    print(f\"LLL:  {lll:.4f}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\n5. Creating diagnostic plots...\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # Loss curves\n",
        "    train_losses, val_losses, train_r2s, val_r2s = history\n",
        "    axes[0, 0].plot(train_losses, label='Train Loss', alpha=0.8)\n",
        "    axes[0, 0].plot(val_losses, label='Validation Loss', alpha=0.8)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss (MSE)')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # R² curves\n",
        "    axes[0, 1].plot(train_r2s, label='Train R²', alpha=0.8)\n",
        "    axes[0, 1].plot(val_r2s, label='Validation R²', alpha=0.8)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('R² Score')\n",
        "    axes[0, 1].set_title('R² Score Progress')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Predictions vs Actuals\n",
        "    axes[1, 0].scatter(test_targets_original, test_preds_original, alpha=0.6, s=20)\n",
        "    min_val = min(test_targets_original.min(), test_preds_original.min())\n",
        "    max_val = max(test_targets_original.max(), test_preds_original.max())\n",
        "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
        "    axes[1, 0].set_xlabel('Actual Values')\n",
        "    axes[1, 0].set_ylabel('Predicted Values')\n",
        "    axes[1, 0].set_title(f'Predictions vs Actuals (R² = {r2:.4f})')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Residual plot\n",
        "    residuals = test_targets_original - test_preds_original\n",
        "    axes[1, 1].scatter(test_preds_original, residuals, alpha=0.6, s=20)\n",
        "    axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
        "    axes[1, 1].set_xlabel('Predicted Values')\n",
        "    axes[1, 1].set_ylabel('Residuals')\n",
        "    axes[1, 1].set_title('Residual Plot')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return model, preprocessing_pipeline, target_scaler\n",
        "\n",
        "# Execute the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    model, preprocessing_pipeline, target_scaler = main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:01:37.676991Z",
          "iopub.execute_input": "2025-09-25T12:01:37.677691Z",
          "iopub.status.idle": "2025-09-25T12:01:44.123683Z",
          "shell.execute_reply.started": "2025-09-25T12:01:37.677661Z",
          "shell.execute_reply": "2025-09-25T12:01:44.122986Z"
        },
        "id": "ZqaF1vnJyl4D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "HfNc2UjQyl4I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "kzgUyusvyl4J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inculdes CT Scan"
      ],
      "metadata": {
        "id": "4qYekumryl4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# COMPLETE MULTI-MODAL OSIC PULMONARY FIBROSIS MODEL\n",
        "# Integrating Tabular Data + CT DICOM Images\n",
        "# Professional Implementation with 10+ Years ML Experience\n",
        "# =============================================================================\n",
        "\n",
        "# SECTION 1: IMPORTS AND CONFIGURATION\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Medical Imaging\n",
        "import pydicom\n",
        "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
        "import cv2\n",
        "\n",
        "# Vision Models\n",
        "import torchvision.models as models\n",
        "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
        "\n",
        "# ML Tools\n",
        "from sklearn.model_selection import GroupKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Augmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for early detection pipeline\"\"\"\n",
        "    # Paths\n",
        "    data_dir: Path = Path(\"/kaggle/input/lung-fibrosis-detection\")\n",
        "    output_dir: Path = Path(\"/kaggle/working\")\n",
        "\n",
        "    # Data Processing\n",
        "    img_size: int = 224\n",
        "    n_slices: int = 5\n",
        "    window_center: int = -600\n",
        "    window_width: int = 1500\n",
        "\n",
        "    # Classification Classes\n",
        "    classes: List[str] = field(default_factory=lambda: [\"Normal\", \"Early_Fibrosis\", \"Advanced_Fibrosis\"])\n",
        "    n_classes: int = 3\n",
        "\n",
        "    # Model Architecture\n",
        "    model_type: str = \"MultiModal\"\n",
        "    backbone: str = \"efficientnet\"\n",
        "    tabular_hidden_dims: Tuple[int, ...] = (512, 256, 128, 64)\n",
        "    fusion_method: str = \"attention\"\n",
        "    dropout_rate: float = 0.4\n",
        "    use_batch_norm: bool = True\n",
        "\n",
        "    # Training\n",
        "    n_folds: int = 5\n",
        "    batch_size: int = 32\n",
        "    num_epochs: int = 100\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    scheduler: str = 'cosine'\n",
        "    patience: int = 15\n",
        "    min_delta: float = 0.001\n",
        "    gradient_clip: float = 1.0\n",
        "\n",
        "    # Class Balance\n",
        "    use_class_weights: bool = True\n",
        "    focal_loss_alpha: float = 0.25\n",
        "    focal_loss_gamma: float = 2.0\n",
        "\n",
        "    # Augmentation\n",
        "    use_augmentation: bool = True\n",
        "    aug_prob: float = 0.7\n",
        "    use_mixup: bool = False\n",
        "\n",
        "    # Advanced\n",
        "    use_mixed_precision: bool = True\n",
        "    num_workers: int = 2\n",
        "    pin_memory: bool = True\n",
        "    seed: int = 42\n",
        "\n",
        "    # Validation\n",
        "    val_split: float = 0.15\n",
        "    test_split: float = 0.15\n",
        "\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# =============================================================================\n",
        "# REPRODUCIBILITY\n",
        "# =============================================================================\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Ensure reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(config.seed)\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 2: DATA LOADING AND PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def load_osic_data():\n",
        "    \"\"\"Load OSIC dataset with proper handling\"\"\"\n",
        "    try:\n",
        "        # Load actual OSIC data\n",
        "        train_df = pd.read_csv(config.data_dir / \"train.csv\")\n",
        "        logger.info(f\"Loaded OSIC dataset with shape: {train_df.shape}\")\n",
        "\n",
        "        # Basic data validation\n",
        "        required_cols = ['Patient', 'Weeks', 'FVC', 'Percent', 'Age', 'Sex', 'SmokingStatus']\n",
        "        assert all(col in train_df.columns for col in required_cols), \"Missing required columns\"\n",
        "\n",
        "        return train_df\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not load OSIC data: {e}\")\n",
        "        logger.info(\"Creating synthetic data for demonstration...\")\n",
        "\n",
        "        # Create realistic synthetic OSIC-like data\n",
        "        np.random.seed(config.seed)\n",
        "        n_patients = 100\n",
        "        records = []\n",
        "\n",
        "        for patient_id in range(n_patients):\n",
        "            n_visits = np.random.randint(3, 10)\n",
        "            baseline_fvc = np.random.normal(2500, 700)\n",
        "            age = np.random.randint(50, 85)\n",
        "            sex = np.random.choice(['Male', 'Female'])\n",
        "            smoking = np.random.choice(['Never smoked', 'Ex-smoker', 'Currently smokes'])\n",
        "\n",
        "            for visit in range(n_visits):\n",
        "                week = visit * np.random.randint(1, 6)\n",
        "                # Simulate FVC decline\n",
        "                fvc = baseline_fvc - np.random.normal(5, 2) * week\n",
        "                fvc = max(fvc, 500)  # Minimum FVC\n",
        "\n",
        "                records.append({\n",
        "                    'Patient': f'ID{patient_id:04d}',\n",
        "                    'Weeks': week,\n",
        "                    'FVC': fvc,\n",
        "                    'Percent': np.random.normal(50, 15),\n",
        "                    'Age': age,\n",
        "                    'Sex': sex,\n",
        "                    'SmokingStatus': smoking\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 3: MEDICAL IMAGE PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "class DicomProcessor:\n",
        "    \"\"\"Professional DICOM processing with proper windowing\"\"\"\n",
        "\n",
        "    def __init__(self, window_center: int = -600, window_width: int = 1500):\n",
        "        self.window_center = window_center\n",
        "        self.window_width = window_width\n",
        "\n",
        "    def load_dicom(self, path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load and preprocess DICOM file\"\"\"\n",
        "        try:\n",
        "            dcm = pydicom.dcmread(path)\n",
        "\n",
        "            # Apply DICOM transformations\n",
        "            img = dcm.pixel_array.astype(np.float32)\n",
        "\n",
        "            # Apply modality LUT\n",
        "            if hasattr(dcm, 'RescaleSlope') and hasattr(dcm, 'RescaleIntercept'):\n",
        "                img = img * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "\n",
        "            # Apply windowing\n",
        "            img = self.apply_windowing(img)\n",
        "\n",
        "            # Normalize to [0, 1]\n",
        "            img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "            return img.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Failed to load DICOM {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def apply_windowing(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply lung window settings\"\"\"\n",
        "        min_val = self.window_center - self.window_width // 2\n",
        "        max_val = self.window_center + self.window_width // 2\n",
        "        return np.clip(img, min_val, max_val)\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 4: FEATURE ENGINEERING\n",
        "# =============================================================================\n",
        "\n",
        "class TabularFeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering for tabular data\"\"\"\n",
        "\n",
        "    def __init__(self, train_df: pd.DataFrame):\n",
        "        self.train_df = train_df\n",
        "        self.scaler = RobustScaler()\n",
        "        self.patient_features = {}\n",
        "        self._prepare_features()\n",
        "\n",
        "    def _prepare_features(self):\n",
        "        \"\"\"Create comprehensive feature set\"\"\"\n",
        "        for patient in self.train_df['Patient'].unique():\n",
        "            patient_data = self.train_df[self.train_df['Patient'] == patient].sort_values('Weeks')\n",
        "\n",
        "            # Baseline measurements\n",
        "            baseline = patient_data.iloc[0]\n",
        "\n",
        "            # Calculate FVC trajectory\n",
        "            if len(patient_data) > 1:\n",
        "                weeks = patient_data['Weeks'].values\n",
        "                fvc = patient_data['FVC'].values\n",
        "                # Linear regression for slope\n",
        "                slope = np.polyfit(weeks, fvc, 1)[0] if len(weeks) > 1 else 0\n",
        "                std_dev = np.std(fvc)\n",
        "            else:\n",
        "                slope = 0\n",
        "                std_dev = 0\n",
        "\n",
        "            self.patient_features[patient] = {\n",
        "                'Age': baseline['Age'],\n",
        "                'Sex': baseline['Sex'],\n",
        "                'SmokingStatus': baseline['SmokingStatus'],\n",
        "                'BaselineFVC': baseline['FVC'],\n",
        "                'BaselinePercent': baseline['Percent'],\n",
        "                'BaselineWeeks': baseline['Weeks'],\n",
        "                'FVCSlope': slope,\n",
        "                'FVCStdDev': std_dev,\n",
        "                'NumMeasurements': len(patient_data)\n",
        "            }\n",
        "\n",
        "        # Fit scaler\n",
        "        self._fit_scaler()\n",
        "\n",
        "    def _fit_scaler(self):\n",
        "        \"\"\"Fit scaler on all features\"\"\"\n",
        "        features = []\n",
        "        for stats in self.patient_features.values():\n",
        "            features.append(self._encode_features(stats, 0))\n",
        "        self.scaler.fit(features)\n",
        "\n",
        "    def _encode_features(self, stats: Dict, current_week: float) -> np.ndarray:\n",
        "        \"\"\"Encode features for a given patient and week\"\"\"\n",
        "        # One-hot encode categorical\n",
        "        sex_male = 1 if stats['Sex'] == 'Male' else 0\n",
        "\n",
        "        smoke_never = 1 if stats['SmokingStatus'] == 'Never smoked' else 0\n",
        "        smoke_ex = 1 if stats['SmokingStatus'] == 'Ex-smoker' else 0\n",
        "        smoke_current = 1 if stats['SmokingStatus'] == 'Currently smokes' else 0\n",
        "\n",
        "        # Time features\n",
        "        week_delta = current_week - stats['BaselineWeeks']\n",
        "        week_squared = week_delta ** 2\n",
        "\n",
        "        # Interaction features\n",
        "        age_week = stats['Age'] * week_delta / 100\n",
        "\n",
        "        # Expected FVC\n",
        "        expected_fvc = stats['BaselineFVC'] + stats['FVCSlope'] * week_delta\n",
        "\n",
        "        features = [\n",
        "            stats['Age'] / 100,\n",
        "            sex_male,\n",
        "            smoke_never,\n",
        "            smoke_ex,\n",
        "            smoke_current,\n",
        "            stats['BaselineFVC'] / 5000,\n",
        "            stats['BaselinePercent'] / 100,\n",
        "            week_delta / 52,\n",
        "            week_squared / (52 ** 2),\n",
        "            stats['FVCSlope'] / 100,\n",
        "            stats['FVCStdDev'] / 1000,\n",
        "            age_week,\n",
        "            expected_fvc / 5000,\n",
        "            stats['NumMeasurements'] / 10\n",
        "        ]\n",
        "\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    def get_features(self, patient_id: str, week: float) -> np.ndarray:\n",
        "        \"\"\"Get scaled features\"\"\"\n",
        "        if patient_id not in self.patient_features:\n",
        "            # Return zero features if patient not found\n",
        "            return np.zeros(14, dtype=np.float32)\n",
        "\n",
        "        features = self._encode_features(self.patient_features[patient_id], week)\n",
        "        return self.scaler.transform([features])[0]\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 5: MULTI-MODAL DATASET\n",
        "# =============================================================================\n",
        "\n",
        "class OSICMultiModalDataset(Dataset):\n",
        "    \"\"\"Dataset combining tabular and image data\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        patient_data: pd.DataFrame,\n",
        "        feature_engineer: TabularFeatureEngineer,\n",
        "        img_dir: Optional[Path] = None,\n",
        "        transform: Optional[A.Compose] = None,\n",
        "        n_slices: int = 3,\n",
        "        is_train: bool = True\n",
        "    ):\n",
        "        self.patient_data = patient_data\n",
        "        self.feature_engineer = feature_engineer\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.n_slices = n_slices\n",
        "        self.is_train = is_train\n",
        "        self.dicom_processor = DicomProcessor()\n",
        "\n",
        "        # Prepare samples\n",
        "        self.samples = []\n",
        "        self.patient_images = {}\n",
        "\n",
        "        # Check for DICOM availability\n",
        "        self.has_images = img_dir is not None and img_dir.exists()\n",
        "\n",
        "        if self.has_images:\n",
        "            self._load_patient_images()\n",
        "\n",
        "        # Create samples\n",
        "        for _, row in patient_data.iterrows():\n",
        "            self.samples.append({\n",
        "                'patient': row['Patient'],\n",
        "                'week': row['Weeks'],\n",
        "                'fvc': row['FVC']\n",
        "            })\n",
        "\n",
        "        logger.info(f\"Dataset created with {len(self.samples)} samples\")\n",
        "\n",
        "    def _load_patient_images(self):\n",
        "        \"\"\"Load available DICOM images for each patient\"\"\"\n",
        "        for patient in self.patient_data['Patient'].unique():\n",
        "            patient_dir = self.img_dir / patient\n",
        "\n",
        "            if patient_dir.exists():\n",
        "                dicom_files = sorted(patient_dir.glob(\"*.dcm\"))\n",
        "\n",
        "                if dicom_files:\n",
        "                    # Select evenly spaced slices\n",
        "                    n_files = len(dicom_files)\n",
        "                    if n_files >= self.n_slices:\n",
        "                        indices = np.linspace(0, n_files-1, self.n_slices, dtype=int)\n",
        "                    else:\n",
        "                        indices = list(range(n_files))\n",
        "\n",
        "                    self.patient_images[patient] = [dicom_files[i] for i in indices]\n",
        "                else:\n",
        "                    self.patient_images[patient] = None\n",
        "            else:\n",
        "                self.patient_images[patient] = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        patient = sample['patient']\n",
        "        week = sample['week']\n",
        "        fvc = sample['fvc']\n",
        "\n",
        "        # Get tabular features\n",
        "        tabular = self.feature_engineer.get_features(patient, week)\n",
        "\n",
        "        # Get image features\n",
        "        if self.has_images and patient in self.patient_images:\n",
        "            slices = self.patient_images.get(patient)\n",
        "            if slices:\n",
        "                images = []\n",
        "                for slice_path in slices[:3]:  # Maximum 3 slices\n",
        "                    img = self.dicom_processor.load_dicom(str(slice_path))\n",
        "                    if img is not None:\n",
        "                        # Resize to target size\n",
        "                        img = cv2.resize(img, (config.img_size, config.img_size))\n",
        "                        images.append(img)\n",
        "\n",
        "                # Create 3-channel image\n",
        "                if len(images) >= 3:\n",
        "                    img = np.stack(images[:3], axis=-1)\n",
        "                elif len(images) == 2:\n",
        "                    img = np.stack([images[0], images[1], images[1]], axis=-1)\n",
        "                elif len(images) == 1:\n",
        "                    img = np.stack([images[0], images[0], images[0]], axis=-1)\n",
        "                else:\n",
        "                    img = np.zeros((config.img_size, config.img_size, 3), dtype=np.float32)\n",
        "            else:\n",
        "                img = np.zeros((config.img_size, config.img_size, 3), dtype=np.float32)\n",
        "        else:\n",
        "            # No images available - use zeros\n",
        "            img = np.zeros((config.img_size, config.img_size, 3), dtype=np.float32)\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img)\n",
        "            img = augmented['image']\n",
        "        else:\n",
        "            img = torch.from_numpy(img.transpose(2, 0, 1)).float()\n",
        "\n",
        "        return {\n",
        "            'image': img,\n",
        "            'tabular': torch.tensor(tabular, dtype=torch.float32),\n",
        "            'target': torch.tensor(fvc, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 6: AUGMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def get_transforms(is_train: bool = True, img_size: int = 224) -> A.Compose:\n",
        "    \"\"\"Get augmentation pipeline\"\"\"\n",
        "    if is_train:\n",
        "        return A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.1,\n",
        "                scale_limit=0.1,\n",
        "                rotate_limit=10,\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.OneOf([\n",
        "                A.GaussNoise(var_limit=(10, 50)),\n",
        "                A.GaussianBlur(blur_limit=3),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
        "            ], p=0.5),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 7: NEURAL NETWORK ARCHITECTURES\n",
        "# =============================================================================\n",
        "\n",
        "# Tabular Network Components\n",
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"Residual block for tabular data\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(out_dim) if config.use_batch_norm else nn.Identity()\n",
        "        self.bn2 = nn.BatchNorm1d(out_dim) if config.use_batch_norm else nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.shortcut = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.fc1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.bn2(self.fc2(out))\n",
        "        out += shortcut\n",
        "        return F.relu(out)\n",
        "\n",
        "# Attention Mechanisms\n",
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\"Cross-modal attention between image and tabular features\"\"\"\n",
        "    def __init__(self, img_dim, tab_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.img_proj = nn.Linear(img_dim, hidden_dim)\n",
        "        self.tab_proj = nn.Linear(tab_dim, hidden_dim)\n",
        "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "\n",
        "    def forward(self, img_feat, tab_feat):\n",
        "        # Project features\n",
        "        img_proj = self.img_proj(img_feat).unsqueeze(1)  # [B, 1, hidden]\n",
        "        tab_proj = self.tab_proj(tab_feat).unsqueeze(1)  # [B, 1, hidden]\n",
        "\n",
        "        # Cross attention\n",
        "        combined = torch.cat([img_proj, tab_proj], dim=1)  # [B, 2, hidden]\n",
        "        attended, _ = self.attention(combined, combined, combined)\n",
        "\n",
        "        # Aggregate\n",
        "        return attended.mean(dim=1)  # [B, hidden]\n",
        "\n",
        "# Main Multi-Modal Model\n",
        "class OSICMultiModalModel(nn.Module):\n",
        "    \"\"\"Complete multi-modal architecture\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tabular_dim: int = 14,\n",
        "        hidden_dims: Tuple[int] = (512, 256, 128, 64),\n",
        "        dropout_rate: float = 0.3,\n",
        "        backbone: str = \"efficientnet\",\n",
        "        fusion_method: str = \"attention\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Image backbone\n",
        "        if backbone == \"efficientnet\":\n",
        "            self.image_backbone = efficientnet_b3(weights=EfficientNet_B3_Weights.DEFAULT)\n",
        "            img_feat_dim = self.image_backbone.classifier[1].in_features\n",
        "            self.image_backbone.classifier = nn.Identity()\n",
        "        elif backbone == \"resnet50\":\n",
        "            self.image_backbone = models.resnet50(pretrained=True)\n",
        "            img_feat_dim = self.image_backbone.fc.in_features\n",
        "            self.image_backbone.fc = nn.Identity()\n",
        "        else:\n",
        "            self.image_backbone = models.densenet121(pretrained=True)\n",
        "            img_feat_dim = self.image_backbone.classifier.in_features\n",
        "            self.image_backbone.classifier = nn.Identity()\n",
        "\n",
        "        # Tabular network\n",
        "        tab_layers = []\n",
        "        prev_dim = tabular_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            tab_layers.append(ResNetBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "        self.tabular_net = nn.Sequential(*tab_layers)\n",
        "        tab_feat_dim = hidden_dims[-1]\n",
        "\n",
        "        # Fusion method\n",
        "        self.fusion_method = fusion_method\n",
        "        if fusion_method == \"attention\":\n",
        "            self.fusion = CrossModalAttention(img_feat_dim, tab_feat_dim)\n",
        "            fusion_dim = 256\n",
        "        elif fusion_method == \"gated\":\n",
        "            self.gate = nn.Sequential(\n",
        "                nn.Linear(img_feat_dim + tab_feat_dim, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 2),\n",
        "                nn.Softmax(dim=1)\n",
        "            )\n",
        "            fusion_dim = img_feat_dim + tab_feat_dim\n",
        "        else:  # concat\n",
        "            fusion_dim = img_feat_dim + tab_feat_dim\n",
        "\n",
        "        # Final regression head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 256),\n",
        "            nn.BatchNorm1d(256) if config.use_batch_norm else nn.Identity(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128) if config.use_batch_norm else nn.Identity(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # Uncertainty head (for confidence estimation)\n",
        "        self.uncertainty_head = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights properly\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        # Extract features\n",
        "        img_features = self.image_backbone(image)\n",
        "        tab_features = self.tabular_net(tabular)\n",
        "\n",
        "        # Fusion\n",
        "        if self.fusion_method == \"attention\":\n",
        "            combined = self.fusion(img_features, tab_features)\n",
        "        elif self.fusion_method == \"gated\":\n",
        "            concat_features = torch.cat([img_features, tab_features], dim=1)\n",
        "            gates = self.gate(concat_features)\n",
        "            combined = gates[:, 0:1] * img_features + gates[:, 1:2] * tab_features\n",
        "        else:\n",
        "            combined = torch.cat([img_features, tab_features], dim=1)\n",
        "\n",
        "        # Predictions\n",
        "        prediction = self.head(combined).squeeze(-1)\n",
        "        log_variance = self.uncertainty_head(combined).squeeze(-1)\n",
        "\n",
        "        return prediction, log_variance\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 8: LOSS FUNCTIONS AND METRICS\n",
        "# =============================================================================\n",
        "\n",
        "class RobustLoss(nn.Module):\n",
        "    \"\"\"Combined loss with uncertainty\"\"\"\n",
        "    def __init__(self, alpha: float = 0.7, beta: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.mae = nn.L1Loss()\n",
        "\n",
        "    def forward(self, pred_mean, pred_log_var, target):\n",
        "        # Heteroscedastic uncertainty loss\n",
        "        precision = torch.exp(-pred_log_var)\n",
        "        mse_loss = precision * (pred_mean - target) ** 2 + pred_log_var\n",
        "        nll_loss = 0.5 * torch.mean(mse_loss)\n",
        "\n",
        "        # MAE for robustness\n",
        "        mae_loss = self.mae(pred_mean, target)\n",
        "\n",
        "        # Combined\n",
        "        total_loss = self.alpha * nll_loss + self.beta * mae_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "def laplace_log_likelihood(y_true, y_pred, sigma):\n",
        "    \"\"\"Calculate Laplace Log Likelihood\"\"\"\n",
        "    sigma = np.maximum(sigma, 70)  # OSIC specific\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000)\n",
        "    metric = -np.sqrt(2) * delta / sigma - np.log(np.sqrt(2) * sigma)\n",
        "    return np.mean(metric)\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 9: TRAINING PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping with best model restoration\"\"\"\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_score = -float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if score > self.best_score + self.min_delta:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.best_weights:\n",
        "                    model.load_state_dict(self.best_weights)\n",
        "                logger.info(f\"Early stopping triggered after {self.counter} epochs\")\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, scaler, config):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch in tqdm(loader, desc='Training'):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        tabular = batch['tabular'].to(DEVICE)\n",
        "        targets = batch['target'].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training\n",
        "        with autocast(enabled=config.use_mixed_precision):\n",
        "            predictions, log_var = model(images, tabular)\n",
        "            loss = criterion(predictions, log_var, targets)\n",
        "\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, loader, criterion):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_sigmas = []\n",
        "\n",
        "    for batch in tqdm(loader, desc='Validation'):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        tabular = batch['tabular'].to(DEVICE)\n",
        "        targets = batch['target'].to(DEVICE)\n",
        "\n",
        "        predictions, log_var = model(images, tabular)\n",
        "        loss = criterion(predictions, log_var, targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_sigmas.extend(torch.exp(0.5 * log_var).cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "    all_sigmas = np.array(all_sigmas)\n",
        "\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "    lll = laplace_log_likelihood(all_targets, all_preds, all_sigmas)\n",
        "\n",
        "    return np.mean(losses), {'mae': mae, 'rmse': rmse, 'r2': r2, 'lll': lll}\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 10: MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"OSIC MULTI-MODAL PIPELINE - COMPLETE IMPLEMENTATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create output directory\n",
        "    config.output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # 1. Load and prepare data\n",
        "    logger.info(\"Loading OSIC data...\")\n",
        "    train_df = load_osic_data()\n",
        "\n",
        "    # 2. Create feature engineer\n",
        "    logger.info(\"Creating feature engineer...\")\n",
        "    feature_engineer = TabularFeatureEngineer(train_df)\n",
        "\n",
        "    # 3. Prepare patient-level splits (avoiding data leakage)\n",
        "    patients = train_df['Patient'].unique()\n",
        "    train_patients, temp_patients = train_test_split(\n",
        "        patients,\n",
        "        test_size=(config.val_split + config.test_split),\n",
        "        random_state=config.seed\n",
        "    )\n",
        "    val_patients, test_patients = train_test_split(\n",
        "        temp_patients,\n",
        "        test_size=config.test_split/(config.val_split + config.test_split),\n",
        "        random_state=config.seed\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_data = train_df[train_df['Patient'].isin(train_patients)]\n",
        "    val_data = train_df[train_df['Patient'].isin(val_patients)]\n",
        "    test_data = train_df[train_df['Patient'].isin(test_patients)]\n",
        "\n",
        "    logger.info(f\"Train samples: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
        "\n",
        "    # 4. Create transforms\n",
        "    train_transform = get_transforms(is_train=True, img_size=config.img_size)\n",
        "    val_transform = get_transforms(is_train=False, img_size=config.img_size)\n",
        "\n",
        "    # 5. Create datasets\n",
        "    img_dir = config.data_dir / \"train\" if (config.data_dir / \"train\").exists() else None\n",
        "\n",
        "    train_dataset = OSICMultiModalDataset(\n",
        "        train_data, feature_engineer, img_dir, train_transform, config.n_slices, is_train=True\n",
        "    )\n",
        "    val_dataset = OSICMultiModalDataset(\n",
        "        val_data, feature_engineer, img_dir, val_transform, config.n_slices, is_train=False\n",
        "    )\n",
        "    test_dataset = OSICMultiModalDataset(\n",
        "        test_data, feature_engineer, img_dir, val_transform, config.n_slices, is_train=False\n",
        "    )\n",
        "\n",
        "    # 6. Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory\n",
        "    )\n",
        "\n",
        "    # 7. Create model\n",
        "    logger.info(f\"Creating {config.backbone} model...\")\n",
        "    model = OSICMultiModalModel(\n",
        "        tabular_dim=14,  # Based on feature engineer\n",
        "        hidden_dims=config.tabular_hidden_dims,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "        backbone=config.backbone,\n",
        "        fusion_method=config.fusion_method\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # 8. Training setup\n",
        "    criterion = RobustLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    if config.scheduler == 'cosine':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "        )\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=config.patience, min_delta=config.min_delta)\n",
        "    scaler = GradScaler() if config.use_mixed_precision else None\n",
        "\n",
        "    # 9. Training loop\n",
        "    logger.info(\"Starting training...\")\n",
        "    best_score = -float('inf')\n",
        "    train_history = {'train_loss': [], 'val_loss': [], 'val_r2': [], 'val_lll': []}\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        logger.info(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, config)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_metrics = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "        # Update scheduler\n",
        "        if config.scheduler == 'cosine':\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            scheduler.step(val_metrics['r2'])\n",
        "\n",
        "        # Log metrics\n",
        "        logger.info(\n",
        "            f\"Train Loss: {train_loss:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f} | \"\n",
        "            f\"R2: {val_metrics['r2']:.4f} | \"\n",
        "            f\"MAE: {val_metrics['mae']:.4f} | \"\n",
        "            f\"LLL: {val_metrics['lll']:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save history\n",
        "        train_history['train_loss'].append(train_loss)\n",
        "        train_history['val_loss'].append(val_loss)\n",
        "        train_history['val_r2'].append(val_metrics['r2'])\n",
        "        train_history['val_lll'].append(val_metrics['lll'])\n",
        "\n",
        "        # Early stopping\n",
        "        current_score = val_metrics['r2']\n",
        "        early_stopper(current_score, model)\n",
        "\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_score': best_score,\n",
        "                'config': config\n",
        "            }, config.output_dir / 'best_model.pth')\n",
        "            logger.info(f\"New best model saved with R2: {best_score:.4f}\")\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            break\n",
        "\n",
        "        # Memory cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # 10. Final evaluation on test set\n",
        "    logger.info(\"Evaluating on test set...\")\n",
        "    test_loss, test_metrics = validate_epoch(model, test_loader, criterion)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL TEST SET RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test MAE: {test_metrics['mae']:.4f}\")\n",
        "    print(f\"Test RMSE: {test_metrics['rmse']:.4f}\")\n",
        "    print(f\"Test R²: {test_metrics['r2']:.4f}\")\n",
        "    print(f\"Test LLL: {test_metrics['lll']:.4f}\")\n",
        "\n",
        "    # 11. Create visualizations\n",
        "    logger.info(\"Creating visualizations...\")\n",
        "    create_training_plots(train_history)\n",
        "\n",
        "    # 12. Feature importance analysis (if possible)\n",
        "    try:\n",
        "        analyze_model_predictions(model, test_loader, test_data, feature_engineer)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not create prediction analysis: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return model, feature_engineer, train_history\n",
        "\n",
        "def create_training_plots(history):\n",
        "    \"\"\"Create training visualization plots\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0, 0].plot(history['train_loss'], label='Train Loss', alpha=0.7)\n",
        "    axes[0, 0].plot(history['val_loss'], label='Validation Loss', alpha=0.7)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # R² progression\n",
        "    axes[0, 1].plot(history['val_r2'], label='Validation R²', color='green', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('R² Score')\n",
        "    axes[0, 1].set_title('R² Score Progression')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # LLL progression\n",
        "    axes[1, 0].plot(history['val_lll'], label='Validation LLL', color='red', alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Laplace Log Likelihood')\n",
        "    axes[1, 0].set_title('Laplace Log Likelihood Progression')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning curves summary\n",
        "    axes[1, 1].text(0.1, 0.8, f\"Best Validation R²: {max(history['val_r2']):.4f}\",\n",
        "                   transform=axes[1, 1].transAxes, fontsize=12)\n",
        "    axes[1, 1].text(0.1, 0.6, f\"Best Validation LLL: {max(history['val_lll']):.4f}\",\n",
        "                   transform=axes[1, 1].transAxes, fontsize=12)\n",
        "    axes[1, 1].text(0.1, 0.4, f\"Final Train Loss: {history['train_loss'][-1]:.4f}\",\n",
        "                   transform=axes[1, 1].transAxes, fontsize=12)\n",
        "    axes[1, 1].text(0.1, 0.2, f\"Final Val Loss: {history['val_loss'][-1]:.4f}\",\n",
        "                   transform=axes[1, 1].transAxes, fontsize=12)\n",
        "    axes[1, 1].set_title('Training Summary')\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "@torch.no_grad()\n",
        "def analyze_model_predictions(model, loader, data, feature_engineer):\n",
        "    \"\"\"Analyze model predictions and create diagnostic plots\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    uncertainties = []\n",
        "\n",
        "    for batch in tqdm(loader, desc='Analyzing predictions'):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        tabular = batch['tabular'].to(DEVICE)\n",
        "        batch_targets = batch['target'].cpu().numpy()\n",
        "\n",
        "        pred_mean, log_var = model(images, tabular)\n",
        "        pred_uncertainty = torch.exp(0.5 * log_var)\n",
        "\n",
        "        predictions.extend(pred_mean.cpu().numpy())\n",
        "        targets.extend(batch_targets)\n",
        "        uncertainties.extend(pred_uncertainty.cpu().numpy())\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    targets = np.array(targets)\n",
        "    uncertainties = np.array(uncertainties)\n",
        "\n",
        "    # Create diagnostic plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Predictions vs Targets\n",
        "    axes[0, 0].scatter(targets, predictions, alpha=0.6, s=20)\n",
        "    min_val, max_val = min(targets.min(), predictions.min()), max(targets.max(), predictions.max())\n",
        "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
        "    axes[0, 0].set_xlabel('True FVC')\n",
        "    axes[0, 0].set_ylabel('Predicted FVC')\n",
        "    axes[0, 0].set_title('Predictions vs True Values')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Residual plot\n",
        "    residuals = targets - predictions\n",
        "    axes[0, 1].scatter(predictions, residuals, alpha=0.6, s=20)\n",
        "    axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
        "    axes[0, 1].set_xlabel('Predicted FVC')\n",
        "    axes[0, 1].set_ylabel('Residuals')\n",
        "    axes[0, 1].set_title('Residual Plot')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Uncertainty vs Error\n",
        "    abs_errors = np.abs(residuals)\n",
        "    axes[1, 0].scatter(uncertainties, abs_errors, alpha=0.6, s=20)\n",
        "    axes[1, 0].set_xlabel('Predicted Uncertainty')\n",
        "    axes[1, 0].set_ylabel('Absolute Error')\n",
        "    axes[1, 0].set_title('Uncertainty vs Error')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Error distribution\n",
        "    axes[1, 1].hist(residuals, bins=30, alpha=0.7, density=True)\n",
        "    axes[1, 1].set_xlabel('Residuals')\n",
        "    axes[1, 1].set_ylabel('Density')\n",
        "    axes[1, 1].set_title('Residual Distribution')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.output_dir / 'prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print additional statistics\n",
        "    print(f\"\\nPrediction Analysis:\")\n",
        "    print(f\"Mean Absolute Error: {np.mean(abs_errors):.4f}\")\n",
        "    print(f\"Root Mean Square Error: {np.sqrt(np.mean(residuals**2)):.4f}\")\n",
        "    print(f\"R² Score: {r2_score(targets, predictions):.4f}\")\n",
        "    print(f\"Mean Uncertainty: {np.mean(uncertainties):.4f}\")\n",
        "    print(f\"Std Uncertainty: {np.std(uncertainties):.4f}\")\n",
        "\n",
        "# Execute the complete pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, feature_engineer, history = main()\n",
        "        logger.info(\"Pipeline completed successfully!\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline failed with error: {e}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:54:24.205495Z",
          "iopub.execute_input": "2025-09-25T12:54:24.20578Z",
          "iopub.status.idle": "2025-09-25T12:54:24.271122Z",
          "shell.execute_reply.started": "2025-09-25T12:54:24.205739Z",
          "shell.execute_reply": "2025-09-25T12:54:24.270158Z"
        },
        "id": "III1OV6Jyl4L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disease Progression Prediction\n",
        "the above code"
      ],
      "metadata": {
        "id": "Ues1EmEsyl4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "azpSOHtGyl4P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "hgeDJJ53yl4P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jp-Ws2K7yl4Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# EARLY DETECTION OF PULMONARY FIBROSIS - CLASSIFICATION PIPELINE\n",
        "# Multi-Modal Deep Learning for Screening & Diagnosis\n",
        "# Professional Implementation for Medical Imaging\n",
        "# ============================================================================="
      ],
      "metadata": {
        "id": "ftgF3DRRyl4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EARLY DETECTION OF PULMONARY FIBROSIS - CLASSIFICATION PIPELINE\n",
        "# Multi-Modal Deep Learning for Screening & Diagnosis\n",
        "# Professional Implementation for Medical Imaging\n",
        "# =============================================================================\n",
        "\n",
        "# SECTION 1: IMPORTS AND CONFIGURATION\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Medical Imaging\n",
        "import pydicom\n",
        "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
        "import cv2\n",
        "\n",
        "# Vision Models\n",
        "import torchvision.models as models\n",
        "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
        "\n",
        "# ML Tools\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Augmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION FOR EARLY DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for early detection pipeline\"\"\"\n",
        "    # Paths\n",
        "    data_dir: Path = Path(\"/kaggle/input/lung-fibrosis-detection\")\n",
        "    output_dir: Path = Path(\"/kaggle/working\")\n",
        "\n",
        "    # Data Processing\n",
        "    img_size: int = 224\n",
        "    n_slices: int = 5\n",
        "    window_center: int = -600\n",
        "    window_width: int = 1500\n",
        "\n",
        "    # Classification Classes\n",
        "    classes: List[str] = field(default_factory=lambda: [\"Normal\", \"Early_Fibrosis\", \"Advanced_Fibrosis\"])\n",
        "    n_classes: int = 3\n",
        "\n",
        "    # Model Architecture\n",
        "    model_type: str = \"MultiModal\"\n",
        "    backbone: str = \"efficientnet\"\n",
        "    tabular_hidden_dims: Tuple[int, ...] = (512, 256, 128, 64)\n",
        "    fusion_method: str = \"attention\"\n",
        "    dropout_rate: float = 0.4\n",
        "    use_batch_norm: bool = True\n",
        "\n",
        "    # Training\n",
        "    n_folds: int = 5\n",
        "    batch_size: int = 32\n",
        "    num_epochs: int = 100\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    scheduler: str = 'cosine'\n",
        "    patience: int = 15\n",
        "    min_delta: float = 0.001\n",
        "    gradient_clip: float = 1.0\n",
        "\n",
        "    # Class Balance\n",
        "    use_class_weights: bool = True\n",
        "    focal_loss_alpha: float = 0.25\n",
        "    focal_loss_gamma: float = 2.0\n",
        "\n",
        "    # Augmentation\n",
        "    use_augmentation: bool = True\n",
        "    aug_prob: float = 0.7\n",
        "    use_mixup: bool = False\n",
        "\n",
        "    # Advanced\n",
        "    use_mixed_precision: bool = True\n",
        "    num_workers: int = 2\n",
        "    pin_memory: bool = True\n",
        "    seed: int = 42\n",
        "\n",
        "    # Validation\n",
        "    val_split: float = 0.15\n",
        "    test_split: float = 0.15\n",
        "\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# =============================================================================\n",
        "# REPRODUCIBILITY\n",
        "# =============================================================================\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Ensure reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(config.seed)\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 2: SYNTHETIC DATA GENERATION FOR DEMONSTRATION\n",
        "# =============================================================================\n",
        "\n",
        "def create_synthetic_detection_data():\n",
        "    \"\"\"Create realistic synthetic data for lung fibrosis detection\"\"\"\n",
        "    np.random.seed(config.seed)\n",
        "\n",
        "    # Simulate patient cohort for screening study\n",
        "    n_normal = 800      # Healthy controls\n",
        "    n_early = 150       # Early fibrosis (subclinical)\n",
        "    n_advanced = 100    # Advanced fibrosis\n",
        "\n",
        "    records = []\n",
        "    patient_id = 0\n",
        "\n",
        "    # Normal patients (age 20-70, good lung function)\n",
        "    for _ in range(n_normal):\n",
        "        age = np.random.uniform(20, 70)\n",
        "        sex = np.random.choice(['Male', 'Female'])\n",
        "        smoking = np.random.choice(['Never', 'Former', 'Current'], p=[0.6, 0.3, 0.1])\n",
        "\n",
        "        # Normal lung function\n",
        "        fvc = np.random.normal(3500, 500)\n",
        "        dlco = np.random.normal(85, 15)  # Diffusion capacity\n",
        "\n",
        "        # Environmental/occupational exposure (lower risk)\n",
        "        dust_exposure = np.random.choice(['None', 'Low', 'Moderate'], p=[0.7, 0.2, 0.1])\n",
        "        family_history = np.random.choice([0, 1], p=[0.9, 0.1])\n",
        "\n",
        "        records.append({\n",
        "            'Patient_ID': f'P{patient_id:05d}',\n",
        "            'Age': age,\n",
        "            'Sex': sex,\n",
        "            'Smoking_Status': smoking,\n",
        "            'FVC': fvc,\n",
        "            'DLCO': dlco,\n",
        "            'Dust_Exposure': dust_exposure,\n",
        "            'Family_History': family_history,\n",
        "            'Dyspnea_Score': np.random.randint(0, 2),  # 0-1 for normal\n",
        "            'Cough': np.random.choice([0, 1], p=[0.8, 0.2]),\n",
        "            'Weight_Loss': 0,  # Rare in normal\n",
        "            'Diagnosis': 'Normal'\n",
        "        })\n",
        "        patient_id += 1\n",
        "\n",
        "    # Early fibrosis (age 40-75, subtle symptoms)\n",
        "    for _ in range(n_early):\n",
        "        age = np.random.uniform(40, 75)\n",
        "        sex = np.random.choice(['Male', 'Female'], p=[0.6, 0.4])  # Male predominance\n",
        "        smoking = np.random.choice(['Never', 'Former', 'Current'], p=[0.4, 0.5, 0.1])\n",
        "\n",
        "        # Mildly reduced lung function\n",
        "        fvc = np.random.normal(3000, 400)\n",
        "        dlco = np.random.normal(70, 12)\n",
        "\n",
        "        # Higher risk factors\n",
        "        dust_exposure = np.random.choice(['None', 'Low', 'Moderate', 'High'], p=[0.3, 0.3, 0.3, 0.1])\n",
        "        family_history = np.random.choice([0, 1], p=[0.7, 0.3])\n",
        "\n",
        "        records.append({\n",
        "            'Patient_ID': f'P{patient_id:05d}',\n",
        "            'Age': age,\n",
        "            'Sex': sex,\n",
        "            'Smoking_Status': smoking,\n",
        "            'FVC': fvc,\n",
        "            'DLCO': dlco,\n",
        "            'Dust_Exposure': dust_exposure,\n",
        "            'Family_History': family_history,\n",
        "            'Dyspnea_Score': np.random.randint(1, 4),  # 1-3 for early\n",
        "            'Cough': np.random.choice([0, 1], p=[0.4, 0.6]),\n",
        "            'Weight_Loss': np.random.choice([0, 1], p=[0.8, 0.2]),\n",
        "            'Diagnosis': 'Early_Fibrosis'\n",
        "        })\n",
        "        patient_id += 1\n",
        "\n",
        "    # Advanced fibrosis (age 50-80, clear symptoms)\n",
        "    for _ in range(n_advanced):\n",
        "        age = np.random.uniform(50, 80)\n",
        "        sex = np.random.choice(['Male', 'Female'], p=[0.7, 0.3])\n",
        "        smoking = np.random.choice(['Never', 'Former', 'Current'], p=[0.3, 0.6, 0.1])\n",
        "\n",
        "        # Significantly reduced lung function\n",
        "        fvc = np.random.normal(2200, 400)\n",
        "        dlco = np.random.normal(50, 15)\n",
        "\n",
        "        # High risk factors\n",
        "        dust_exposure = np.random.choice(['Low', 'Moderate', 'High'], p=[0.2, 0.4, 0.4])\n",
        "        family_history = np.random.choice([0, 1], p=[0.5, 0.5])\n",
        "\n",
        "        records.append({\n",
        "            'Patient_ID': f'P{patient_id:05d}',\n",
        "            'Age': age,\n",
        "            'Sex': sex,\n",
        "            'Smoking_Status': smoking,\n",
        "            'FVC': fvc,\n",
        "            'DLCO': dlco,\n",
        "            'Dust_Exposure': dust_exposure,\n",
        "            'Family_History': family_history,\n",
        "            'Dyspnea_Score': np.random.randint(3, 5),  # 3-4 for advanced\n",
        "            'Cough': np.random.choice([0, 1], p=[0.2, 0.8]),\n",
        "            'Weight_Loss': np.random.choice([0, 1], p=[0.4, 0.6]),\n",
        "            'Diagnosis': 'Advanced_Fibrosis'\n",
        "        })\n",
        "        patient_id += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    logger.info(f\"Created synthetic dataset with {len(df)} patients\")\n",
        "    logger.info(f\"Class distribution:\\n{df['Diagnosis'].value_counts()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 3: MEDICAL IMAGE PROCESSING (ENHANCED FOR DETECTION)\n",
        "# =============================================================================\n",
        "\n",
        "class DicomProcessor:\n",
        "    \"\"\"Enhanced DICOM processing for fibrosis detection\"\"\"\n",
        "\n",
        "    def __init__(self, window_center: int = -600, window_width: int = 1500):\n",
        "        self.window_center = window_center\n",
        "        self.window_width = window_width\n",
        "\n",
        "    def load_dicom(self, path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load and preprocess DICOM file with enhanced processing\"\"\"\n",
        "        try:\n",
        "            dcm = pydicom.dcmread(path)\n",
        "            img = dcm.pixel_array.astype(np.float32)\n",
        "\n",
        "            # Apply modality LUT\n",
        "            if hasattr(dcm, 'RescaleSlope') and hasattr(dcm, 'RescaleIntercept'):\n",
        "                img = img * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "\n",
        "            # Apply windowing\n",
        "            img = self.apply_windowing(img)\n",
        "\n",
        "            # Enhanced preprocessing for fibrosis patterns\n",
        "            img = self.enhance_fibrosis_patterns(img)\n",
        "\n",
        "            # Normalize to [0, 1]\n",
        "            img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "            return img.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Failed to load DICOM {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def apply_windowing(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply lung window settings\"\"\"\n",
        "        min_val = self.window_center - self.window_width // 2\n",
        "        max_val = self.window_center + self.window_width // 2\n",
        "        return np.clip(img, min_val, max_val)\n",
        "\n",
        "    def enhance_fibrosis_patterns(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Enhance patterns relevant to fibrosis detection\"\"\"\n",
        "        # CLAHE for contrast enhancement\n",
        "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "        img_uint8 = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)\n",
        "        enhanced = clahe.apply(img_uint8).astype(np.float32)\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 4: FEATURE ENGINEERING FOR DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "class DetectionFeatureEngineer:\n",
        "    \"\"\"Feature engineering for early detection\"\"\"\n",
        "\n",
        "    def __init__(self, train_df: pd.DataFrame):\n",
        "        self.train_df = train_df\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "        self._prepare_features()\n",
        "\n",
        "    def _prepare_features(self):\n",
        "        \"\"\"Create comprehensive feature set for detection\"\"\"\n",
        "        # Prepare categorical encoders\n",
        "        categorical_features = ['Sex', 'Smoking_Status', 'Dust_Exposure']\n",
        "        for feature in categorical_features:\n",
        "            le = LabelEncoder()\n",
        "            le.fit(self.train_df[feature])\n",
        "            self.label_encoders[feature] = le\n",
        "\n",
        "        # Prepare numerical features\n",
        "        numerical_features = self._extract_numerical_features(self.train_df)\n",
        "        self.scaler.fit(numerical_features)\n",
        "\n",
        "    def _extract_numerical_features(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Extract and engineer numerical features\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            # Demographics (age-related risk)\n",
        "            age_norm = row['Age'] / 100.0\n",
        "            age_risk = 1.0 if row['Age'] > 60 else 0.0  # Higher risk after 60\n",
        "\n",
        "            # Gender (male predominance in IPF)\n",
        "            sex_male = 1.0 if row['Sex'] == 'Male' else 0.0\n",
        "\n",
        "            # Smoking status\n",
        "            smoke_never = 1.0 if row['Smoking_Status'] == 'Never' else 0.0\n",
        "            smoke_former = 1.0 if row['Smoking_Status'] == 'Former' else 0.0\n",
        "            smoke_current = 1.0 if row['Smoking_Status'] == 'Current' else 0.0\n",
        "\n",
        "            # Lung function (key diagnostic markers)\n",
        "            fvc_norm = row['FVC'] / 5000.0  # Normalize\n",
        "            fvc_reduced = 1.0 if row['FVC'] < 2500 else 0.0  # Below normal\n",
        "            dlco_norm = row['DLCO'] / 100.0\n",
        "            dlco_reduced = 1.0 if row['DLCO'] < 60 else 0.0  # Significantly reduced\n",
        "\n",
        "            # Environmental/occupational exposure\n",
        "            dust_none = 1.0 if row['Dust_Exposure'] == 'None' else 0.0\n",
        "            dust_low = 1.0 if row['Dust_Exposure'] == 'Low' else 0.0\n",
        "            dust_moderate = 1.0 if row['Dust_Exposure'] == 'Moderate' else 0.0\n",
        "            dust_high = 1.0 if row['Dust_Exposure'] == 'High' else 0.0\n",
        "\n",
        "            # Family history and symptoms\n",
        "            family_history = float(row['Family_History'])\n",
        "            dyspnea_score = row['Dyspnea_Score'] / 4.0  # Normalize 0-4 scale\n",
        "            cough = float(row['Cough'])\n",
        "            weight_loss = float(row['Weight_Loss'])\n",
        "\n",
        "            # Composite risk scores\n",
        "            smoking_risk = smoke_former * 0.5 + smoke_current * 1.0\n",
        "            exposure_risk = dust_low * 0.25 + dust_moderate * 0.5 + dust_high * 1.0\n",
        "            symptom_score = dyspnea_score + cough * 0.3 + weight_loss * 0.4\n",
        "\n",
        "            # Interaction features\n",
        "            age_smoking = age_norm * smoking_risk\n",
        "            age_exposure = age_norm * exposure_risk\n",
        "            male_exposure = sex_male * exposure_risk\n",
        "\n",
        "            feature_vector = [\n",
        "                age_norm, age_risk, sex_male,\n",
        "                smoke_never, smoke_former, smoke_current,\n",
        "                fvc_norm, fvc_reduced, dlco_norm, dlco_reduced,\n",
        "                dust_none, dust_low, dust_moderate, dust_high,\n",
        "                family_history, dyspnea_score, cough, weight_loss,\n",
        "                smoking_risk, exposure_risk, symptom_score,\n",
        "                age_smoking, age_exposure, male_exposure\n",
        "            ]\n",
        "\n",
        "            features.append(feature_vector)\n",
        "\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    def get_features(self, patient_data: Dict) -> np.ndarray:\n",
        "        \"\"\"Get scaled features for a single patient\"\"\"\n",
        "        # Convert dict to dataframe format\n",
        "        temp_df = pd.DataFrame([patient_data])\n",
        "        features = self._extract_numerical_features(temp_df)\n",
        "        return self.scaler.transform(features)[0]\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 5: MULTI-MODAL DATASET FOR DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "class FibrosisDetectionDataset(Dataset):\n",
        "    \"\"\"Dataset for early fibrosis detection\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        patient_data: pd.DataFrame,\n",
        "        feature_engineer: DetectionFeatureEngineer,\n",
        "        img_dir: Optional[Path] = None,\n",
        "        transform: Optional[A.Compose] = None,\n",
        "        n_slices: int = 5,\n",
        "        is_train: bool = True\n",
        "    ):\n",
        "        self.patient_data = patient_data\n",
        "        self.feature_engineer = feature_engineer\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.n_slices = n_slices\n",
        "        self.is_train = is_train\n",
        "        self.dicom_processor = DicomProcessor()\n",
        "\n",
        "        # Prepare label mapping\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(config.classes)}\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}\n",
        "\n",
        "        # Check for DICOM availability\n",
        "        self.has_images = img_dir is not None and img_dir.exists()\n",
        "        if self.has_images:\n",
        "            self._load_patient_images()\n",
        "\n",
        "        logger.info(f\"Detection dataset created with {len(patient_data)} patients\")\n",
        "        logger.info(f\"Class distribution: {patient_data['Diagnosis'].value_counts().to_dict()}\")\n",
        "\n",
        "    def _load_patient_images(self):\n",
        "        \"\"\"Load available DICOM images for each patient\"\"\"\n",
        "        self.patient_images = {}\n",
        "        for _, row in self.patient_data.iterrows():\n",
        "            patient_id = row['Patient_ID']\n",
        "            patient_dir = self.img_dir / patient_id\n",
        "\n",
        "            if patient_dir.exists():\n",
        "                dicom_files = sorted(patient_dir.glob(\"*.dcm\"))\n",
        "                if dicom_files:\n",
        "                    # Select representative slices\n",
        "                    n_files = len(dicom_files)\n",
        "                    if n_files >= self.n_slices:\n",
        "                        # Take slices from different regions of lungs\n",
        "                        indices = np.linspace(0, n_files-1, self.n_slices, dtype=int)\n",
        "                    else:\n",
        "                        indices = list(range(n_files))\n",
        "                    self.patient_images[patient_id] = [dicom_files[i] for i in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.patient_data.iloc[idx]\n",
        "        patient_id = row['Patient_ID']\n",
        "        diagnosis = row['Diagnosis']\n",
        "\n",
        "        # Get tabular features\n",
        "        tabular = self.feature_engineer.get_features(row.to_dict())\n",
        "\n",
        "        # Get image features\n",
        "        if self.has_images and patient_id in self.patient_images:\n",
        "            slices = self.patient_images[patient_id]\n",
        "            images = []\n",
        "            for slice_path in slices[:self.n_slices]:\n",
        "                img = self.dicom_processor.load_dicom(str(slice_path))\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, (config.img_size, config.img_size))\n",
        "                    images.append(img)\n",
        "\n",
        "            # Create multi-channel image\n",
        "            if len(images) >= 3:\n",
        "                img = np.stack(images[:3], axis=-1)\n",
        "            elif len(images) == 2:\n",
        "                img = np.stack([images[0], images[1], images[0]], axis=-1)\n",
        "            elif len(images) == 1:\n",
        "                img = np.stack([images[0]] * 3, axis=-1)\n",
        "            else:\n",
        "                img = np.zeros((config.img_size, config.img_size, 3), dtype=np.float32)\n",
        "        else:\n",
        "            # Generate synthetic texture patterns based on diagnosis for demo\n",
        "            img = self._generate_synthetic_ct_pattern(diagnosis)\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img)\n",
        "            img = augmented['image']\n",
        "        else:\n",
        "            img = torch.from_numpy(img.transpose(2, 0, 1)).float()\n",
        "\n",
        "        # Get label\n",
        "        label = self.class_to_idx[diagnosis]\n",
        "\n",
        "        return {\n",
        "            'image': img,\n",
        "            'tabular': torch.tensor(tabular, dtype=torch.float32),\n",
        "            'target': torch.tensor(label, dtype=torch.long),\n",
        "            'patient_id': patient_id\n",
        "        }\n",
        "\n",
        "    def _generate_synthetic_ct_pattern(self, diagnosis: str) -> np.ndarray:\n",
        "        \"\"\"Generate synthetic CT-like patterns for demonstration\"\"\"\n",
        "        img = np.random.rand(config.img_size, config.img_size, 3).astype(np.float32)\n",
        "\n",
        "        if diagnosis == 'Normal':\n",
        "            # Smooth, uniform pattern\n",
        "            img = cv2.GaussianBlur(img, (15, 15), 0) * 0.5 + 0.3\n",
        "        elif diagnosis == 'Early_Fibrosis':\n",
        "            # Subtle irregular patterns\n",
        "            noise = np.random.rand(config.img_size, config.img_size, 3) * 0.3\n",
        "            img = cv2.GaussianBlur(img, (7, 7), 0) * 0.6 + noise + 0.2\n",
        "        else:  # Advanced_Fibrosis\n",
        "            # Prominent irregular, honeycomb-like patterns\n",
        "            noise = np.random.rand(config.img_size, config.img_size, 3) * 0.5\n",
        "            honeycomb = np.sin(np.linspace(0, 10*np.pi, config.img_size))\n",
        "            honeycomb = np.outer(honeycomb, honeycomb)[:, :, np.newaxis] * 0.3\n",
        "            img = img * 0.4 + noise + np.repeat(honeycomb, 3, axis=2) + 0.1\n",
        "\n",
        "        return np.clip(img, 0, 1)\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 6: AUGMENTATION FOR MEDICAL DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "def get_detection_transforms(is_train: bool = True, img_size: int = 224) -> A.Compose:\n",
        "    \"\"\"Get medical-appropriate augmentation pipeline\"\"\"\n",
        "    if is_train:\n",
        "        return A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.05,  # Conservative shifts for medical\n",
        "                scale_limit=0.1,\n",
        "                rotate_limit=5,    # Small rotations\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.OneOf([\n",
        "                A.GaussNoise(var_limit=(5, 15)),\n",
        "                A.GaussianBlur(blur_limit=3),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
        "                A.RandomGamma(gamma_limit=(80, 120)),\n",
        "            ], p=0.6),\n",
        "            A.CoarseDropout(\n",
        "                max_holes=8,\n",
        "                max_height=32,\n",
        "                max_width=32,\n",
        "                p=0.3\n",
        "            ),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 7: NEURAL NETWORK ARCHITECTURES FOR DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class AttentionModule(nn.Module):\n",
        "    \"\"\"Attention mechanism for highlighting important regions\"\"\"\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_channels // reduction, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
        "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
        "        attention = avg_out + max_out\n",
        "        return x * attention.view(b, c, 1, 1)\n",
        "\n",
        "class FibrosisDetectionModel(nn.Module):\n",
        "    \"\"\"Multi-modal model for fibrosis detection\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes: int = 3,\n",
        "        tabular_dim: int = 24,\n",
        "        hidden_dims: Tuple[int] = (512, 256, 128),\n",
        "        dropout_rate: float = 0.4,\n",
        "        backbone: str = \"efficientnet\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Image backbone with attention\n",
        "        if backbone == \"efficientnet\":\n",
        "            self.image_backbone = efficientnet_b3(weights=EfficientNet_B3_Weights.DEFAULT)\n",
        "            img_feat_dim = self.image_backbone.classifier[1].in_features\n",
        "            self.image_backbone.classifier = nn.Identity()\n",
        "        else:\n",
        "            self.image_backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "            img_feat_dim = self.image_backbone.fc.in_features\n",
        "            self.image_backbone.fc = nn.Identity()\n",
        "\n",
        "        # Add attention to image features\n",
        "        self.image_attention = AttentionModule(img_feat_dim)\n",
        "\n",
        "        # Tabular network with residual connections\n",
        "        tab_layers = []\n",
        "        prev_dim = tabular_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            tab_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.tabular_net = nn.Sequential(*tab_layers)\n",
        "        tab_feat_dim = hidden_dims[-1]\n",
        "\n",
        "        # Cross-modal fusion\n",
        "        self.fusion = nn.MultiheadAttention(\n",
        "            embed_dim=256,\n",
        "            num_heads=8,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.img_proj = nn.Linear(img_feat_dim, 256)\n",
        "        self.tab_proj = nn.Linear(tab_feat_dim, 256)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, n_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights properly\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        # Extract image features\n",
        "        img_features = self.image_backbone(image)\n",
        "        img_features = self.image_attention(img_features.unsqueeze(-1).unsqueeze(-1)).squeeze()\n",
        "        img_projected = self.img_proj(img_features)\n",
        "\n",
        "        # Extract tabular features\n",
        "        tab_features = self.tabular_net(tabular)\n",
        "        tab_projected = self.tab_proj(tab_features)\n",
        "\n",
        "        # Cross-modal attention\n",
        "        img_seq = img_projected.unsqueeze(1)  # [B, 1, 256]\n",
        "        tab_seq = tab_projected.unsqueeze(1)  # [B, 1, 256]\n",
        "        combined_seq = torch.cat([img_seq, tab_seq], dim=1)  # [B, 2, 256]\n",
        "\n",
        "        attended, _ = self.fusion(combined_seq, combined_seq, combined_seq)\n",
        "        fused_features = attended.mean(dim=1)  # [B, 256]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 8: TRAINING PIPELINE FOR DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "class EarlyStoppingDetection:\n",
        "    \"\"\"Early stopping for classification\"\"\"\n",
        "    def __init__(self, patience=15, min_delta=0.001, metric='f1'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.metric = metric\n",
        "        self.best_score = 0.0\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if score > self.best_score + self.min_delta:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.best_weights:\n",
        "                    model.load_state_dict(self.best_weights)\n",
        "                logger.info(f\"Early stopping triggered. Best {self.metric}: {self.best_score:.4f}\")\n",
        "\n",
        "def train_detection_epoch(model, loader, optimizer, criterion, scaler):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in tqdm(loader, desc='Training'):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        tabular = batch['tabular'].to(DEVICE)\n",
        "        targets = batch['target'].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(enabled=config.use_mixed_precision):\n",
        "            logits = model(images, tabular)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_loss = total_loss / len(loader)\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_detection_epoch(model, loader, criterion):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "\n",
        "    for batch in tqdm(loader, desc='Validation'):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        tabular = batch['tabular'].to(DEVICE)\n",
        "        targets = batch['target'].to(DEVICE)\n",
        "\n",
        "        logits = model(images, tabular)\n",
        "        loss = criterion(logits, targets)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Get predictions and probabilities\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        _, preds = torch.max(logits, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_targets, all_preds)\n",
        "    precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_targets, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_targets, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    # Multi-class AUC\n",
        "    try:\n",
        "        auc = roc_auc_score(all_targets, np.array(all_probs), multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        auc = 0.0\n",
        "\n",
        "    metrics = {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "    return metrics, all_preds, all_targets\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 9: MAIN EXECUTION FOR EARLY DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "def main_detection():\n",
        "    \"\"\"Main detection pipeline\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LUNG FIBROSIS EARLY DETECTION PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create output directory\n",
        "    config.output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # 1. Create synthetic detection data\n",
        "    logger.info(\"Creating synthetic detection dataset...\")\n",
        "    df = create_synthetic_detection_data()\n",
        "\n",
        "    # 2. Create feature engineer\n",
        "    logger.info(\"Creating feature engineer...\")\n",
        "    feature_engineer = DetectionFeatureEngineer(df)\n",
        "\n",
        "    # 3. Stratified splits to maintain class balance\n",
        "    patients = df['Patient_ID'].unique()\n",
        "    labels = df.groupby('Patient_ID')['Diagnosis'].first()\n",
        "\n",
        "    train_patients, temp_patients = train_test_split(\n",
        "        patients,\n",
        "        test_size=(config.val_split + config.test_split),\n",
        "        stratify=labels,\n",
        "        random_state=config.seed\n",
        "    )\n",
        "\n",
        "    temp_labels = labels[temp_patients]\n",
        "    val_patients, test_patients = train_test_split(\n",
        "        temp_patients,\n",
        "        test_size=config.test_split/(config.val_split + config.test_split),\n",
        "        stratify=temp_labels,\n",
        "        random_state=config.seed\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_data = df[df['Patient_ID'].isin(train_patients)]\n",
        "    val_data = df[df['Patient_ID'].isin(val_patients)]\n",
        "    test_data = df[df['Patient_ID'].isin(test_patients)]\n",
        "\n",
        "    logger.info(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
        "\n",
        "    # 4. Create transforms\n",
        "    train_transform = get_detection_transforms(is_train=True, img_size=config.img_size)\n",
        "    val_transform = get_detection_transforms(is_train=False, img_size=config.img_size)\n",
        "\n",
        "    # 5. Create datasets\n",
        "    train_dataset = FibrosisDetectionDataset(\n",
        "        train_data, feature_engineer, None, train_transform, config.n_slices, is_train=True\n",
        "    )\n",
        "    val_dataset = FibrosisDetectionDataset(\n",
        "        val_data, feature_engineer, None, val_transform, config.n_slices, is_train=False\n",
        "    )\n",
        "    test_dataset = FibrosisDetectionDataset(\n",
        "        test_data, feature_engineer, None, val_transform, config.n_slices, is_train=False\n",
        "    )\n",
        "\n",
        "    # 6. Handle class imbalance with weighted sampling\n",
        "    if config.use_class_weights:\n",
        "        class_counts = train_data['Diagnosis'].value_counts()\n",
        "        class_weights = compute_class_weight(\n",
        "            'balanced',\n",
        "            classes=np.unique(train_data['Diagnosis']),\n",
        "            y=train_data['Diagnosis']\n",
        "        )\n",
        "        class_weight_dict = dict(zip(np.unique(train_data['Diagnosis']), class_weights))\n",
        "        logger.info(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "        # Create weighted sampler\n",
        "        sample_weights = [class_weight_dict[label] for label in train_data['Diagnosis']]\n",
        "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "        shuffle = False\n",
        "    else:\n",
        "        sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    # 7. Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=sampler,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory\n",
        "    )\n",
        "\n",
        "    # 8. Create model\n",
        "    logger.info(f\"Creating detection model...\")\n",
        "    model = FibrosisDetectionModel(\n",
        "        n_classes=config.n_classes,\n",
        "        tabular_dim=24,  # Based on feature engineer\n",
        "        hidden_dims=config.tabular_hidden_dims,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "        backbone=config.backbone\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # 9. Training setup\n",
        "    if config.use_class_weights:\n",
        "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "    else:\n",
        "        criterion = FocalLoss(alpha=config.focal_loss_alpha, gamma=config.focal_loss_gamma)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=20, T_mult=2, eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    early_stopper = EarlyStoppingDetection(patience=config.patience, metric='f1')\n",
        "    scaler = GradScaler() if config.use_mixed_precision else None\n",
        "\n",
        "    # 10. Training loop\n",
        "    logger.info(\"Starting training...\")\n",
        "    best_f1 = 0.0\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        logger.info(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_detection_epoch(model, train_loader, optimizer, criterion, scaler)\n",
        "\n",
        "        # Validate\n",
        "        val_metrics, val_preds, val_targets = validate_detection_epoch(model, val_loader, criterion)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log metrics\n",
        "        logger.info(\n",
        "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "            f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | \"\n",
        "            f\"Val F1: {val_metrics['f1']:.4f} | Val AUC: {val_metrics['auc']:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_acc'].append(val_metrics['accuracy'])\n",
        "        history['val_f1'].append(val_metrics['f1'])\n",
        "        history['val_auc'].append(val_metrics['auc'])\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopper(val_metrics['f1'], model)\n",
        "\n",
        "        if val_metrics['f1'] > best_f1:\n",
        "            best_f1 = val_metrics['f1']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_f1': best_f1,\n",
        "                'config': config\n",
        "            }, config.output_dir / 'best_detection_model.pth')\n",
        "            logger.info(f\"New best model saved with F1: {best_f1:.4f}\")\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            break\n",
        "\n",
        "        # Memory cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # 11. Final evaluation\n",
        "    logger.info(\"Evaluating on test set...\")\n",
        "    test_metrics, test_preds, test_targets = validate_detection_epoch(model, test_loader, criterion)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL TEST SET RESULTS - EARLY DETECTION\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"Test F1-Score: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"Test AUC: {test_metrics['auc']:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(test_targets, test_preds, target_names=config.classes))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(test_targets, test_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=config.classes, yticklabels=config.classes)\n",
        "    plt.title('Confusion Matrix - Fibrosis Detection')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.output_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 12. Create visualizations\n",
        "    create_detection_plots(history)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EARLY DETECTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return model, feature_engineer, history\n",
        "\n",
        "def create_detection_plots(history):\n",
        "    \"\"\"Create training visualization plots for detection\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0, 0].plot(history['train_loss'], label='Train Loss', alpha=0.7)\n",
        "    axes[0, 0].plot(history['val_loss'], label='Validation Loss', alpha=0.7)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy curves\n",
        "    axes[0, 1].plot(history['train_acc'], label='Train Accuracy', alpha=0.7)\n",
        "    axes[0, 1].plot([x*100 for x in history['val_acc']], label='Validation Accuracy', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # F1 Score progression\n",
        "    axes[1, 0].plot(history['val_f1'], label='Validation F1', color='green', alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('F1 Score')\n",
        "    axes[1, 0].set_title('F1 Score Progression')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # AUC progression\n",
        "    axes[1, 1].plot(history['val_auc'], label='Validation AUC', color='red', alpha=0.7)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('AUC Score')\n",
        "    axes[1, 1].set_title('AUC Score Progression')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.output_dir / 'detection_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Execute the detection pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, feature_engineer, history = main_detection()\n",
        "        logger.info(\"Early detection pipeline completed successfully!\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline failed with error: {e}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:54:24.271937Z",
          "iopub.status.idle": "2025-09-25T12:54:24.272169Z",
          "shell.execute_reply.started": "2025-09-25T12:54:24.272062Z",
          "shell.execute_reply": "2025-09-25T12:54:24.272072Z"
        },
        "id": "SSpkqM4Hyl4R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "9YeMeOTyyl4W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "d_rI9BD7yl4W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "UjtvvXLmyl4W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "fOz9tVKAyl4W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "w2mpoyzXyl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ar3X6XaQyl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "hTupuJxSyl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jQqHpvG-yl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "LU4Sdoheyl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "qCoR-JFNyl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "UGuVF9wPyl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "DrqOAK3ryl4X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "5HMXYAHCyl4Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "U1S8CpJ4yl4Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "utWQTRC2yl4Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT"
      ],
      "metadata": {
        "id": "y_wDiq2Tyl4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# COMPLETE MULTI-MODAL OSIC PULMONARY FIBROSIS MODEL (with epoch metric prints)\n",
        "# =============================================================================\n",
        "\n",
        "# SECTION 1: IMPORTS AND CONFIGURATION\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "import sys\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Medical Imaging\n",
        "import pydicom\n",
        "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
        "import cv2\n",
        "\n",
        "# Vision Models\n",
        "import torchvision.models as models\n",
        "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
        "\n",
        "# ML Tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: Path = Path(\"/kaggle/input/osic-pulmonary-fibrosis-progression\")\n",
        "    output_dir: Path = Path(\"/kaggle/working\")\n",
        "    img_size: int = 224\n",
        "    n_slices: int = 3\n",
        "    window_center: int = -600\n",
        "    window_width: int = 1500\n",
        "    model_type: str = \"MultiModal\"\n",
        "    backbone: str = \"efficientnet\"\n",
        "    tabular_hidden_dims: Tuple[int] = (512, 256, 128, 64)\n",
        "    fusion_method: str = \"attention\"\n",
        "    dropout_rate: float = 0.3\n",
        "    use_batch_norm: bool = True\n",
        "    n_folds: int = 5\n",
        "    batch_size: int = 32\n",
        "    num_epochs: int = 50\n",
        "    learning_rate: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    scheduler: str = 'cosine'\n",
        "    patience: int = 10\n",
        "    min_delta: float = 0.001\n",
        "    gradient_clip: float = 1.0\n",
        "    use_augmentation: bool = True\n",
        "    aug_prob: float = 0.5\n",
        "    use_mixup: bool = True\n",
        "    mixup_alpha: float = 0.2\n",
        "    use_mixed_precision: bool = True\n",
        "    num_workers: int = 2\n",
        "    pin_memory: bool = True\n",
        "    seed: int = 42\n",
        "    val_split: float = 0.15\n",
        "    test_split: float = 0.15\n",
        "\n",
        "config = Config()\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(config.seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 2: DATA LOADING AND PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def load_osic_data():\n",
        "    \"\"\"Load OSIC dataset with proper handling\"\"\"\n",
        "    try:\n",
        "        train_df = pd.read_csv(config.data_dir / \"train.csv\")\n",
        "        logger.info(f\"Loaded OSIC dataset with shape: {train_df.shape}\")\n",
        "\n",
        "        # Basic data validation\n",
        "        required_cols = ['Patient', 'Weeks', 'FVC', 'Percent', 'Age', 'Sex', 'SmokingStatus']\n",
        "        assert all(col in train_df.columns for col in required_cols), \"Missing required columns\"\n",
        "\n",
        "        return train_df\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not load OSIC data: {e}\")\n",
        "        logger.info(\"Creating synthetic data for demonstration...\")\n",
        "\n",
        "        # Create realistic synthetic OSIC-like data\n",
        "        np.random.seed(config.seed)\n",
        "        n_patients = 100\n",
        "        records = []\n",
        "\n",
        "        for patient_id in range(n_patients):\n",
        "            n_visits = np.random.randint(3, 10)\n",
        "            baseline_fvc = np.random.normal(2500, 700)\n",
        "            age = np.random.randint(50, 85)\n",
        "            sex = np.random.choice(['Male', 'Female'])\n",
        "            smoking = np.random.choice(['Never smoked', 'Ex-smoker', 'Currently smokes'])\n",
        "\n",
        "            for visit in range(n_visits):\n",
        "                week = visit * np.random.randint(1, 6)\n",
        "                fvc = baseline_fvc - np.random.normal(5, 2) * week\n",
        "                fvc = max(fvc, 500)\n",
        "\n",
        "                records.append({\n",
        "                    'Patient': f'ID{patient_id:04d}',\n",
        "                    'Weeks': week,\n",
        "                    'FVC': fvc,\n",
        "                    'Percent': np.random.normal(50, 15),\n",
        "                    'Age': age,\n",
        "                    'Sex': sex,\n",
        "                    'SmokingStatus': smoking\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 3: MEDICAL IMAGE PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "class DicomProcessor:\n",
        "    \"\"\"Professional DICOM processing with proper windowing\"\"\"\n",
        "\n",
        "    def __init__(self, window_center: int = -600, window_width: int = 1500):\n",
        "        self.window_center = window_center\n",
        "        self.window_width = window_width\n",
        "\n",
        "    def load_dicom(self, path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load and preprocess DICOM file\"\"\"\n",
        "        try:\n",
        "            dcm = pydicom.dcmread(path)\n",
        "\n",
        "            # Apply DICOM transformations\n",
        "            img = dcm.pixel_array.astype(np.float32)\n",
        "\n",
        "            # Apply modality LUT\n",
        "            if hasattr(dcm, 'RescaleSlope') and hasattr(dcm, 'RescaleIntercept'):\n",
        "                img = img * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "\n",
        "            # Apply windowing\n",
        "            img = self.apply_windowing(img)\n",
        "\n",
        "            # Normalize to [0, 1]\n",
        "            img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "            return img.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Failed to load DICOM {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def apply_windowing(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply lung window settings\"\"\"\n",
        "        min_val = self.window_center - self.window_width // 2\n",
        "        max_val = self.window_center + self.window_width // 2\n",
        "        return np.clip(img, min_val, max_val)\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 4: FEATURE ENGINEERING\n",
        "# =============================================================================\n",
        "\n",
        "class TabularFeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering for tabular data\"\"\"\n",
        "\n",
        "    def __init__(self, train_df: pd.DataFrame):\n",
        "        self.train_df = train_df\n",
        "        self.scaler = RobustScaler()\n",
        "        self.patient_features = {}\n",
        "        self._prepare_features()\n",
        "\n",
        "    def _prepare_features(self):\n",
        "        \"\"\"Create comprehensive feature set\"\"\"\n",
        "        for patient in self.train_df['Patient'].unique():\n",
        "            patient_data = self.train_df[self.train_df['Patient'] == patient].sort_values('Weeks')\n",
        "\n",
        "            # Baseline measurements\n",
        "            baseline = patient_data.iloc[0]\n",
        "\n",
        "            # Calculate FVC trajectory\n",
        "            if len(patient_data) > 1:\n",
        "                weeks = patient_data['Weeks'].values\n",
        "                fvc = patient_data['FVC'].values\n",
        "                # Linear regression for slope\n",
        "                slope = np.polyfit(weeks, fvc, 1)[0] if len(weeks) > 1 else 0\n",
        "                std_dev = np.std(fvc)\n",
        "            else:\n",
        "                slope = 0\n",
        "                std_dev = 0\n",
        "\n",
        "            self.patient_features[patient] = {\n",
        "                'Age': baseline['Age'],\n",
        "                'Sex': baseline['Sex'],\n",
        "                'SmokingStatus': baseline['SmokingStatus'],\n",
        "                'BaselineFVC': baseline['FVC'],\n",
        "                'BaselinePercent': baseline['Percent'],\n",
        "                'BaselineWeeks': baseline['Weeks'],\n",
        "                'FVCSlope': slope,\n",
        "                'FVCStdDev': std_dev,\n",
        "                'NumMeasurements': len(patient_data)\n",
        "            }\n",
        "\n",
        "        # Fit scaler\n",
        "        self._fit_scaler()\n",
        "\n",
        "    def _fit_scaler(self):\n",
        "        \"\"\"Fit scaler on all features\"\"\"\n",
        "        features = []\n",
        "        for stats in self.patient_features.values():\n",
        "            features.append(self._encode_features(stats, 0))\n",
        "        self.scaler.fit(features)\n",
        "\n",
        "    def _encode_features(self, stats: Dict, current_week: float) -> np.ndarray:\n",
        "        \"\"\"Encode features for a given patient and week\"\"\"\n",
        "        sex_male = 1 if stats['Sex'] == 'Male' else 0\n",
        "\n",
        "        smoke_never = 1 if stats['SmokingStatus'] == 'Never smoked' else 0\n",
        "        smoke_ex = 1 if stats['SmokingStatus'] == 'Ex-smoker' else 0\n",
        "        smoke_current = 1 if stats['SmokingStatus'] == 'Currently smokes' else 0\n",
        "\n",
        "        # Time features\n",
        "        week_delta = current_week - stats['BaselineWeeks']\n",
        "        week_squared = week_delta ** 2\n",
        "\n",
        "        # Interaction features\n",
        "        age_week = stats['Age'] * week_delta / 100\n",
        "\n",
        "        # Expected FVC\n",
        "        expected_fvc = stats['BaselineFVC'] + stats['FVCSlope'] * week_delta\n",
        "\n",
        "        features = [\n",
        "            stats['Age'] / 100,\n",
        "            sex_male,\n",
        "            smoke_never,\n",
        "            smoke_ex,\n",
        "            smoke_current,\n",
        "            stats['BaselineFVC'] / 5000,\n",
        "            stats['BaselinePercent'] / 100,\n",
        "            week_delta / 52,\n",
        "            week_squared / (52 ** 2),\n",
        "            stats['FVCSlope'] / 100,\n",
        "            stats['FVCStdDev'] / 1000,\n",
        "            age_week,\n",
        "            expected_fvc / 5000,\n",
        "            stats['NumMeasurements'] / 10\n",
        "        ]\n",
        "\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    def get_features(self, patient_id: str, week: float) -> np.ndarray:\n",
        "        \"\"\"Get scaled features\"\"\"\n",
        "        if patient_id not in self.patient_features:\n",
        "            # Return zero features if patient not found\n",
        "            return np.zeros(14, dtype=np.float32)\n",
        "\n",
        "        stats = self.patient_features[patient_id]\n",
        "        features = self._encode_features(stats, week)\n",
        "        return self.scaler.transform([features])[0]\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 5: MULTI-MODAL DATASET\n",
        "# =============================================================================\n",
        "\n",
        "class OSICMultiModalDataset(Dataset):\n",
        "    \"\"\"Dataset combining tabular and image data\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        patient_data: pd.DataFrame,\n",
        "        feature_engineer: TabularFeatureEngineer,\n",
        "        img_dir: Optional[Path] = None,\n",
        "        transform: Optional[A.Compose] = None,\n",
        "        n_slices: int = 3,\n",
        "        is_train: bool = True\n",
        "    ):\n",
        "        self.patient_data = patient_data\n",
        "        self.feature_engineer = feature_engineer\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.n_slices = n_slices\n",
        "        self.is_train = is_train\n",
        "        self.dicom_processor = DicomProcessor()\n",
        "\n",
        "        # Prepare samples\n",
        "        self.samples = []\n",
        "        self.patient_images = {}\n",
        "\n",
        "        # Check for DICOM availability\n",
        "        self.has_images = img_dir is not None and img_dir.exists()\n",
        "\n",
        "        if self.has_images:\n",
        "            self._load_patient_images()\n",
        "\n",
        "        # Create samples\n",
        "        for _, row in patient_data.iterrows():\n",
        "            self.samples.append({\n",
        "                'patient': row['Patient'],\n",
        "                'week': row['Weeks'],\n",
        "                'fvc': row['FVC']\n",
        "            })\n",
        "\n",
        "        logger.info(f\"Dataset created with {len(self.samples)} samples\")\n",
        "\n",
        "    def _load_patient_images(self):\n",
        "        \"\"\"Load available DICOM images for each patient\"\"\"\n",
        "        for patient in self.patient_data['Patient'].unique():\n",
        "            patient_dir = self.img_dir / patient\n",
        "\n",
        "            if patient_dir.exists():\n",
        "                dicom_files = sorted(patient_dir.glob(\"*.dcm\"))\n",
        "\n",
        "                if dicom_files:\n",
        "                    # Select evenly spaced slices\n",
        "                    n_files = len(dicom_files)\n",
        "                    if n_files >= self.n_slices:\n",
        "                        indices = np.linspace(0, n_files-1, self.n_slices, dtype=int)\n",
        "                    else:\n",
        "                        indices = list(range(n_files))\n",
        "\n",
        "                    self.patient_images[patient] = [dicom_files[i] for i in indices]\n",
        "                else:\n",
        "                    self.patient_images[patient] = None\n",
        "            else:\n",
        "                self.patient_images[patient] = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        patient = sample['patient']\n",
        "        week = sample['week']\n",
        "        fvc = sample['fvc']\n",
        "\n",
        "        # Get tabular features\n",
        "        tabular = self.feature_engineer.get_features(patient, week)\n",
        "\n",
        "        # Get image features\n",
        "        if self.has_images and patient in self.patient_images:\n",
        "            slices = self.patient_images.get(patient)\n",
        "            if slices:\n",
        "                images = []\n",
        "                for slice_path in slices[:3]:  # Maximum 3 slices\n",
        "                    img = self.dicom_processor.load_dicom(str(slice_path))\n",
        "                    if img is not None:\n",
        "                        # Resize to target size\n",
        "                        img = cv2.resize(img, (config.img_size, config.img_size))\n",
        "                        images.append(img)\n",
        "\n",
        "                # Create 3-channel image\n",
        "                if len(images) >= 3:\n",
        "                    img = np.stack(images[:3], axis=-1)\n",
        "                elif len(images) == 2:\n",
        "                    img = np.stack([images[0], images[1], images[1]], axis=-1)\n",
        "                elif len(images) == 1:\n",
        "                    img = np.stack([images[0], images[0], images[0]], axis=-1)\n",
        "                else:\n",
        "                    img = np.zeros((config.img_size, config.img_size, 3), dtype=np.float32)\n",
        "            else:\n",
        "                img = np.zeros((config.img_size, config.img_size, 3), dtype=np.float32)\n",
        "        else:\n",
        "            # No images available - use zeros\n",
        "            img = np.zeros((config.img_size, config.img_size, 3), dtype=np.float32)\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img)\n",
        "            img = augmented['image']\n",
        "        else:\n",
        "            img = torch.from_numpy(img.transpose(2, 0, 1)).float()\n",
        "\n",
        "        return {\n",
        "            'image': img,\n",
        "            'tabular': torch.tensor(tabular, dtype=torch.float32),\n",
        "            'target': torch.tensor(fvc, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 6: AUGMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def get_transforms(is_train: bool = True, img_size: int = 224) -> A.Compose:\n",
        "    \"\"\"Get augmentation pipeline\"\"\"\n",
        "    if is_train:\n",
        "        return A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.1,\n",
        "                scale_limit=0.1,\n",
        "                rotate_limit=10,\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.OneOf([\n",
        "                A.GaussNoise(var_limit=(10, 50)),\n",
        "                A.GaussianBlur(blur_limit=3),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
        "            ], p=0.5),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 7: NEURAL NETWORK ARCHITECTURES\n",
        "# =============================================================================\n",
        "\n",
        "# Tabular Network Components\n",
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"Residual block for tabular data\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(out_dim) if config.use_batch_norm else nn.Identity()\n",
        "        self.bn2 = nn.BatchNorm1d(out_dim) if config.use_batch_norm else nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.shortcut = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.fc1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.bn2(self.fc2(out))\n",
        "        out += shortcut\n",
        "        return F.relu(out)\n",
        "\n",
        "# Attention Mechanisms\n",
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\"Cross-modal attention between image and tabular features\"\"\"\n",
        "    def __init__(self, img_dim, tab_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.img_proj = nn.Linear(img_dim, hidden_dim)\n",
        "        self.tab_proj = nn.Linear(tab_dim, hidden_dim)\n",
        "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "\n",
        "    def forward(self, img_feat, tab_feat):\n",
        "        # Project features\n",
        "        img_proj = self.img_proj(img_feat).unsqueeze(1)  # [B, 1, hidden]\n",
        "        tab_proj = self.tab_proj(tab_feat).unsqueeze(1)  # [B, 1, hidden]\n",
        "\n",
        "        # Cross attention\n",
        "        combined = torch.cat([img_proj, tab_proj], dim=1)  # [B, 2, hidden]\n",
        "        attended, _ = self.attention(combined, combined, combined)\n",
        "\n",
        "        # Aggregate\n",
        "        return attended.mean(dim=1)  # [B, hidden]\n",
        "\n",
        "# Main Multi-Modal Model\n",
        "class OSICMultiModalModel(nn.Module):\n",
        "    \"\"\"Complete multi-modal architecture\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tabular_dim: int = 14,\n",
        "        hidden_dims: Tuple[int] = (512, 256, 128, 64),\n",
        "        dropout_rate: float = 0.3,\n",
        "        backbone: str = \"efficientnet\",\n",
        "        fusion_method: str = \"attention\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Image backbone\n",
        "        if backbone == \"efficientnet\":\n",
        "            self.image_backbone = efficientnet_b3(weights=EfficientNet_B3_Weights.DEFAULT)\n",
        "            img_feat_dim = self.image_backbone.classifier[1].in_features\n",
        "            self.image_backbone.classifier = nn.Identity()\n",
        "        elif backbone == \"resnet50\":\n",
        "            self.image_backbone = models.resnet50(pretrained=True)\n",
        "            img_feat_dim = self.image_backbone.fc.in_features\n",
        "            self.image_backbone.fc = nn.Identity()\n",
        "        else:\n",
        "            self.image_backbone = models.densenet121(pretrained=True)\n",
        "            img_feat_dim = self.image_backbone.classifier.in_features\n",
        "            self.image_backbone.classifier = nn.Identity()\n",
        "\n",
        "        # Tabular network\n",
        "        tab_layers = []\n",
        "        prev_dim = tabular_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            tab_layers.append(ResNetBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "        self.tabular_net = nn.Sequential(*tab_layers)\n",
        "        tab_feat_dim = hidden_dims[-1]\n",
        "\n",
        "        # Fusion method\n",
        "        self.fusion_method = fusion_method\n",
        "        if fusion_method == \"attention\":\n",
        "            self.fusion = CrossModalAttention(img_feat_dim, tab_feat_dim)\n",
        "            fusion_dim = 256\n",
        "        elif fusion_method == \"gated\":\n",
        "            self.gate = nn.Sequential(\n",
        "                nn.Linear(img_feat_dim + tab_feat_dim, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 2),\n",
        "                nn.Softmax(dim=1)\n",
        "            )\n",
        "            fusion_dim = img_feat_dim + tab_feat_dim\n",
        "        else:  # concat\n",
        "            fusion_dim = img_feat_dim + tab_feat_dim\n",
        "\n",
        "        # Final regression head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 256),\n",
        "            nn.BatchNorm1d(256) if config.use_batch_norm else nn.Identity(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128) if config.use_batch_norm else nn.Identity(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # Uncertainty head (for confidence estimation)\n",
        "        self.uncertainty_head = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights properly\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        # Extract features\n",
        "        img_features = self.image_backbone(image)\n",
        "        tab_features = self.tabular_net(tabular)\n",
        "\n",
        "        # Fusion\n",
        "        if self.fusion_method == \"attention\":\n",
        "            combined = self.fusion(img_features, tab_features)\n",
        "        elif self.fusion_method == \"gated\":\n",
        "            concat_features = torch.cat([img_features, tab_features], dim=1)\n",
        "            gates = self.gate(concat_features)\n",
        "            combined = gates[:, 0:1] * img_features + gates[:, 1:2] * tab_features\n",
        "        else:\n",
        "            combined = torch.cat([img_features, tab_features], dim=1)\n",
        "\n",
        "        # Predictions\n",
        "        prediction = self.head(combined).squeeze(-1)\n",
        "        log_variance = self.uncertainty_head(combined).squeeze(-1)\n",
        "\n",
        "        return prediction, log_variance\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 8: LOSS FUNCTIONS AND METRICS\n",
        "# =============================================================================\n",
        "\n",
        "class RobustLoss(nn.Module):\n",
        "    \"\"\"Combined loss with uncertainty\"\"\"\n",
        "    def __init__(self, alpha: float = 0.7, beta: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.mae = nn.L1Loss()\n",
        "\n",
        "    def forward(self, pred_mean, pred_log_var, target):\n",
        "        # Heteroscedastic uncertainty loss\n",
        "        precision = torch.exp(-pred_log_var)\n",
        "        mse_loss = precision * (pred_mean - target) ** 2 + pred_log_var\n",
        "        nll_loss = 0.5 * torch.mean(mse_loss)\n",
        "\n",
        "        # MAE for robustness\n",
        "        mae_loss = self.mae(pred_mean, target)\n",
        "\n",
        "        # Combined\n",
        "        total_loss = self.alpha * nll_loss + self.beta * mae_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "def laplace_log_likelihood(y_true, y_pred, sigma):\n",
        "    \"\"\"Calculate Laplace Log Likelihood\"\"\"\n",
        "    sigma = np.maximum(sigma, 70)  # OSIC specific\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000)\n",
        "    metric = -np.sqrt(2) * delta / sigma - np.log(np.sqrt(2) * sigma)\n",
        "    return np.mean(metric)\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 9: TRAINING PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping with best model restoration\"\"\"\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_score = -float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if score > self.best_score + self.min_delta:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.best_weights:\n",
        "                    model.load_state_dict(self.best_weights)\n",
        "                logger.info(f\"Early stopping triggered after {self.counter} epochs\")\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, scaler, config):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch in tqdm(loader, desc='Training'):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        tabular = batch['tabular'].to(DEVICE)\n",
        "        targets = batch['target'].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training\n",
        "        with autocast(enabled=config.use_mixed_precision):\n",
        "            predictions, log_var = model(images, tabular)\n",
        "            loss = criterion(predictions, log_var, targets)\n",
        "\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, loader, criterion):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_sigmas = []\n",
        "\n",
        "    for batch in tqdm(loader, desc='Validation'):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        tabular = batch['tabular'].to(DEVICE)\n",
        "        targets = batch['target'].to(DEVICE)\n",
        "\n",
        "        predictions, log_var = model(images, tabular)\n",
        "        loss = criterion(predictions, log_var, targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_sigmas.extend(torch.exp(0.5 * log_var).cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "    all_sigmas = np.array(all_sigmas)\n",
        "\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "    lll = laplace_log_likelihood(all_targets, all_preds, all_sigmas)\n",
        "\n",
        "    return np.mean(losses), {'mae': mae, 'rmse': rmse, 'r2': r2, 'lll': lll}\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"OSIC MULTI-MODAL PIPELINE - COMPLETE IMPLEMENTATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create output directory\n",
        "    config.output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # 1. Load and prepare data\n",
        "    logger.info(\"Loading OSIC data...\")\n",
        "    train_df = load_osic_data()\n",
        "\n",
        "    # 2. Create feature engineer\n",
        "    logger.info(\"Creating feature engineer...\")\n",
        "    feature_engineer = TabularFeatureEngineer(train_df)\n",
        "\n",
        "    # 3. Prepare patient-level splits (avoiding data leakage)\n",
        "    patients = train_df['Patient'].unique()\n",
        "    train_patients, temp_patients = train_test_split(\n",
        "        patients,\n",
        "        test_size=(config.val_split + config.test_split),\n",
        "        random_state=config.seed\n",
        "    )\n",
        "    val_patients, test_patients = train_test_split(\n",
        "        temp_patients,\n",
        "        test_size=config.test_split/(config.val_split + config.test_split),\n",
        "        random_state=config.seed\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_data = train_df[train_df['Patient'].isin(train_patients)]\n",
        "    val_data = train_df[train_df['Patient'].isin(val_patients)]\n",
        "    test_data = train_df[train_df['Patient'].isin(test_patients)]\n",
        "\n",
        "    logger.info(f\"Train samples: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
        "\n",
        "    # 4. Create transforms\n",
        "    train_transform = get_transforms(is_train=True, img_size=config.img_size)\n",
        "    val_transform = get_transforms(is_train=False, img_size=config.img_size)\n",
        "\n",
        "    # 5. Create datasets\n",
        "    img_dir = config.data_dir / \"train\" if (config.data_dir / \"train\").exists() else None\n",
        "\n",
        "    train_dataset = OSICMultiModalDataset(\n",
        "        train_data, feature_engineer, img_dir, train_transform, config.n_slices, is_train=True\n",
        "    )\n",
        "    val_dataset = OSICMultiModalDataset(\n",
        "        val_data, feature_engineer, img_dir, val_transform, config.n_slices, is_train=False\n",
        "    )\n",
        "    test_dataset = OSICMultiModalDataset(\n",
        "        test_data, feature_engineer, img_dir, val_transform, config.n_slices, is_train=False\n",
        "    )\n",
        "\n",
        "    # 6. Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory\n",
        "    )\n",
        "\n",
        "    # 7. Create model\n",
        "    logger.info(f\"Creating {config.backbone} model...\")\n",
        "    model = OSICMultiModalModel(\n",
        "        tabular_dim=14,  # Based on feature engineer\n",
        "        hidden_dims=config.tabular_hidden_dims,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "        backbone=config.backbone,\n",
        "        fusion_method=config.fusion_method\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # 8. Training setup\n",
        "    criterion = RobustLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    if config.scheduler == 'cosine':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "        )\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=config.patience, min_delta=config.min_delta)\n",
        "    scaler = GradScaler() if config.use_mixed_precision else None\n",
        "\n",
        "    # 9. Training loop\n",
        "    print(\"Starting training...\")\n",
        "    best_score = -float('inf')\n",
        "    train_history = {'train_loss': [], 'val_loss': [], 'val_r2': [], 'val_lll': []}\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, config)\n",
        "        val_loss, val_metrics = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "        if config.scheduler == 'cosine':\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            scheduler.step(val_metrics['r2'])\n",
        "\n",
        "        # Print metrics for this epoch\n",
        "        print(f\"Epoch [{epoch+1}/{config.num_epochs}], \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "              f\"MAE: {val_metrics['mae']:.4f}, RMSE: {val_metrics['rmse']:.4f}, \"\n",
        "              f\"R2: {val_metrics['r2']:.4f}, LLL: {val_metrics['lll']:.4f}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        train_history['train_loss'].append(train_loss)\n",
        "        train_history['val_loss'].append(val_loss)\n",
        "        train_history['val_r2'].append(val_metrics['r2'])\n",
        "        train_history['val_lll'].append(val_metrics['lll'])\n",
        "\n",
        "        early_stopper(val_metrics['r2'], model)\n",
        "\n",
        "        if val_metrics['r2'] > best_score:\n",
        "            best_score = val_metrics['r2']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_score': best_score,\n",
        "                'config': config\n",
        "            }, config.output_dir / 'best_model.pth')\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # 10. Final evaluation on test set\n",
        "    test_loss, test_metrics = validate_epoch(model, test_loader, criterion)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL TEST SET RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test MAE: {test_metrics['mae']:.4f}\")\n",
        "    print(f\"Test RMSE: {test_metrics['rmse']:.4f}\")\n",
        "    print(f\"Test R²: {test_metrics['r2']:.4f}\")\n",
        "    print(f\"Test LLL: {test_metrics['lll']:.4f}\")\n",
        "\n",
        "    # 11. Create visualizations\n",
        "    logger.info(\"Creating visualizations...\")\n",
        "    create_training_plots(train_history)\n",
        "\n",
        "    # 12. Feature importance analysis (if possible)\n",
        "    try:\n",
        "        analyze_model_predictions(model, test_loader, test_data, feature_engineer)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not create prediction analysis: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:46:53.26711Z",
          "iopub.execute_input": "2025-09-25T12:46:53.267869Z",
          "iopub.status.idle": "2025-09-25T12:54:24.202735Z",
          "shell.execute_reply.started": "2025-09-25T12:46:53.267832Z",
          "shell.execute_reply": "2025-09-25T12:54:24.201863Z"
        },
        "id": "803-Y28yyl4a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Goal Component                                                   |    Covered? | Evidence in current pipeline                                                                                        | Required Changes / Clarifications                                                                                                                                                                                                     |                 Priority | Next Action (code-level)                                                                                                                                            |\n",
        "| ---------------------------------------------------------------- | ----------: | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -----------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Target: early prediction of pulmonary fibrosis (progression)** | **Partial** | Pipeline predicts FVC (continuous) and uncertainty — FVC decline is proxy for progression.                          | Confirm *exact* early-prediction definition (e.g., predict future FVC at Δweeks, binary progression within X weeks, or risk score). If you want binary/early-warning, add a classification head or thresholding on predicted decline. |                     High | Decide target form (continuous future FVC vs. binary event). Add target-generation code in `load_osic_data()` to create `target_week` / `delta_week` and label.     |\n",
        "| **Time horizon / label generation**                              |          No | Current code uses existing FVC at each row as target.                                                               | Create labels for *future* FVC (e.g., FVC at baseline + 12 weeks). Need to align scans/timestamps per patient.                                                                                                                        |                     High | Implement function `build_future_target(df, horizon_weeks)` that merges baseline rows with follow-up FVC at `week + horizon`. Use nearest-match within tolerance.   |\n",
        "| **DICOM CT inclusion (per-patient imaging)**                     |         Yes | `OSICMultiModalDataset` reads patient DICOM folders, selects slices, applies windowing and builds 3-channel inputs. | Consider selecting slices relative to baseline scan date used for the prediction; support 2D slice selection around lung region or small 3D patches.                                                                                  |                     High | In dataset `__getitem__`, select CTs corresponding to the *baseline* visit (match by scan date or nearest week). Add `scan_date` or mapping file.                   |\n",
        "| **Tabular features for early prediction**                        |         Yes | `TabularFeatureEngineer` computes baseline stats, slope, expected FVC, time features.                               | Ensure features are computed **using only data available at prediction time** (no leakage of future measurements).                                                                                                                    |                 Critical | In `get_features(patient, week)` enforce using only rows `<= week`. Modify `_prepare_features` to store temporal sequences and compute features per-week on demand. |\n",
        "| **Model architecture (image + tabular fusion)**                  |         Yes | `OSICMultiModalModel` with attention fusion; uncertainty head included.                                             | Good choice. For early prediction consider adding sequence modeling (if using longitudinal inputs) or include baseline+follow-up branches.                                                                                            |                   Medium | If using multiple prior visits, add LSTM/temporal encoder for tabular sequences and fuse with image embedding.                                                      |\n",
        "| **Loss / uncertainty modeling**                                  |         Yes | `RobustLoss` combines heteroscedastic NLL and MAE; LLL metric computed.                                             | If target becomes binary progression, add classification loss (CE/BCE) or multi-task loss (regression + classification).                                                                                                              | High (depends on target) | For regression: keep `RobustLoss`. For binary: add `BCEWithLogitsLoss` and weight multi-task losses.                                                                |\n",
        "| **Evaluation & splits (grouped by patient)**                     |         Yes | Patient-level train/val/test splitting used.                                                                        | For early prediction evaluate at the *prediction horizon*; report time-dependent metrics (MAE, AUC if binary) and calibration of uncertainty (coverage).                                                                              |                 Critical | Implement `evaluate_at_horizon(loader, horizon)` and compute per-horizon metrics; save per-epoch metrics.                                                           |\n",
        "| **Data leakage checks**                                          |     Partial | Group-split implemented but feature engineer may use future info.                                                   | Ensure feature engineering and image selection never use future target/visits.                                                                                                                                                        |                 Critical | Add unit tests/assertions that for each sample, engineered features and images correspond to time ≤ prediction time.                                                |\n",
        "| **Augmentation & robustness**                                    |         Yes | Augmentations for slices present.                                                                                   | Consider domain-specific augmentations (intensity scaling, elastic deformations) and test sensitivity to slice selection.                                                                                                             |                   Medium | Add augmentation config toggles and an ablation routine to test slice-counts (`n_slices=1..5`).                                                                     |\n",
        "| **Interpretability / clinical utility**                          |          No | Diagnostics plots exist (pred vs true, residuals).                                                                  | Add SHAP/feature importance for tabular part and Grad-CAM or saliency maps for image backbone to explain predictions.                                                                                                                 |                   Medium | Export feature_importance via permutation and implement Grad-CAM visualization for sample scans.                                                                    |\n",
        "| **Deployment / inference pipeline**                              |          No | Training & saving best model exists.                                                                                | Need inference function that accepts baseline tabular + baseline CT and outputs predicted risk/FVC + uncertainty and human-readable report.                                                                                           |               Low→Medium | Implement `predict_for_patient(patient_id, week)` that loads preprocessing pipeline, model, and returns outputs + visualizations.                                   |\n",
        "| **Next experiments to prioritize**                               |           — | —                                                                                                                   | 1) Define target horizon; 2) Fix temporal leakage; 3) Match CT to baseline visit; 4) Train & evaluate at horizon; 5) Add binary progression head if required.                                                                         |                        — | Implement `build_future_target()`, update dataset mapping to baseline CTs, and run one short experiment (2–5 epochs) printing per-epoch metrics.                    |\n"
      ],
      "metadata": {
        "id": "hZeNjjc4yl4c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "99jo8EJvyl4k"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}